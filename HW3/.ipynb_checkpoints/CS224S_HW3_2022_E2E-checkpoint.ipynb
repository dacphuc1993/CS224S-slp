{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgfqwLKSPaAw"
   },
   "source": [
    "# CS224S Assignment 3: Deep Learning for End-to-End Speech Recognition\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXsGVZtG-dzr"
   },
   "source": [
    "This notebook is worth 70 / 100 possible points for homework 3. You should be able to train all models in Colab. We encourage you to read general PyTorch / Lightning tutorials as necessary as you work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh8A_5L1dq6-"
   },
   "source": [
    "**Note:** You will need to make a copy of this Colab notebook in your Google Drive before you can edit it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9ahWfoydgHv"
   },
   "outputs": [],
   "source": [
    "#@title Setup Cell\n",
    "\n",
    "#@markdown **Mount Google Drive.** We will use your `cs224s_spring2022`\n",
    "#@markdown directory in your Google Drive to store all relevant files,\n",
    "#@markdown including `utils.py`.\n",
    "\n",
    "# Do not modify.\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs224s_spring2022'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir $DRIVE_PATH\n",
    "\n",
    "SYM_PATH = '/content/cs224s_spring2022'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH\n",
    "\n",
    "#@markdown **Data.** It takes ~5-10 minutes to download the dataset the first\n",
    "#@markdown time you run this cell. Afterwards it will stay saved in\n",
    "#@markdown `cs224s_spring2022/data/harper_valley_bank_minified`.\n",
    "\n",
    "\n",
    "DATA_PATH = '{}/data'.format(SYM_PATH)\n",
    "if not os.path.exists(DATA_PATH):\n",
    "  %mkdir $DATA_PATH\n",
    "%cd $DATA_PATH\n",
    "if not os.path.exists(os.path.join(DATA_PATH, 'harper_valley_bank_minified')):\n",
    "  !wget -q http://web.stanford.edu/class/cs224s/download/harper_valley_bank_minified.zip\n",
    "  !unzip -q harper_valley_bank_minified.zip\n",
    "  %rm harper_valley_bank_minified.zip\n",
    "\n",
    "#@markdown **Experiments.** Model checkpoints will be saved in your\n",
    "#@markdown `cs224s_spring2022/trained_models` directory.\n",
    "MODEL_PATH = '{}/trained_models'.format(SYM_PATH)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "  %mkdir $MODEL_PATH\n",
    "\n",
    "%cd $SYM_PATH\n",
    "if not os.path.exists(os.path.join(SYM_PATH, 'utils.py')):\n",
    "  !wget -q http://web.stanford.edu/class/cs224s/download/utils.py\n",
    "\n",
    "#@markdown **Note on Sessions.** You have to run this cell each new session\n",
    "#@markdown (sessions either expire after some time or after you close the notebook).\n",
    "#@markdown You may have to periodically go to **Runtime** > **Factory reset runtime**\n",
    "#@markdown if you are experiencing Colab environment issues. Requirements must be installed each new session.\n",
    "\n",
    "# !pip -q install pytorch_lightning\n",
    "# !pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch_lightning\n",
    "# !pip install wandb\n",
    "# !pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from glob import glob\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import *\n",
    "from IPython.display import Audio\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7eUiyEhfnYV"
   },
   "source": [
    "# Part 1: ML Speech Data Pipeline\n",
    "\n",
    "HarperValleyBank consists of 23 hours of audio from 1,446 human-human conversations between 59 unique speakers. For your convenience, we store in `harper_valley_bank_minified` all utterance audio waveforms as `numpy` arrays in `data.h5` and all transcripts and labels as `numpy` arrays in `labels.npz`.\n",
    "\n",
    "Our custom dataset class `HarperValleyBank` should inherit `torch.utils.data.Dataset` and overwrite the following methods:\n",
    "- `__len__` so that `len(dataset)` returns the size of the dataset.\n",
    "- `__getitem__` to support the indexing such that `dataset[i]` can be used to get the `i`th dataset sample.\n",
    "\n",
    "There are a few special features that the `HarperValleyBank` class should exhibit.\n",
    "- **Fixed-length data.** Both the extracted audio features and the character labels will inherently be sequences of different lengths. However, in order to store data in a minibatch during training, we need to make the lengths uniform. To do so, we can first enforce a maximum length for audio waves and a maximum length for labels (note that these two maximum lengths are not necessarily the same). We have preprocessed all sequences to be cropped by single utterances as opposed to conversations. Next, we can crop and pad each sequence with a pad token (e.g. `3`) such that all audio sequences and all label sequences are their respective maximum lengths. We will also store the actual lengths of each sequence so that the model does not learn from the padded indices.\n",
    "- **Sequence representation.** We are training a character-level model, so the ASR model is responsible for predicting each spoken character. Therefore, we must convert our transcript text to a list of indices representing 34 possible characters (see the global variable `VOCAB`) and a few domain-specific tokens (see the global variable `SILENT_VOCAB` e.g. `[laughter]`). Think of each character as its own class.\n",
    "```\n",
    "Raw utterance:  hi this is an example .\n",
    "List of characters: ['h', 'i', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', '.']\n",
    "List of indices: [18, 19, 3, 30, 18, 19, 29, 3, 19, 29, 3, 11, 24, 3, 15, 34, 11, 23, 26, 22, 15, 3, 6]\n",
    "```\n",
    "- **Special tokens.** Although this next part is provided, it is worth pointing out. Aside from the padding index, there are three special tokens in our vocabulary:\n",
    "    - A blank token (`epsilon`, represented by index `0`) which designates a padded index and plays a special role in CTC.\n",
    "    - A start-of-sentence token (`SOS`, represented by index `1`) which designates the start of a sentence.\n",
    "    - An end-of-sentence token (`EOS`, represented by index `2`) which designates the end of a sentence.\n",
    "```\n",
    "Example label sequence: [18, 19, 3, 30, 18]\n",
    "Add an END token: [18, 19, 3, 30, 18, 2]\n",
    "```\n",
    "Suppose the maximum label sequence has length 10.\n",
    "```\n",
    "Padded label sequence: [18, 19, 3, 30, 18, 2, 0, 0, 0, 0]\n",
    "Label sequence length: 6\n",
    "```\n",
    "\n",
    "**It may be helpful to first read through the `HarperValleyBank` starter code and `utils.py` to get familiar with the data pipeline.**\n",
    "\n",
    "Below, we provide a cell for you to index into the raw data and listen to randomly chosen samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iGmsjnx_karw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 8117: \"which card would you like replaced\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "DATA_PATH = \".\"\n",
    "SYM_PATH = \".\"\n",
    "\n",
    "MODEL_PATH = f'{SYM_PATH}/trained_models'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "  os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
    "waveform_h5 = h5py.File(os.path.join(root, 'data.h5'), 'r')\n",
    "waveform_data = waveform_h5.get('waveforms')\n",
    "label_data = np.load(os.path.join(root, 'labels.npz'))\n",
    "assert len(waveform_data) == len(label_data['human_transcripts'])\n",
    "index = random.randint(0, len(waveform_data) - 1)\n",
    "w = waveform_data[f'{index}'][:]\n",
    "t = label_data['human_transcripts'][index]\n",
    "\n",
    "print('index {}: \"{}\"\\n'.format(index, t))\n",
    "Audio(w, rate=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3aU-u6laAzt"
   },
   "source": [
    "## **Task 1.1: Set up primary task data (5 Points)**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "Notice that the `__getitem__` returns 4 objects:\n",
    "- `inputs`: the padded log-Mel spectrogram features.\n",
    "- `input_lengths`: the true length of the unpadded spectrogram features.\n",
    "- `labels`: the padded character labels.\n",
    "- `label_lengths`: the true length of the unpadded character labels.\n",
    "\n",
    "These objects will be used for what we call our *primary* task: speech recognition. In later parts, we will use *auxiliary* tasks to perform multi-task learning toward boosting speech recognition.\n",
    "\n",
    "** Implement the `get_primary_task_data` method.** This will be used in the `__getitem__` method of `HarperValleyBank` and later its subclass for multi-task learning, and it is responsible for extracting log-Mel spectrogram features from the raw audio clips. Do not modify other methods. You should pass the sanity check at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QQ2x5IuxfuZf"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4155426855.py, line 218)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 218\u001b[1;36m\u001b[0m\n\u001b[1;33m    dtype=torch.float32)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from utils import (\n",
    "  prune_transcripts, pad_wav, pad_transcript_label, get_transcript_labels,\n",
    "  get_cer_per_sample)\n",
    "\n",
    "\n",
    "# HarperValleyBank character vocabulary\n",
    "VOCAB = [' ', \"'\", '~', '-', '.', '<', '>', '[', ']', 'a', 'b', 'c', 'd', 'e',\n",
    "         'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's',\n",
    "         't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "         \n",
    "SILENT_VOCAB = ['[baby]', '[ringing]', '[laughter]', '[kids]', '[music]', \n",
    "                '[noise]', '[unintelligible]', '[dogs]', '[cough]']\n",
    "\n",
    "\n",
    "class HarperValleyBank(Dataset):\n",
    "  \"\"\"Dataset to be used to train CTC, LAS, and MTL.\n",
    "  \n",
    "  Args:\n",
    "    root: string\n",
    "          path to the data files.\n",
    "    split: string (default: train)\n",
    "            choices: train | val | test\n",
    "            which split of data to load\n",
    "    n_mels: integer (default: 128)\n",
    "            number of mel frequencies\n",
    "    n_fft: integer (default: 256)\n",
    "            number of fourier components\n",
    "    win_length: integer (default: 256)\n",
    "                should be <= n_fft\n",
    "    hop_length: integer (default: 128)\n",
    "                number of frames to skip in between\n",
    "    wav_max_length: integer (default: 200)\n",
    "                    maximum number of timesteps in a waveform\n",
    "    transcript_max_length: integer (default: 200)\n",
    "                            maximum number of timesteps in a transcript\n",
    "    append_eos_token: boolean (default: False)\n",
    "                      add EOS token to the end of every transcription\n",
    "                      this is used for LAS (and LAS+CTC models)\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "      self, root, split='train', n_mels=128, n_fft=256, win_length=256, \n",
    "      hop_length=128, wav_max_length=200, transcript_max_length=200, \n",
    "      append_eos_token=False):\n",
    "    super().__init__()\n",
    "    print(f'> Constructing HarperValleyBank {split} dataset...')\n",
    "\n",
    "    self.label_data = np.load(os.path.join(root, 'labels.npz'))   \n",
    "    self.root = root\n",
    "    self.wav_max_length = wav_max_length\n",
    "    self.transcript_max_length = transcript_max_length\n",
    "\n",
    "    self.input_dim = n_mels\n",
    "    self.n_mels = n_mels\n",
    "    self.n_fft = n_fft\n",
    "    self.win_length = win_length\n",
    "    self.hop_length = hop_length\n",
    "\n",
    "    # Prune away very short examples.\n",
    "    # This returns a list of indices of examples longer than 3 words.\n",
    "    valid_indices = prune_transcripts(self.label_data['human_transcripts'])\n",
    "\n",
    "    # Decides which indices belong to which split.\n",
    "    train_indices, val_indices, test_indices = self.split_data(valid_indices)\n",
    "\n",
    "    if split == 'train':\n",
    "      indices = train_indices\n",
    "    elif split == 'val':\n",
    "      indices = val_indices\n",
    "    elif split == 'test':\n",
    "      indices = test_indices\n",
    "    else:\n",
    "      raise Exception(f'Split {split} not supported.')\n",
    "\n",
    "    raw_human_transcripts = self.label_data['human_transcripts'].tolist()\n",
    "    human_transcript_labels = get_transcript_labels(\n",
    "      raw_human_transcripts, VOCAB, SILENT_VOCAB)\n",
    "  \n",
    "    # Increment all indices by 4 to reserve the following special tokens:\n",
    "    #   0 for epsilon\n",
    "    #   1 for start-of-sentence (SOS)\n",
    "    #   2 for end-of-sentence (EOS)\n",
    "    #   3 for padding \n",
    "    num_special_tokens = 4\n",
    "    human_transcript_labels = [list(np.array(lab) + num_special_tokens) \n",
    "                                for lab in human_transcript_labels]\n",
    "    # CTC doesn't use SOS nor EOS; LAS doesn't use EPS but add anyway.\n",
    "    eps_index, sos_index, eos_index, pad_index = 0, 1, 2, 3\n",
    "\n",
    "    if append_eos_token:\n",
    "      # Ensert an EOS token to the end of all the labels.\n",
    "      # This is important for the LAS objective.\n",
    "      human_transcript_labels_ = []\n",
    "      for i in range(len(human_transcript_labels)):\n",
    "        new_label_i = human_transcript_labels[i] + [eos_index]\n",
    "        human_transcript_labels_.append(new_label_i)\n",
    "      human_transcript_labels = human_transcript_labels_\n",
    "    self.human_transcript_labels = human_transcript_labels\n",
    "  \n",
    "    # Include epsilon, SOS, and EOS tokens.\n",
    "    self.num_class = len(VOCAB) + len(SILENT_VOCAB) + num_special_tokens\n",
    "    self.num_labels = self.num_class  # These are interchangeable.\n",
    "    self.eps_index = eps_index\n",
    "    self.sos_index = sos_index\n",
    "    self.eos_index = eos_index\n",
    "    self.pad_index = pad_index # Use this index for padding.\n",
    "\n",
    "    self.indices = indices\n",
    "\n",
    "  def indices_to_chars(self, indices):\n",
    "    # indices: list of integers in vocab\n",
    "    # add special characters in front (since we did this above)\n",
    "    full_vocab = ['<eps>', '<sos>', '<eos>', '<pad>'] + VOCAB + SILENT_VOCAB\n",
    "    chars = [full_vocab[ind] for ind in indices]\n",
    "    return chars\n",
    "\n",
    "  def split_data(self, valid_indices, train_ratio = 0.8, val_ratio = 0.1):\n",
    "    \"\"\"Splits data into train, val, and test sets based on speaker. When \n",
    "    evaluating methods on the test split, we measure how well they generalize\n",
    "    to new (unseen) speakers.\n",
    "    \n",
    "    Concretely, this stores and returns indices belonging to each split.\n",
    "    \"\"\"\n",
    "    # Fix seed so everyone reproduces the same splits.\n",
    "    rs = np.random.RandomState(42)\n",
    "\n",
    "    speaker_ids = self.label_data['speaker_ids']\n",
    "    unique_speaker_ids = sorted(list(set(speaker_ids)))\n",
    "    unique_speaker_ids = np.array(unique_speaker_ids)\n",
    "\n",
    "    # Shuffle so the speaker IDs are distributed.\n",
    "    rs.shuffle(unique_speaker_ids)\n",
    "\n",
    "    num_speaker = len(unique_speaker_ids)\n",
    "    num_train = int(train_ratio * num_speaker)\n",
    "    num_val = int(val_ratio * num_speaker)\n",
    "    num_test = num_speaker - num_train - num_val\n",
    "\n",
    "    train_speaker_ids = unique_speaker_ids[:num_train]\n",
    "    val_speaker_ids = unique_speaker_ids[num_train:num_train+num_val]\n",
    "    test_speaker_ids = unique_speaker_ids[num_train+num_val:]\n",
    "\n",
    "    train_speaker_dict = dict(zip(train_speaker_ids, ['train'] * num_train))\n",
    "    val_speaker_dict = dict(zip(val_speaker_ids, ['val'] * num_val))\n",
    "    test_speaker_dict = dict(zip(test_speaker_ids, ['test'] * num_test))\n",
    "    speaker_dict = {**train_speaker_dict, **val_speaker_dict, \n",
    "                    **test_speaker_dict} \n",
    "\n",
    "    train_indices, val_indices, test_indices = [], [], []\n",
    "    for i in range(len(speaker_ids)):\n",
    "      speaker_id = speaker_ids[i]\n",
    "      if speaker_dict[speaker_id] == 'train':\n",
    "          train_indices.append(i)\n",
    "      elif speaker_dict[speaker_id] == 'val':\n",
    "          val_indices.append(i)\n",
    "      elif speaker_dict[speaker_id] == 'test':\n",
    "          test_indices.append(i)\n",
    "      else:\n",
    "          raise Exception('split not recognized.')\n",
    "\n",
    "    train_indices = np.array(train_indices)\n",
    "    val_indices = np.array(val_indices)\n",
    "    test_indices = np.array(test_indices)\n",
    "\n",
    "    # Make sure to only keep \"valid indices\" i.e. those with more than 4 \n",
    "    # words in the transcription.\n",
    "    train_indices = np.intersect1d(train_indices, valid_indices)\n",
    "    val_indices = np.intersect1d(val_indices, valid_indices)\n",
    "    test_indices = np.intersect1d(test_indices, valid_indices)\n",
    "\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "  def get_primary_task_data(self, index):\n",
    "    \"\"\"Returns audio and transcript information for a single utterance.\n",
    "\n",
    "    Args:\n",
    "      index: Index of an utterance.\n",
    "\n",
    "    Returns:\n",
    "      log melspectrogram, wav length, transcript label, transcript length\n",
    "    \"\"\"\n",
    "    input_feature = None\n",
    "    input_length = None\n",
    "    human_transcript_label = None\n",
    "    human_transcript_length = None\n",
    "\n",
    "    wav = self.waveform_data[f'{index}'][:] # An h5py file uses string keys.\n",
    "    sr = 8000 # We fix the sample rate for you.\n",
    "\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(1.1)\n",
    "    # - Compute the mel spectrogram of the audio crop.\n",
    "    # - Convert the mel spectrogram to log space and normalize it.\n",
    "    # - This is your primary task feature. Note that models will expect feature\n",
    "    #   inputs of shape (T, n_mels).\n",
    "    # - Pad the feature so that all features are fixed-length and\n",
    "    #   convert it into a tensor.\n",
    "    # - Likewise, retrieve and pad the corresponding transcript label sequence.\n",
    "    #\n",
    "    # Hint:\n",
    "    # - Refer to https://librosa.org/doc/latest/index.html.\n",
    "    # - Use `librosa.feature.melspectrogram` and `librosa.util.normalize`.\n",
    "    # - Make sure to use our provided sr, n_mels, n_fft, win_length, \n",
    "    # - and hop_length\n",
    "    # - utils.py has helpful padding functions.\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=wav, sr=sr, n_mels=self.n_mels, n_fft=self.n_fft, \n",
    "        win_length=self.win_length, hop_length=self.hop_length)\n",
    "\n",
    "    # Convert the mel spectrogram to log space and normalize it.\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    log_mel_spectrogram = librosa.util.normalize(log_mel_spectrogram)\n",
    "    \n",
    "    # Pad the feature so that all features are fixed-length.\n",
    "    try:\n",
    "        padding_size = max(0, self.wav_max_length - len(log_mel_spectrogram.T))\n",
    "        input_feature = torch.tensor(\n",
    "            np.pad(log_mel_spectrogram.T, ((0, padding_size)), (0, 0)), 'constant', constant_values=3),\n",
    "            dtype=torch.float32)\n",
    "    except:\n",
    "        print(\"Error at input_feature\")\n",
    "        print(self.wav_max_length - len(log_mel_spectrogram.T))\n",
    "    # input_feature = torch.clamp(input_feature, min=0)\n",
    "    # Retrieve and pad the corresponding transcript label sequence.\n",
    "    transcript_labels = self.human_transcript_labels[index]\n",
    "    human_transcript_length = len(transcript_labels)\n",
    "    transcript_labels = transcript_labels + [self.eos_index]  # Add EOS token\n",
    "    try:\n",
    "        padding_size_label = max(0, self.transcript_max_length - len(transcript_labels))\n",
    "        human_transcript_label = torch.tensor(\n",
    "            np.pad(transcript_labels, (0, padding_size_label), 'constant', constant_values=3),\n",
    "            dtype=torch.long)\n",
    "    except:\n",
    "        print(\"Error at human_label\")\n",
    "        print(self.transcript_max_length - len(transcript_labels))\n",
    "\n",
    "    input_length = len(log_mel_spectrogram.T)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "\n",
    "    return input_feature, input_length, human_transcript_label, human_transcript_length\n",
    "\n",
    "  def load_waveforms(self):\n",
    "    # Make a file pointer to waveforms file.\n",
    "    waveform_h5 = h5py.File(os.path.join(self.root, 'data.h5'), 'r')\n",
    "    self.waveform_data = waveform_h5.get('waveforms')\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"Serves primary task data for a single utterance.\"\"\"\n",
    "    if not hasattr(self, 'waveform_data'):\n",
    "      # Do this in __getitem__ function so we enable multiprocessing.\n",
    "      self.load_waveforms()\n",
    "    index = int(self.indices[index])\n",
    "    return self.get_primary_task_data(index)\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"Returns total number of utterances in the dataset.\"\"\"\n",
    "    return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsxCZGiucJRc"
   },
   "source": [
    "**Sanity check.** Let's check that your dataset implementation is correct. This will be important to properly run our experiments in later parts. In particular, make sure your `__getitem__` and `__len__` are implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lTi-avLRcIuE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Constructing HarperValleyBank train dataset...\n",
      "> Constructing HarperValleyBank val dataset...\n",
      "> Constructing HarperValleyBank test dataset...\n",
      "200\n",
      "\n",
      "Validated dataset class implementation!\n"
     ]
    }
   ],
   "source": [
    " # Do not modify.\n",
    "root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
    "train_dataset = HarperValleyBank(root, split='train')\n",
    "val_dataset = HarperValleyBank(root, split='val')\n",
    "test_dataset = HarperValleyBank(root, split='test')\n",
    "\n",
    "assert len(train_dataset) == 10402\n",
    "assert len(val_dataset) == 679\n",
    "assert len(test_dataset) == 2854 \n",
    "\n",
    "input, input_length, label, label_length = train_dataset.__getitem__(224)\n",
    "\n",
    "\n",
    "assert input.size() == torch.Size([train_dataset.wav_max_length, train_dataset.n_mels])\n",
    "assert input_length == 92\n",
    "assert label_length == 26\n",
    "print('\\nValidated dataset class implementation!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrjMYi5U6RkM"
   },
   "source": [
    "# Part 2: Connectionist Temporal Classification (CTC) Neural Network\n",
    "\n",
    "Our first experiment will be a [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) (Graves et al.) model trained on our primary task of speech recognition.\n",
    "\n",
    "As an overview, given an input matrix of shape `batch_size x sequence_length x feature_dim`, the network encodes the input speech features with an LSTM, producing a tensor of shape `batch_size x sequence_length x hidden_dim`. Using an additional linear layer, we transform this to `batch_size x sequence_length x vocab_size`, representing the probability of transcribing each character in the vocabulary at each time step. This is directly given to the CTC loss function.\n",
    "\n",
    "We will use [Weights & Biases](https://wandb.ai) to log loss curves and character error rates (CER) in the cloud. You can create a free account [here](https://wandb.ai/site)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMBltzWIRWRI"
   },
   "source": [
    "## **CTC Network**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "You will use the CTC objective to train your network. Previously, you implemented the CTC loss function from scratch. For this assignment, you may use PyTorch's implementation. Filling out this section will be necessary to carry out later experiments.\n",
    "\n",
    "** Fill out `get_ctc_loss` using `F.ctc_loss`.**\n",
    "\n",
    "** Read through the starter code and fill out the `forward` pass of `CTCEncoderDecoder`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ua4VUy5fwlEZ"
   },
   "outputs": [],
   "source": [
    "def get_ctc_loss(\n",
    "    log_probs, targets, input_lengths, target_lengths, blank=0):\n",
    "  \"\"\"Connectionist Temporal Classification objective function.\"\"\"\n",
    "  ctc_loss = None\n",
    "  log_probs = log_probs.contiguous()\n",
    "  targets = targets.long()\n",
    "  input_lengths = input_lengths.long()\n",
    "  target_lengths = target_lengths.long()\n",
    "  ############################ START OF YOUR CODE ############################\n",
    "  # TODO(2.1)\n",
    "  # Hint:\n",
    "  # - `F.ctc_loss`: https://pytorch.org/docs/stable/nn.functional.html#ctc-loss\n",
    "  # - log_probs is passed in with shape (batch_size, input_length, num_classes).\n",
    "  # - Notice that `F.ctc_loss` expects log_probs of shape\n",
    "  #   (input_length, batch_size, num_classes)\n",
    "  # - Turn on zero_infinity.\n",
    "  # Transpose log_probs to match the shape expected by F.ctc_loss\n",
    "  log_probs = log_probs.transpose(0, 1)\n",
    "\n",
    "  # Compute CTC loss using PyTorch's F.ctc_loss\n",
    "  ctc_loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=blank, zero_infinity=True)\n",
    "  ############################# END OF YOUR CODE #############################\n",
    "  return ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fb6IkAvR5xkY"
   },
   "outputs": [],
   "source": [
    "class CTCEncoderDecoder(nn.Module):\n",
    "  \"\"\"\n",
    "  Encoder-Decoder model trained with CTC objective.\n",
    "\n",
    "  Args:\n",
    "    input_dim: integer\n",
    "                number of input features\n",
    "    num_class: integer\n",
    "                size of transcription vocabulary\n",
    "    num_layers: integer (default: 2)\n",
    "                number of layers in encoder LSTM\n",
    "    hidden_dim: integer (default: 128)\n",
    "                number of hidden dimensions for encoder LSTM\n",
    "    bidirectional: boolean (default: True)\n",
    "                    is the encoder LSTM bidirectional?\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "      self, input_dim, num_class, num_layers=2, hidden_dim=128,\n",
    "      bidirectional=True):\n",
    "    super().__init__()\n",
    "    # Note: `batch_first=True` argument implies the inputs to the LSTM should\n",
    "    # be of shape (batch_size x T x D) instead of (T x batch_size x D).\n",
    "    self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            bidirectional=bidirectional, batch_first=True)\n",
    "    self.decoder = nn.Linear(hidden_dim * 2, num_class)\n",
    "    self.input_dim = input_dim\n",
    "    self.num_class = num_class\n",
    "    self.num_layers = num_layers\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embedding_dim = hidden_dim * num_layers * 2 * \\\n",
    "                          (2 if bidirectional else 1)\n",
    "\n",
    "  def combine_h_and_c(self, h, c):\n",
    "    \"\"\"Combine the signals from RNN hidden and cell states.\"\"\"\n",
    "    batch_size = h.size(1)\n",
    "    h = h.permute(1, 0, 2).contiguous()\n",
    "    c = c.permute(1, 0, 2).contiguous()\n",
    "    h = h.view(batch_size, -1)\n",
    "    c = c.view(batch_size, -1)\n",
    "    return torch.cat([h, c], dim=1)  # just concatenate\n",
    "\n",
    "  def forward(self, inputs, input_lengths):\n",
    "    batch_size, max_length, _ = inputs.size()\n",
    "    # `torch.nn.utils.rnn.pack_padded_sequence` collapses padded sequences\n",
    "    # to a contiguous chunk\n",
    "    inputs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "        inputs, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "    log_probs = None\n",
    "    h, c = None, None\n",
    "    \n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(2.1)\n",
    "    # Hint:\n",
    "    # - Refer to https://pytorch.org/docs/stable/nn.html\n",
    "    # - Use `self.encoder` to get the encodings output which is of shape\n",
    "    #   (batch_size, max_length, num_directions*hidden_dim) and the\n",
    "    #   hidden states and cell states which are both of shape\n",
    "    #   (batch_size, num_layers*num_directions, hidden_dim)\n",
    "    # - Pad outputs with `0.` using `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "    #   (turn on batch_first and set total_length as max_length).\n",
    "    # - Apply 50% dropout.\n",
    "    # - Use `self.decoder` to take the embeddings sequence and return\n",
    "    #   probabilities for each character.\n",
    "    # - Make sure to then convert to log probabilities.\n",
    "      \n",
    "    # Apply 50% dropout to the data within the packed sequence\n",
    "    encoding, (h, c) = self.encoder(inputs)  # encoding is a PackedSequence\n",
    "    output_unpacked, len_unpacked = torch.nn.utils.rnn.pad_packed_sequence(encoding, batch_first=True, total_length=max_length)  \n",
    "    \n",
    "    dropped = F.dropout(output_unpacked, p=0.5, training=self.training)\n",
    "\n",
    "    # Apply linear layer (self.decoder) to get log probabilities\n",
    "    log_probs = F.log_softmax(self.decoder(dropped), dim=2)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    \n",
    "    # The extracted embedding is not used for the ASR task but will be\n",
    "    # needed for other auxiliary tasks.\n",
    "    embedding = self.combine_h_and_c(h, c)\n",
    "    return log_probs, embedding\n",
    "\n",
    "  def get_loss(\n",
    "      self, log_probs, targets, input_lengths, target_lengths, blank=0):\n",
    "    return get_ctc_loss(\n",
    "        log_probs, targets, input_lengths, target_lengths, blank)\n",
    "\n",
    "  def decode(self, log_probs, input_lengths, labels, label_lengths,\n",
    "             sos_index, eos_index, pad_index, eps_index):\n",
    "    # Use greedy decoding.\n",
    "    decoded = torch.argmax(log_probs, dim=2)\n",
    "    batch_size = decoded.size(0)\n",
    "    # Collapse each decoded sequence using CTC rules.\n",
    "    hypotheses = []\n",
    "    for i in range(batch_size):\n",
    "      hypotheses_i = self.ctc_collapse(decoded[i], input_lengths[i].item(),\n",
    "                                       blank_index=eps_index)\n",
    "      hypotheses.append(hypotheses_i)\n",
    "\n",
    "    hypothesis_lengths = input_lengths.cpu().numpy().tolist()\n",
    "    if labels is None: # Run at inference time.\n",
    "      references, reference_lengths = None, None\n",
    "    else:\n",
    "      references = labels.cpu().numpy().tolist()\n",
    "      reference_lengths = label_lengths.cpu().numpy().tolist()\n",
    "\n",
    "    return hypotheses, hypothesis_lengths, references, reference_lengths\n",
    "\n",
    "  def ctc_collapse(self, seq, seq_len, blank_index=0):\n",
    "    result = []\n",
    "    for i, tok in enumerate(seq[:seq_len]):\n",
    "      if tok.item() != blank_index:  # remove blanks\n",
    "        if i != 0 and tok.item() == seq[i-1].item():  # remove dups\n",
    "          pass\n",
    "        else:\n",
    "          result.append(tok.item())\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nnZCcWpLTzm"
   },
   "source": [
    "## **Introduction to PyTorch Lightning**\n",
    "\n",
    "**Walkthrough**\n",
    "\n",
    "*This section is a walkthrough and will not require any code or answers.* We will use [PyTorch Lightning](https://www.pytorchlightning.ai/), a lightweight wrapper framework for PyTorch, to run our experiments. You can learn more about the lightning toolkit [here](https://github.com/PyTorchLightning/pytorch-lightning). As a short introduction, Pytorch Lightning is a scaffold for training deep learning models. It handles a lot of the usual pipeline for you (e.g. looping over the training set, calling your optimizer). It has several callback handlers you can overwrite to specify your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2fD95O2MLRWN"
   },
   "outputs": [],
   "source": [
    "# Do not modify.\n",
    "\n",
    "class LightningCTC(pl.LightningModule):\n",
    "  \"\"\"PyTorch Lightning class for training a CTC model.\n",
    "\n",
    "  Args:\n",
    "    n_mels: number of mel frequencies. (default: 128)          \n",
    "    n_fft: number of fourier features. (default: 256)          \n",
    "    win_length: number of frames in a window. (default: 256)              \n",
    "    hop_length: number of frames to hop in computing spectrogram. (default: 128)               \n",
    "    wav_max_length: max number of timesteps in a waveform spectrogram. (default: 200)                  \n",
    "    transcript_max_length: max number of characters in decoded transcription. (default: 200)                         \n",
    "    learning_rate: learning rate for Adam optimizer. (default: 1e-3)                  \n",
    "    batch_size: batch size used in optimization and evaluation. (default: 256)               \n",
    "    weight_decay: weight decay for Adam optimizer. (default: 1e-5)               \n",
    "    encoder_num_layers: number of layers in LSTM encoder. (default: 2)                       \n",
    "    encoder_hidden_dim: number of hidden dimensions in LSTM encoder. (default: 256)\n",
    "    encoder_bidirectional: directionality of LSTM encoder. (default: True)                         \n",
    "  \"\"\"\n",
    "  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n",
    "               wav_max_length=200, transcript_max_length=200, \n",
    "               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n",
    "               encoder_num_layers=2, encoder_hidden_dim=256, \n",
    "               encoder_bidirectional=True):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.n_mels = n_mels\n",
    "    self.n_fft = n_fft\n",
    "    self.win_length = win_length\n",
    "    self.hop_length = hop_length\n",
    "    self.lr = learning_rate\n",
    "    self.batch_size = batch_size\n",
    "    self.weight_decay = weight_decay\n",
    "    self.wav_max_length = wav_max_length\n",
    "    self.transcript_max_length = transcript_max_length\n",
    "    self.train_dataset, self.val_dataset, self.test_dataset = \\\n",
    "      self.create_datasets()\n",
    "    self.encoder_num_layers = encoder_num_layers\n",
    "    self.encoder_hidden_dim = encoder_hidden_dim\n",
    "    self.encoder_bidirectional = encoder_bidirectional\n",
    "    self.validation_step_outputs = []\n",
    "    self.test_step_outputs = []\n",
    "\n",
    "    # Instantiate the CTC encoder/decoder.\n",
    "    self.model = self.create_model()\n",
    "\n",
    "  def create_model(self):\n",
    "    model = CTCEncoderDecoder(\n",
    "      self.train_dataset.input_dim,\n",
    "      self.train_dataset.num_class,\n",
    "      num_layers=self.encoder_num_layers,\n",
    "      hidden_dim=self.encoder_hidden_dim,\n",
    "      bidirectional=self.encoder_bidirectional)\n",
    "    return model\n",
    "\n",
    "  def create_datasets(self):\n",
    "    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
    "    train_dataset = HarperValleyBank(\n",
    "        root, split='train', n_mels=self.n_mels, n_fft=self.n_fft, \n",
    "        win_length=self.win_length, hop_length=self.hop_length,\n",
    "        wav_max_length=self.wav_max_length,\n",
    "        transcript_max_length=self.transcript_max_length,\n",
    "        append_eos_token=False)\n",
    "    val_dataset = HarperValleyBank(\n",
    "        root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "        win_length=self.win_length, hop_length=self.hop_length, \n",
    "        wav_max_length=self.wav_max_length,\n",
    "        transcript_max_length=self.transcript_max_length,\n",
    "        append_eos_token=False) \n",
    "    test_dataset = HarperValleyBank(\n",
    "        root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "        win_length=self.win_length, hop_length=self.hop_length,\n",
    "        wav_max_length=self.wav_max_length,\n",
    "        transcript_max_length=self.transcript_max_length,\n",
    "        append_eos_token=False) \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optim = torch.optim.AdamW(self.model.parameters(),\n",
    "                              lr=self.lr, weight_decay=self.weight_decay)\n",
    "    return [optim], [] # <-- put scheduler in here if you want to use one\n",
    "\n",
    "  def get_loss(self, log_probs, input_lengths, labels, label_lengths):\n",
    "    loss = self.model.get_loss(log_probs, labels, input_lengths, label_lengths,\n",
    "                                blank=self.train_dataset.eps_index)\n",
    "    return loss\n",
    "\n",
    "  def forward(self, inputs, input_lengths, labels, label_lengths):\n",
    "    log_probs, embedding = self.model(inputs, input_lengths)\n",
    "    return log_probs, embedding\n",
    "\n",
    "  def get_primary_task_loss(self, batch, split='train'):\n",
    "    \"\"\"Returns ASR model losses, metrics, and embeddings for a batch.\"\"\"\n",
    "    inputs, input_lengths = batch[0], batch[1]\n",
    "    labels, label_lengths = batch[2], batch[3]\n",
    "\n",
    "    if split == 'train':\n",
    "      log_probs, embedding = self.forward(\n",
    "          inputs, input_lengths, labels, label_lengths)\n",
    "    else:\n",
    "      # do not pass labels to not teacher force after training\n",
    "      log_probs, embedding = self.forward(\n",
    "          inputs, input_lengths, None, None)\n",
    "\n",
    "    loss = self.get_loss(log_probs, input_lengths, labels, label_lengths)\n",
    "\n",
    "    # Compute CER (no gradient necessary).\n",
    "    with torch.no_grad():\n",
    "      hypotheses, hypothesis_lengths, references, reference_lengths = \\\n",
    "        self.model.decode(\n",
    "            log_probs, input_lengths, labels, label_lengths,\n",
    "            self.train_dataset.sos_index,\n",
    "            self.train_dataset.eos_index,\n",
    "            self.train_dataset.pad_index,\n",
    "            self.train_dataset.eps_index)\n",
    "      cer_per_sample = get_cer_per_sample(\n",
    "          hypotheses, hypothesis_lengths, references, reference_lengths)\n",
    "      cer = cer_per_sample.mean()\n",
    "      metrics = {f'{split}_loss': loss, f'{split}_cer': cer}\n",
    "\n",
    "    return loss, metrics, embedding\n",
    "\n",
    "  # Overwrite TRAIN\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss, metrics, _ = self.get_primary_task_loss(batch, split='train')\n",
    "    self.log_dict(metrics)\n",
    "    # self.log('train_loss', loss, prog_bar=True, on_step=True)\n",
    "    # self.log('train_cer', metrics['train_cer'], prog_bar=True, on_step=True)\n",
    "    return loss\n",
    "\n",
    "  # Overwrite VALIDATION: get next minibatch\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss, metrics, _ = self.get_primary_task_loss(batch, split='val')\n",
    "    self.validation_step_outputs.append(metrics)\n",
    "    return metrics\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    loss, metrics, _ = self.get_primary_task_loss(batch, split='test')\n",
    "    self.test_step_outputs.append(metrics)\n",
    "    return metrics\n",
    "\n",
    "  # Overwrite: e.g. accumulate stats (avg over CER and loss)\n",
    "  def on_validation_epoch_end(self):\n",
    "    \"\"\"Called at the end of validation step to aggregate outputs.\"\"\"\n",
    "    # outputs is list of metrics from every validation_step (over a\n",
    "    # validation epoch).\n",
    "    metrics = { \n",
    "      # important that these are torch Tensors!\n",
    "      'val_loss': torch.tensor([elem['val_loss']\n",
    "                                for elem in self.validation_step_outputs]).float().mean(),\n",
    "      'val_cer': torch.tensor([elem['val_cer']\n",
    "                                for elem in self.validation_step_outputs]).float().mean()\n",
    "    }\n",
    "    # self.log('val_loss', metrics['val_loss'], prog_bar=True)\n",
    "    # self.log('val_cer', metrics['val_cer'], prog_bar=True)\n",
    "    self.validation_step_outputs.clear()  # free memory\n",
    "    self.log_dict(metrics)\n",
    "\n",
    "  def on_test_epoch_end(self):\n",
    "    metrics = { \n",
    "      'test_loss': torch.tensor([elem['test_loss']\n",
    "                                  for elem in self.test_step_outputs]).float().mean(),\n",
    "      'test_cer': torch.tensor([elem['test_cer']\n",
    "                                for elem in self.test_step_outputs]).float().mean()\n",
    "    }\n",
    "    self.test_step_outputs.clear()\n",
    "    self.log_dict(metrics)\n",
    "    \n",
    "  def train_dataloader(self):\n",
    "    # - important to shuffle to not overfit!\n",
    "    # - drop the last batch to preserve consistent batch sizes\n",
    "    loader = DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                        shuffle=True, pin_memory=True, drop_last=True)\n",
    "    return loader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    loader = DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                        shuffle=False, pin_memory=True)\n",
    "    return loader\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    loader = DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
    "                        shuffle=False, pin_memory=True)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY6Sl-rH950z"
   },
   "source": [
    "## **Task 2.1: Train a network with CTC [15 Points]**\n",
    "\n",
    "**Training & Written Response**\n",
    "\n",
    "Go to **Runtime** > **Change runtime type** and set **Hardware accelerator** to **GPU**.\n",
    "\n",
    "This section will be graded based on 1) your model's performance in regards to loss plots and CER plots and 2) your response for qualitative assessments of your plots.\n",
    "\n",
    "** Train the CTC network with the default hyperparameters we provide.** \n",
    "\n",
    "With batch size 128, one epoch of optimizing CTC takes roughly 3 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n",
    "\n",
    "** Paste screenshots from your Weights & Biases dashboard of your loss curve and CER curve in the cell marked \"Plots\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "viZWdpyhJkX0"
   },
   "outputs": [],
   "source": [
    "WANDB_NAME = 'dacphuc1993' # Fill in your Weights & Biases ID here.\n",
    "\n",
    "def run(system, config, ckpt_dir, epochs=1, monitor_key='val_loss', \n",
    "        use_gpu=False, seed=1337):\n",
    "  random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  np.random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "  SystemClass = globals()[system]\n",
    "  system = SystemClass(**config)\n",
    "\n",
    "  checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(MODEL_PATH, ckpt_dir),\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=monitor_key, \n",
    "    mode='min')\n",
    "  \n",
    "  wandb.init(project='cs224s', entity=WANDB_NAME, name=ckpt_dir, \n",
    "             config=config, sync_tensorboard=True)\n",
    "  wandb_logger = WandbLogger()\n",
    "  \n",
    "  if use_gpu:\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1, \n",
    "        max_epochs=epochs, \n",
    "        min_epochs=epochs, \n",
    "        enable_checkpointing=True,\n",
    "        callbacks=checkpoint_callback, \n",
    "        logger=wandb_logger)\n",
    "  else:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs, min_epochs=epochs, enable_checkpointing=True,\n",
    "        callbacks=checkpoint_callback, logger=wandb_logger)\n",
    "  \n",
    "  trainer.fit(system)\n",
    "  result = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2H9tt8gSKVH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Constructing HarperValleyBank train dataset...\n",
      "> Constructing HarperValleyBank val dataset...\n",
      "> Constructing HarperValleyBank test dataset...\n",
      "Epoch 0:  40%|                                                                                          | 64/162 [08:21<12:47,  0.13it/s, v_num=z26r]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2157: UserWarning: Run (ynzcz26r) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cv1m7696) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train_cer</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_cer</td><td>1.0</td></tr><tr><td>train_loss</td><td>3.01853</td></tr><tr><td>trainer/global_step</td><td>49</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ctc</strong> at: <a href='https://wandb.ai/dacphuc1993/cs224s/runs/cv1m7696' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/runs/cv1m7696</a><br/> View job at <a href='https://wandb.ai/dacphuc1993/cs224s/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyODg5MzA0MQ==/version_details/v1' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyODg5MzA0MQ==/version_details/v1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240110_220332-cv1m7696\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cv1m7696). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Phuc\\Desktop\\CS224S-slp\\HW3\\wandb\\run-20240110_220705-cukyquh6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dacphuc1993/cs224s/runs/cukyquh6' target=\"_blank\">ctc</a></strong> to <a href='https://wandb.ai/dacphuc1993/cs224s' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dacphuc1993/cs224s' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dacphuc1993/cs224s/runs/cukyquh6' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/runs/cukyquh6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | CTCEncoderDecoder | 2.4 M \n",
      "--------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.568     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                                                                                                                        | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 162/162 [02:41<00:00,  1.01it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:00<00:04,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:01<00:05,  1.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:02<00:05,  1.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:02<00:04,  1.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:03<00:04,  1.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:04<00:03,  1.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:05<00:02,  1.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:05<00:02,  1.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:06<00:01,  1.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:07<00:00,  1.35it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:07<00:00,  1.39it/s]\u001b[A\n",
      "Epoch 0: 100%|| 162/162 [02:49<00:00,  0.96it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 162: 'val_loss' reached 3.01178 (best 3.01178), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=0-step=162.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 162/162 [02:39<00:00,  1.01it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:00<00:04,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:01<00:05,  1.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:02<00:05,  1.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:02<00:04,  1.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:03<00:04,  1.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:04<00:03,  1.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:05<00:02,  1.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:05<00:02,  1.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:06<00:01,  1.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:07<00:00,  1.36it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:07<00:00,  1.40it/s]\u001b[A\n",
      "Epoch 1: 100%|| 162/162 [02:47<00:00,  0.97it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 324: 'val_loss' reached 2.96074 (best 2.96074), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=1-step=324.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 162/162 [02:39<00:00,  1.01it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:00<00:05,  1.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:01<00:05,  1.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:02<00:05,  1.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:02<00:05,  1.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:03<00:04,  1.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:04<00:03,  1.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:05<00:03,  1.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:06<00:02,  1.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:06<00:01,  1.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:07<00:00,  1.31it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:08<00:00,  1.36it/s]\u001b[A\n",
      "Epoch 2: 100%|| 162/162 [02:48<00:00,  0.96it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 486: 'val_loss' reached 2.92703 (best 2.92703), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=2-step=486.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 162/162 [02:41<00:00,  1.01it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:00<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:01<00:06,  1.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:02<00:06,  1.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:03<00:05,  1.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:03<00:04,  1.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:05<00:04,  1.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:06<00:03,  1.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:07<00:02,  1.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:08<00:01,  1.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:08<00:00,  1.13it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:09<00:00,  1.17it/s]\u001b[A\n",
      "Epoch 3: 100%|| 162/162 [02:50<00:00,  0.95it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 648: 'val_loss' reached 2.78909 (best 2.78909), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=3-step=648.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 162/162 [02:45<00:00,  0.98it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:00<00:07,  1.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:01<00:07,  1.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:02<00:06,  1.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:03<00:06,  1.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:04<00:05,  1.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:05<00:04,  1.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:07<00:04,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:08<00:03,  0.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:09<00:02,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:10<00:01,  0.93it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:11<00:00,  0.97it/s]\u001b[A\n",
      "Epoch 4: 100%|| 162/162 [02:57<00:00,  0.91it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 810: 'val_loss' reached 2.43153 (best 2.43153), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=4-step=810.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 162/162 [03:08<00:00,  0.86it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:00<00:06,  1.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:01<00:07,  1.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:02<00:07,  1.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:03<00:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:04<00:05,  1.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:05<00:04,  1.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:07<00:04,  1.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:08<00:03,  1.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:09<00:02,  0.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:10<00:01,  0.98it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:10<00:00,  1.02it/s]\u001b[A\n",
      "Epoch 5: 100%|| 162/162 [03:19<00:00,  0.81it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 972: 'val_loss' reached 2.03561 (best 2.03561), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=5-step=972.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|| 162/162 [03:17<00:00,  0.82it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:00<00:07,  1.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:01<00:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:02<00:07,  1.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:03<00:06,  1.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:05<00:06,  1.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:06<00:05,  1.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:07<00:04,  0.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:08<00:03,  0.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:09<00:02,  0.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:09<00:00,  1.00it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:10<00:00,  1.04it/s]\u001b[A\n",
      "Epoch 6: 100%|| 162/162 [03:28<00:00,  0.78it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1134: 'val_loss' reached 1.69656 (best 1.69656), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=6-step=1134.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|| 162/162 [03:49<00:00,  0.71it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:12,  0.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:02<00:11,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:04<00:10,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:05<00:09,  0.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:06<00:07,  0.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:07<00:06,  0.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:08<00:05,  0.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:10<00:03,  0.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:11<00:02,  0.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:12<00:01,  0.80it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:13<00:00,  0.83it/s]\u001b[A\n",
      "Epoch 7: 100%|| 162/162 [04:03<00:00,  0.67it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1296: 'val_loss' reached 1.47724 (best 1.47724), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=7-step=1296.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|| 162/162 [06:28<00:00,  0.42it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:13,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:02<00:12,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:04<00:11,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:05<00:09,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:06<00:08,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:08<00:06,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:09<00:05,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:10<00:04,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:12<00:02,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:13<00:01,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:14<00:00,  0.78it/s]\u001b[A\n",
      "Epoch 8: 100%|| 162/162 [06:42<00:00,  0.40it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1458: 'val_loss' reached 1.34289 (best 1.34289), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=8-step=1458.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|| 162/162 [04:39<00:00,  0.58it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:12,  0.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:02<00:12,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:04<00:11,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:05<00:09,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:07<00:08,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:08<00:07,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:09<00:05,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:11<00:04,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:12<00:02,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:13<00:01,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:14<00:00,  0.75it/s]\u001b[A\n",
      "Epoch 9: 100%|| 162/162 [04:54<00:00,  0.55it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1620: 'val_loss' reached 1.15262 (best 1.15262), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=9-step=1620.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|| 162/162 [08:17<00:00,  0.33it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:02<00:22,  0.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:05<00:24,  0.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:08<00:21,  0.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:10<00:17,  0.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:12<00:14,  0.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:14<00:11,  0.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:16<00:09,  0.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:18<00:07,  0.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:21<00:04,  0.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:23<00:02,  0.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:24<00:00,  0.45it/s]\u001b[A\n",
      "Epoch 10: 100%|| 162/162 [08:43<00:00,  0.31it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1782: 'val_loss' reached 1.06474 (best 1.06474), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=10-step=1782.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|| 162/162 [06:37<00:00,  0.41it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:15,  0.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:04<00:18,  0.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:06<00:16,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:08<00:14,  0.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:09<00:11,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:11<00:09,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:13<00:07,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:15<00:05,  0.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:16<00:03,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:18<00:01,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:19<00:00,  0.57it/s]\u001b[A\n",
      "Epoch 11: 100%|| 162/162 [06:56<00:00,  0.39it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 1944: 'val_loss' reached 0.96763 (best 0.96763), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=11-step=1944.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 162/162 [06:40<00:00,  0.40it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:18,  0.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:04<00:20,  0.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:07<00:19,  0.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:10<00:18,  0.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:12<00:15,  0.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:14<00:12,  0.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:16<00:09,  0.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:19<00:07,  0.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:21<00:04,  0.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:22<00:02,  0.44it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:24<00:00,  0.46it/s]\u001b[A\n",
      "Epoch 12: 100%|| 162/162 [07:05<00:00,  0.38it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2106: 'val_loss' reached 0.91534 (best 0.91534), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=12-step=2106.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|| 162/162 [06:57<00:00,  0.39it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:13,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:02<00:13,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:04<00:12,  0.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:06<00:10,  0.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:07<00:09,  0.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:09<00:07,  0.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:10<00:06,  0.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:12<00:04,  0.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:13<00:03,  0.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:15<00:01,  0.66it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:16<00:00,  0.69it/s]\u001b[A\n",
      "Epoch 13: 100%|| 162/162 [07:13<00:00,  0.37it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 2268: 'val_loss' reached 0.83296 (best 0.83296), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=13-step=2268.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|| 162/162 [07:19<00:00,  0.37it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:02<00:25,  0.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:04<00:20,  0.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:06<00:17,  0.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:08<00:14,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:10<00:12,  0.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:11<00:09,  0.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:13<00:07,  0.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:15<00:05,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:17<00:03,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:18<00:01,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:19<00:00,  0.56it/s]\u001b[A\n",
      "Epoch 14: 100%|| 162/162 [07:39<00:00,  0.35it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 2430: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|| 162/162 [05:51<00:00,  0.46it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:14,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:03<00:14,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:04<00:13,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:06<00:11,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:08<00:09,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:09<00:08,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:11<00:06,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:13<00:04,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:14<00:03,  0.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:16<00:01,  0.62it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:17<00:00,  0.65it/s]\u001b[A\n",
      "Epoch 15: 100%|| 162/162 [06:08<00:00,  0.44it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 2592: 'val_loss' reached 0.76068 (best 0.76068), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=15-step=2592.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|| 162/162 [07:05<00:00,  0.38it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:02<00:21,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:03<00:17,  0.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:05<00:15,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:07<00:12,  0.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:09<00:10,  0.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:10<00:08,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:12<00:07,  0.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:13<00:05,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:15<00:03,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:16<00:01,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:17<00:00,  0.62it/s]\u001b[A\n",
      "Epoch 16: 100%|| 162/162 [07:23<00:00,  0.37it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 2754: 'val_loss' reached 0.72346 (best 0.72346), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=16-step=2754.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|| 162/162 [06:00<00:00,  0.45it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:01<00:15,  0.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:03<00:15,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:05<00:13,  0.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:06<00:11,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:08<00:10,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:10<00:08,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:12<00:06,  0.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:13<00:05,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:15<00:03,  0.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:16<00:01,  0.60it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:17<00:00,  0.63it/s]\u001b[A\n",
      "Epoch 17: 100%|| 162/162 [06:18<00:00,  0.43it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 2916: 'val_loss' reached 0.68555 (best 0.68555), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=17-step=2916.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|| 162/162 [05:31<00:00,  0.49it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:02<00:23,  0.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:04<00:19,  0.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:06<00:16,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:07<00:13,  0.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:09<00:11,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:11<00:09,  0.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:13<00:07,  0.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:15<00:05,  0.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:17<00:03,  0.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:19<00:01,  0.52it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:20<00:00,  0.53it/s]\u001b[A\n",
      "Epoch 18: 100%|| 162/162 [05:52<00:00,  0.46it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 3078: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 162/162 [07:37<00:00,  0.35it/s, v_num=quh6]\n",
      "Validation: |                                                                                                                                                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                                                                                        | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|                                                                                                                                     | 1/11 [00:02<00:23,  0.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|                                                                                                                        | 2/11 [00:04<00:20,  0.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|                                                                                                           | 3/11 [00:06<00:18,  0.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|                                                                                             | 4/11 [00:08<00:15,  0.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|                                                                                | 5/11 [00:10<00:12,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|                                                                  | 6/11 [00:12<00:10,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|                                                     | 7/11 [00:14<00:08,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|                                        | 8/11 [00:16<00:06,  0.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|                          | 9/11 [00:18<00:04,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|             | 10/11 [00:20<00:02,  0.49it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 11/11 [00:21<00:00,  0.51it/s]\u001b[A\n",
      "Epoch 19: 100%|| 162/162 [08:00<00:00,  0.34it/s, v_num=quh6]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 3240: 'val_loss' reached 0.67335 (best 0.67335), saving model to 'C:\\\\Users\\\\Phuc\\\\Desktop\\\\CS224S-slp\\\\HW3\\\\trained_models\\\\ctc\\\\epoch=19-step=3240.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 162/162 [08:00<00:00,  0.34it/s, v_num=quh6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at C:\\Users\\Phuc\\Desktop\\CS224S-slp\\HW3\\trained_models\\ctc\\epoch=19-step=3240.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Phuc\\Desktop\\CS224S-slp\\HW3\\trained_models\\ctc\\epoch=19-step=3240.ckpt\n",
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 45/45 [01:10<00:00,  0.64it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test_cer            0.22882866859436035\n",
      "        test_loss           0.7544283270835876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'n_mels': 128, \n",
    "    'n_fft': 256,\n",
    "    'win_length': 256,\n",
    "    'hop_length': 128,\n",
    "    'wav_max_length': 2048, \n",
    "    'transcript_max_length': 500,\n",
    "    'learning_rate': 1e-3, \n",
    "    'batch_size': 64, \n",
    "    'weight_decay': 0, \n",
    "    'encoder_num_layers': 2, \n",
    "    'encoder_hidden_dim': 256, \n",
    "    'encoder_bidirectional': True,\n",
    "}\n",
    "\n",
    "# NOTES:\n",
    "# -----\n",
    "# - PyTorch Lightning will run 2 steps of validation prior to the first \n",
    "#   epoch to sanity check that validation works (otherwise you \n",
    "#   might waste an epoch training and error).\n",
    "# - The progress bar updates very slowly, the model is likely \n",
    "#   training even if it doesn't look like it is. \n",
    "# - Wandb will generate a URL for you where all the metrics will be logged.\n",
    "# - Every validation loop, the best performing model is saved.\n",
    "# - After training, the system will evaluate performance on the test set.\n",
    "run(system=\"LightningCTC\", config=config, ckpt_dir='ctc', epochs=20, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgZmM-cKtpaG"
   },
   "outputs": [],
   "source": [
    "# You can find the saved checkpoint here:\n",
    "!ls /content/cs224s_spring2022/trained_models/ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ig-B8k-muLt0"
   },
   "outputs": [],
   "source": [
    "# How to load the checkpoint into a CTC system:\n",
    "# LightningCTC.load_from_checkpoint(...) # Fill in your checkpoint path.\n",
    "# To resume training, use pl.Trainer as in the `run` fucntion above. For example:\n",
    "# system = LightningCTC.load_from_checkpoint(...)\n",
    "trainer = pl.Trainer(gpus=1, ...)\n",
    "trainer.fit(system)"
   ]
  },
  {
   "attachments": {
    "c33f6548-9bc3-4e8c-ad92-bae1cbdec144.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_zuwGLr0Pi7"
   },
   "source": [
    "---\n",
    "\n",
    "**Plots:** \n",
    "![image.png](attachment:c33f6548-9bc3-4e8c-ad92-bae1cbdec144.png)\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IXjPxEYMOUB"
   },
   "source": [
    "** Using your plots as evidence in your description, answer the following questions:**\n",
    "\n",
    "a) What is the model's best test CER?\n",
    "\n",
    "b) Does the model learn and converge? What do you notice about CTC loss early in training?\n",
    "\n",
    "c) Does the model overfit? Despite the small dataset size, why might CTC not overfit?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5Fkkc9O1SYv"
   },
   "source": [
    "---\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "a) 0.22\n",
    "\n",
    "b) Yes. CTC loss slowly decrease at the start.\n",
    "\n",
    "c) The model does not overfit because CTC can handle alignment of different sequence length.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngMF5nRLXLJ1"
   },
   "source": [
    "# Part 3: Analysis\n",
    "\n",
    "While looking at validation and test CER is a good way to judge how a model is performing, it is also important to look at specific examples it does well on or fails on, in order to build an intuition for why it fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfYm97bGZpyZ"
   },
   "source": [
    "## **Task 3.1: Lowest and Highest CER Examples [5 Points]**\n",
    "\n",
    "**Implementation & Written Response**\n",
    "\n",
    "** Now we will find and examine a test utterance your model transcribes well and a test utterance it transcribes poorly.** Fill out `get_low_high_cer_wav` to get the lowest and highest CERs and their corresponding utterances in your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UrF1Mt6w1g55"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_low_high_cer_wav(system, device=None):\n",
    "  \"\"\"Gets the test set sample with lowest CER and the sample with highest CER.\n",
    "\n",
    "  Args:\n",
    "    system: Subclassed LightningModule for your model.\n",
    "    device: Instance of torch.device(...) [default: None]\n",
    "\n",
    "  Returns:\n",
    "    lowest CER (float), audio of the lowest CER utterance (ndarray),\n",
    "    highest CER (float), audio of the highest CER utterance (ndarray)\n",
    "  \"\"\"\n",
    "  # Init values.\n",
    "  low_cer = float('inf')\n",
    "  low_idx = 0\n",
    "  high_cer = float('-inf')\n",
    "  high_idx = 0\n",
    "\n",
    "  test_dataloader = system.test_dataloader()\n",
    "  index_lookup = system.test_dataset.indices\n",
    "\n",
    "  pbar = tqdm(total=len(test_dataloader))\n",
    "  for i, batch in enumerate(test_dataloader):\n",
    "    input_features, input_lengths = batch[0], batch[1]\n",
    "    labels, label_lengths = batch[2], batch[3]\n",
    "    batch_size = input_features.size(0)\n",
    "    if device is not None:\n",
    "      input_features = input_features.to(device)\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(3.1)\n",
    "    # Hint:\n",
    "    # - Use `get_cer_per_sample`, which gets a numpy array of\n",
    "    #   CERs for each sample in a batch\n",
    "    # - Use `index_lookup` to map a sample's test set index to\n",
    "    #   its index in the full dataset.\n",
    "      \n",
    "    # Get CER for each sample in the batch\n",
    "    log_probs, embedding = system.model(input_features, input_lengths)\n",
    "    hypotheses, hypothesis_lengths, references, reference_lengths = system.model.decode(log_probs, \n",
    "                                                                                        input_lengths, \n",
    "                                                                                        labels, \n",
    "                                                                                        label_lengths,\n",
    "                                                                                        system.test_dataset.sos_index,\n",
    "                                                                                        system.test_dataset.eos_index,\n",
    "                                                                                        system.test_dataset.pad_index,\n",
    "                                                                                        system.test_dataset.eps_index)\n",
    "    cer_per_sample = get_cer_per_sample(hypotheses, hypothesis_lengths, references, reference_lengths)\n",
    "    # cer = cer_per_sample.mean()\n",
    "\n",
    "    # Update low_cer and low_idx if we find a lower CER\n",
    "    min_cer_sample_idx = np.argmin(cer_per_sample)\n",
    "    min_cer_sample_idx_full_dataset = index_lookup[i * batch_size + min_cer_sample_idx]\n",
    "    min_cer = cer_per_sample[min_cer_sample_idx]\n",
    "\n",
    "    if min_cer < low_cer:\n",
    "        low_cer = min_cer\n",
    "        low_idx = min_cer_sample_idx_full_dataset\n",
    "\n",
    "    # Update high_cer and high_idx if we find a higher CER\n",
    "    max_cer_sample_idx = np.argmax(cer_per_sample)\n",
    "    max_cer_sample_idx_full_dataset = index_lookup[i * batch_size + max_cer_sample_idx]\n",
    "    max_cer = cer_per_sample[max_cer_sample_idx]\n",
    "\n",
    "    if max_cer > high_cer:\n",
    "        high_cer = max_cer\n",
    "        high_idx = max_cer_sample_idx_full_dataset\n",
    "\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    pbar.update()\n",
    "  pbar.close()\n",
    "\n",
    "  # Retrieve ndarray wav data from the original h5py file.\n",
    "  system.test_dataset.load_waveforms()\n",
    "  waveform_data = system.test_dataset.waveform_data\n",
    "  low_wav = waveform_data[f'{low_idx}'][:]\n",
    "  high_wav = waveform_data[f'{high_idx}'][:]\n",
    "\n",
    "  return low_cer, low_wav, high_cer, high_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ie7HDMvVqgSP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Constructing HarperValleyBank train dataset...\n",
      "> Constructing HarperValleyBank val dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/45 [24:48<?, ?it/s]\n",
      "  0%|                                                                                                                                                                                    | 0/45 [01:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Constructing HarperValleyBank test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 45/45 [01:14<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = None\n",
    "############################## START OF YOUR CODE ##############################\n",
    "# TODO(3.1)\n",
    "# Add your CTC checkpoint path.\n",
    "checkpoint_path = \"./trained_models/ctc/epoch=19-step=3240.ckpt\"\n",
    "############################## END OF YOUR CODE ################################\n",
    "\n",
    "device = torch.device('cuda')\n",
    "system = LightningCTC.load_from_checkpoint(checkpoint_path)\n",
    "system = system.to(device)\n",
    "system.eval()\n",
    "low_cer, low_wav, high_cer, high_wav = get_low_high_cer_wav(system, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "wygAmfkc-H5n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance with lowest CER: 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Utterance with lowest CER: {}\\n'.format(low_cer))\n",
    "Audio(low_wav, rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wvsavm_F_BjF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance with highest CER: 0.8888888888888888\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Utterance with highest CER: {}\\n'.format(high_cer))\n",
    "Audio(high_wav, rate=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DpQsZADTfKA"
   },
   "source": [
    "** What are the lowest and highest CERs? Why do you think CTC got these CERs for these utterances?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VFJdCCr1yFk"
   },
   "source": [
    "---\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Lowest CER: 0.0\n",
    "\n",
    "Highest CER: 0.88\n",
    "\n",
    "The highest CER utterence is likely an agent saying \"how can I help you\" but skipped syllables. Thus, it is difficult to transcribe correctly.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TtjP0eSZ_tI"
   },
   "source": [
    "## **Task 3.2: Run inference using your model [5 points]**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "** Similar to `get_low_high_cer_wav`, we'll run inference on a single audio file and see what the model transcribes.** Fill in `run_inference` to have your system decode test utterances from a `.WAV` file. We will later run this function in Parts 5 and 7 to qualitatively evaluate systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aBJwFvv2aHsK"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4009232588.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[40], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    dtype=torch.float32)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def run_inference(\n",
    "    system, wav, device=None, sr=8000, n_mels=128, n_fft=256, win_length=256, \n",
    "    hop_length=128, wav_max_length=512, labels=None, label_lengths=None):\n",
    "    \"\"\"Run your system on a .WAV file and returns a string utterance.\n",
    "    \n",
    "    Args:\n",
    "    system: a pl.LightningModule for your chosen model.\n",
    "    wav: a .WAV file of an utterance\n",
    "    device: GPU -> torch.device('cuda')\n",
    "    \n",
    "    Returns:\n",
    "    A string for the utterance transcribed by your model.\n",
    "    \"\"\"\n",
    "    input_feature = None\n",
    "    ############################# START OF YOUR CODE #############################\n",
    "    # TODO(3.2)\n",
    "    # Extract features from the utterance. This is similar to what you implemented\n",
    "    # in `get_primary_task_data`.\n",
    "    # Hint:\n",
    "    # - Make sure to put the extracted features into a batch of size 1.\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=wav, sr=sr, n_mels=n_mels, n_fft=n_fft, \n",
    "        win_length=win_length, hop_length=hop_length)\n",
    "    \n",
    "    # Convert the mel spectrogram to log space and normalize it.\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    log_mel_spectrogram = librosa.util.normalize(log_mel_spectrogram)\n",
    "    \n",
    "    # Pad the feature so that all features are fixed-length.\n",
    "    try:\n",
    "        padding_size = max(0, wav_max_length - len(log_mel_spectrogram.T))\n",
    "        input_feature = torch.tensor(\n",
    "            np.pad(log_mel_spectrogram.T, ((0, padding_size)), (0, 0)), 'constant', constant_values=3),\n",
    "            dtype=torch.float32)\n",
    "    except:\n",
    "        print(\"Error at input_feature\")\n",
    "        print(wav_max_length - len(log_mel_spectrogram.T))\n",
    "    # input_feature = torch.clamp(input_feature, min=0)\n",
    "    # Retrieve and pad the corresponding transcript label sequence.\n",
    "    # transcript_labels = self.human_transcript_labels[index]\n",
    "    # human_transcript_length = len(transcript_labels)\n",
    "    # transcript_labels = transcript_labels + [self.eos_index]  # Add EOS token\n",
    "    # try:\n",
    "    #   padding_size_label = max(0, self.transcript_max_length - len(transcript_labels))\n",
    "    #   human_transcript_label = torch.tensor(\n",
    "    #       np.pad(transcript_labels, (0, padding_size_label), 'constant', constant_values=3),\n",
    "    #       dtype=torch.long)\n",
    "    # except:\n",
    "    #   print(\"Error at human_label\")\n",
    "    #   print(self.transcript_max_length - len(transcript_labels))\n",
    "    \n",
    "    input_length = len(log_mel_spectrogram.T)\n",
    "    \n",
    "    ############################## END OF YOUR CODE ##############################\n",
    "    \n",
    "    input_lengths = torch.LongTensor([input_length])\n",
    "    # Whether or not to use GPU.\n",
    "    if device is not None:\n",
    "    input_feature = input_feature.to(device)\n",
    "    input_lengths = input_lengths.to(device)\n",
    "    if labels is not None:  # to test teacher-forcing\n",
    "      labels = labels.to(device)\n",
    "      labels_lengths = label_lengths.to(device)\n",
    "    \n",
    "    utterance = None\n",
    "    ############################# START OF YOUR CODE #############################\n",
    "    # TODO(3.2)\n",
    "    # Run your system on the utterance input feature to get log probabilities\n",
    "    # and decode the log probabilities into indices. Then turn those indices into\n",
    "    # characters.\n",
    "    log_probs, embedding = system.model(input_feature, input_lengths, labels, label_lengths)\n",
    "    hypotheses, hypothesis_lengths, references, reference_lengths = system.model.decode(log_probs, \n",
    "                                                                                        input_lengths, \n",
    "                                                                                        labels, \n",
    "                                                                                        label_lengths,\n",
    "                                                                                        system.test_dataset.sos_index,\n",
    "                                                                                        system.test_dataset.eos_index,\n",
    "                                                                                        system.test_dataset.pad_index,\n",
    "                                                                                        system.test_dataset.eps_index)\n",
    "    \n",
    "    chars = model.test_dataset.indices_to_chars(hypotheses)\n",
    "    utterance = chars\n",
    "    ############################## END OF YOUR CODE ##############################\n",
    "    return utterance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg_FAAx04iP9"
   },
   "source": [
    "# Part 4: Leveraging Auxiliary Tasks for Multi-Task Learing\n",
    "\n",
    "When designing a speech system, we might care about more than just the transcription. As a bank, we might want to know the intent of the caller, for example.\n",
    "\n",
    "Our dataset includes the dialog action, the intent of the caller, and the sentiment of the caller. In the spirit of an end-to-end system, we will expand the CTC model to make predictions for auxiliary tasks. It is up to you which tasks you decide to multi-task on!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO-ng6Sgbv6d"
   },
   "source": [
    "## **Task 4.1 Working with auxiliary task data [5 Points]**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "** Fill in `__getitem__`. Add one or more auxiliary tasks to your training.** We include `get_auxiliary_labels` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Co1TnD6lf-Kn"
   },
   "outputs": [],
   "source": [
    "class HarperValleyBankMTL(HarperValleyBank):\n",
    "  \"\"\"Like the HarperValleyBank dataset but returns labels for task type, \n",
    "  dialog actions, and sentiment: our three auxiliary tasks.\n",
    "\n",
    "  See `HarperValleyBank` class for description. \n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "    self, root, split='train', n_mels=128, n_fft=128, win_length=256, \n",
    "    hop_length=128, wav_max_length=200, transcript_max_length=200, \n",
    "    append_eos_token=False):\n",
    "    super().__init__(\n",
    "      root, split=split, n_mels=n_mels, n_fft=n_fft,\n",
    "      win_length=win_length, hop_length=hop_length,\n",
    "      wav_max_length=wav_max_length, \n",
    "      transcript_max_length=transcript_max_length,\n",
    "      append_eos_token=append_eos_token)\n",
    "    self.auxiliary_labels = self.get_auxiliary_labels()\n",
    "\n",
    "  def get_auxiliary_labels(self):\n",
    "    \"\"\"Returns auxiliary task labels.\n",
    "    \n",
    "    This function will take the raw auxiliary tasks and convert them\n",
    "    integers labels (for neural networks).\n",
    "\n",
    "    These include: `task_type`, `dialogue_acts`, and `sentiment`.\n",
    "    \"\"\"\n",
    "    # task_types: each element is a string representing a conversation-level\n",
    "    #             label. So all utterances in the same conversation share \n",
    "    #             the same label.\n",
    "    task_types = self.label_data['task_types']\n",
    "    # dialog_acts: each element is a comma-separated string of dialog actions \n",
    "    #              that describe the current utterance\n",
    "    dialog_acts = self.label_data['dialog_acts']\n",
    "    dialog_acts = [acts.split(',') for acts in dialog_acts]\n",
    "    # sentiments: each element is a 3 dimensional vector that sums to 1 \n",
    "    #             representing the probabilities for \n",
    "    #             \"negative\", \"neutral\", and \"positive\"\n",
    "    sentiment_labels = self.label_data['sentiments']\n",
    "\n",
    "    # Get label vocabularies.\n",
    "    task_type_vocab = sorted(set(task_types))\n",
    "    dialog_acts_vocab = sorted(set([item for sublist in dialog_acts\n",
    "                                    for item in sublist]))\n",
    "\n",
    "    task_type_labels = [task_type_vocab.index(t) for t in task_types]\n",
    "\n",
    "    # dialog_acts_labels: list of 1-hot vectors\n",
    "    dialog_acts_labels = []\n",
    "    for acts in dialog_acts:\n",
    "      onehot = [0 for _ in range(len(dialog_acts_vocab))]\n",
    "      for act in acts:\n",
    "        onehot[dialog_acts_vocab.index(act)] = 1\n",
    "      dialog_acts_labels.append(onehot)\n",
    "\n",
    "    # Store number of classes for each auxiliary task.\n",
    "    # Note: \n",
    "    #   - task_type is a N-way classification problem.\n",
    "    #   - dialog_acts is a set of binary classification problems. \n",
    "    #       (more than one dialog action may be \"on\" for an utterance)            \n",
    "    #   - sentiment is a regression problem (match given probabilities).\n",
    "    self.task_type_num_class = len(task_type_vocab)\n",
    "    self.dialog_acts_num_class = len(dialog_acts_vocab)\n",
    "    self.sentiment_num_class = 3\n",
    "    \n",
    "    return task_type_labels, dialog_acts_labels, sentiment_labels\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"Serves multi-task data for a single utterance.\"\"\"\n",
    "    if not hasattr(self, 'waveform_data'):\n",
    "      self.load_waveforms()\n",
    "\n",
    "    index = int(self.indices[index])\n",
    "\n",
    "    primary_task_data = self.get_primary_task_data(index)\n",
    "    auxiliary_task_data = None\n",
    "    \n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.1)\n",
    "    # Get auxiliary task label(s) for this index using\n",
    "    # `self.auxiliary_task_labels`. Populate the object `auxiliary_task_data` \n",
    "    # as a tuple of auxiliary task labels. Make sure to cast appropriate \n",
    "    # torch tensor types for the different labels.\n",
    "\n",
    "    # Get auxiliary task labels for the current index\n",
    "    task_type_label, dialog_acts_label, sentiment_label = self.auxiliary_labels\n",
    "\n",
    "    # Convert labels to torch tensors\n",
    "    task_type_label = torch.tensor(task_type_label[index], dtype=torch.long)\n",
    "    dialog_acts_label = torch.tensor(dialog_acts_label[index], dtype=torch.float32)\n",
    "    sentiment_label = torch.tensor(sentiment_label[index], dtype=torch.float32)\n",
    "\n",
    "    # Combine auxiliary task labels into a tuple\n",
    "    auxiliary_task_data = (task_type_label, dialog_acts_label, sentiment_label)\n",
    "    \n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    if not isinstance(auxiliary_task_data, tuple):\n",
    "      auxiliary_task_data = (auxiliary_task_data,)\n",
    "\n",
    "    return primary_task_data + auxiliary_task_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MND0Dny4FBrN"
   },
   "source": [
    "## **Task 4.2: Implement auxiliary task heads [2 points]**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "** Fill out the `*Classifier` classes for your multi-tasking design choice.** These classifiers only require a single layer in depth. You will use these in `LightningCTCMTL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "2-cjrZkscJUp"
   },
   "outputs": [],
   "source": [
    "class TaskTypeClassifier(nn.Module):\n",
    "  def __init__(self, input_dim, n_classes):\n",
    "    super().__init__()\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    print(f\"n_classes: {n_classes}\")\n",
    "    self.fc = nn.Linear(input_dim, n_classes)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    log_probs = None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    # Hint: This is an N-way classification problem.\n",
    "    # x = self.hidden(inputs)\n",
    "    # x = self.final(x)\n",
    "    # log_probs = F.log_softmax(x, dim=1)\n",
    "    print(f\"input shape: {inputs.shape}\")\n",
    "    print(f\"encoding shape: {self.fc(inputs).shape}\")\n",
    "    encoding = nn.Linear(inputs, input_dim)\n",
    "    print(f\"encoding after 1: {encoding.shape}\")\n",
    "    encoding_2 = nn.Linear(input_dim, n_classes)\n",
    "    # log_probs = F.log_softmax(self.fc(inputs), dim=-1)\n",
    "    print(f\"log_probs shape: {log_probs.shape}\")\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    return log_probs\n",
    "  \n",
    "  def get_loss(self, probs, targets):\n",
    "    loss = None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2) \n",
    "    try:\n",
    "        loss = F.nll_loss(probs, targets)\n",
    "    except:\n",
    "        print(f\"prob size: {probs.shape}\")\n",
    "        print(f\"targets size: {targets.shape}\")\n",
    "    # loss = F.binary_cross_entropy(probs, targets)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    return loss\n",
    "\n",
    "\n",
    "class DialogActsClassifier(nn.Module):\n",
    "  def __init__(self, input_dim, n_classes):\n",
    "    super().__init__()\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    self.fc = nn.Linear(input_dim, n_classes)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    probs = None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    # Hint: One person can have multiple dialog actions.\n",
    "    probs = torch.sigmoid(self.fc(inputs))\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    return probs\n",
    "\n",
    "  def get_loss(self, probs, targets):\n",
    "    loss = None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    # Hint:\n",
    "    # - probs shape: (batch_size, num_dialog_acts)\n",
    "    # - targets shape: (batch_size, num_dialog_acts)\n",
    "    loss = F.binary_cross_entropy(probs, targets)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    return loss\n",
    "\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, input_dim, n_classes):\n",
    "    super().__init__()\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    self.fc = nn.Linear(input_dim, n_classes)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    probs = None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    # Hint:\n",
    "    # - Sentiment is measured as a log probability distribution among multiple\n",
    "    #   possible sentiments.\n",
    "    probs = F.softmax(self.fc(inputs), dim=1)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    return probs\n",
    "\n",
    "  def get_loss(self, pred_probs, target_probs):\n",
    "    loss = None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.2)\n",
    "    # Hint:\n",
    "    # - As usual, the predictions are probabilities. But the labels for\n",
    "    #   sentiment are themselves probabilities. Since the targets are not be\n",
    "    #   single numbers, we cannot just use `F.cross_entropy`.\n",
    "    # - Therefore, you will need to implement cross entropy manually.\n",
    "    #     Refer to wikipedia: https://en.wikipedia.org/wiki/Cross_entropy\n",
    "    # - pred_logits shape: (batch_size, num_sentiment_class)\n",
    "    # - target_logits shape: (batch_size, num_sentiment_class)\n",
    "    loss = -torch.sum(target_probs * torch.log(pred_probs + 1e-10)) / target_probs.size(0)\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfsmjWi3DADg"
   },
   "source": [
    "## **Task 4.3: Implement multi-task learning loss [3 points]**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "**Metrics.** We provide code for computing the metrics for each possible auxiliary task. For task type classification and sentiment classification, you should use accuracy. For dialog acts classification, you should use F1 score (as it is very unbalanced).\n",
    "\n",
    "** Instantiate your auxiliary task models in `__init__` for `LightningCTCMTL` and implement/modify `get_multi_task_loss`.** You can additively combine loss functions and use weighting parameters in your addition to trade off the importance of different tasks during training.\n",
    "\n",
    "**Note:** In general, your choice of weighting parameters is often an important design decision for your system! It is even possible to negatively weight certain tasks that may otherwise bias the primary task prediction through simple correlation. This turns those auxiliary tasks into adversarial tasks (that your model should *not* do well on) and can lead to more robust performance on certain edge cases.\n",
    "\n",
    "**Note:** In the Lightning class below, these weights are expressed as `asr_weight`, `task_type_weight`, `dialog_acts_weight`, and `sentiment_weight`. You should choose them to sum to 1.\n",
    "\n",
    "** Modify `validation_epoch_end` and `test_epoch_end`.** Leave in the necessary code to store metrics for your model's auxiliary tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "M5cllqMeW_pb"
   },
   "outputs": [],
   "source": [
    "class LightningCTCMTL(LightningCTC):\n",
    "  \"\"\"PyTorch Lightning class for training CTC with multi-task learning.\"\"\"\n",
    "  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n",
    "               wav_max_length=200, transcript_max_length=200, \n",
    "               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n",
    "               encoder_num_layers=2, encoder_hidden_dim=256, \n",
    "               encoder_bidirectional=True, asr_weight=1.0, task_type_weight=1.0, \n",
    "               dialog_acts_weight=1.0, sentiment_weight=1.0):\n",
    "    super().__init__(\n",
    "      n_mels=n_mels, hop_length=hop_length, \n",
    "      wav_max_length=wav_max_length, \n",
    "      transcript_max_length=transcript_max_length,\n",
    "      learning_rate=learning_rate, \n",
    "      batch_size=batch_size, \n",
    "      weight_decay=weight_decay, \n",
    "      encoder_num_layers=encoder_num_layers, \n",
    "      encoder_hidden_dim=encoder_hidden_dim, \n",
    "      encoder_bidirectional=encoder_bidirectional)\n",
    "    self.save_hyperparameters()\n",
    "    self.asr_weight = asr_weight\n",
    "    self.task_type_weight = task_type_weight\n",
    "    self.dialog_acts_weight = dialog_acts_weight\n",
    "    self.sentiment_weight = sentiment_weight\n",
    "    train_dataset, val_dataset, test_dataset = self.create_datasets()\n",
    "\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.3)\n",
    "    # Instantiate your auxiliary task models here.\n",
    "    input_dim = self.train_dataset.input_dim # You need to replace this with the actual input dimension\n",
    "    task_type_num_classes = len(self.train_dataset.label_data['task_types'])  # Replace with the actual number of classes for task_type\n",
    "    dialog_acts_num_classes = len(self.train_dataset.label_data['dialog_acts'])  # Replace with the actual number of classes for dialog_acts\n",
    "    sentiment_num_classes = len(self.train_dataset.label_data['sentiments'])  # Replace with the actual number of classes for sentiment\n",
    "\n",
    "    self.task_type_model = TaskTypeClassifier(input_dim, task_type_num_classes)\n",
    "    self.dialog_acts_model = DialogActsClassifier(input_dim, dialog_acts_num_classes)\n",
    "    self.sentiment_model = SentimentClassifier(input_dim, sentiment_num_classes)\n",
    "    \n",
    "    ############################# END OF YOUR CODE #############################\n",
    "\n",
    "  def create_datasets(self):\n",
    "    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
    "    train_dataset = HarperValleyBankMTL(\n",
    "      root, split='train', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "      win_length=self.win_length, hop_length=self.hop_length, \n",
    "      wav_max_length=self.wav_max_length,\n",
    "      transcript_max_length=self.transcript_max_length,\n",
    "      append_eos_token=False)\n",
    "    val_dataset = HarperValleyBankMTL(\n",
    "      root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "      win_length=self.win_length, hop_length=self.hop_length, \n",
    "      wav_max_length=self.wav_max_length,\n",
    "      transcript_max_length=self.transcript_max_length,\n",
    "      append_eos_token=False) \n",
    "    test_dataset = HarperValleyBankMTL(\n",
    "      root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "      win_length=self.win_length, hop_length=self.hop_length, \n",
    "      wav_max_length=self.wav_max_length,\n",
    "      transcript_max_length=self.transcript_max_length,\n",
    "      append_eos_token=False) \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "  def get_multi_task_loss(self, batch, split='train'):\n",
    "    \"\"\"Gets losses and metrics for all task heads.\"\"\"\n",
    "    # Compute loss on the primary ASR task.\n",
    "    asr_loss, asr_metrics, embedding = self.get_primary_task_loss(batch, split)\n",
    "    \n",
    "    # Note: Not all of these have to be used (it is up to your design)\n",
    "    task_type_labels = None\n",
    "    dialog_acts_labels = None\n",
    "    sentiment_labels = None\n",
    "    task_type_log_probs = None\n",
    "    dialog_acts_probs = None\n",
    "    sentiment_log_probs = None\n",
    "    task_type_loss = None\n",
    "    dialog_acts_loss = None\n",
    "    sentiment_loss = None\n",
    "    combined_loss = None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.3)\n",
    "    # Implement multi-task learning by combining multiple objectives.\n",
    "    # Define `combined_loss` here. \n",
    "    # Note: Not all of these have to be used (it is up to your design)\n",
    "    task_type_labels = batch[-4]\n",
    "    dialog_acts_labels = batch[-3]\n",
    "    sentiment_labels = batch[-2]\n",
    "    task_type_log_probs = self.task_type_model(embedding.T)\n",
    "    dialog_acts_probs = self.dialog_acts_model(embedding.T)\n",
    "    sentiment_log_probs = self.sentiment_model(embedding.T)\n",
    "    task_type_loss = self.task_type_model.get_loss(task_type_log_probs, task_type_labels)\n",
    "    dialog_acts_loss = self.dialog_acts_model.get_loss(dialog_acts_probs, dialog_acts_labels)\n",
    "    sentiment_loss = self.sentiment_model.get_loss(sentiment_log_probs, sentiment_labels)\n",
    "    combined_loss = (\n",
    "        self.asr_weight * asr_loss + \n",
    "        self.task_type_weight * task_type_loss +\n",
    "        self.dialog_acts_weight * dialog_acts_loss +\n",
    "        self.sentiment_weight * sentiment_loss\n",
    "    )\n",
    "    \n",
    "    ############################ END OF YOUR CODE ##############################\n",
    "\n",
    "    with torch.no_grad():\n",
    "      ############################ START OF YOUR CODE ##########################\n",
    "      # TODO(4.3)\n",
    "      # No additional code is required here. :)\n",
    "      # We provide how to compute metrics for all possible auxiliary tasks and\n",
    "      # store them in your metrics dictionary. Comment out the metrics for tasks\n",
    "      # you do not plan to use.\n",
    "\n",
    "      # TASK_TYPE: Compare predicted task type to true task type.\n",
    "      task_type_preds = torch.argmax(task_type_log_probs, dim=1)\n",
    "      task_type_acc = \\\n",
    "        (task_type_preds == task_type_labels).float().mean().item()\n",
    "\n",
    "      # DIALOG_ACTS: Compare predicted dialog actions to true dialog actions.\n",
    "      dialog_acts_preds = torch.round(dialog_acts_probs)\n",
    "      dialog_acts_f1 = f1_score(dialog_acts_labels.cpu().numpy().reshape(-1),\n",
    "                                dialog_acts_preds.cpu().numpy().reshape(-1))\n",
    "\n",
    "\n",
    "      # SENTIMENT: Compare largest predicted sentiment to largest true sentim\n",
    "      sentiment_preds = torch.argmax(sentiment_log_probs, dim=1)\n",
    "      sentiment_labels = torch.argmax(sentiment_labels, dim=1)\n",
    "      sentiment_acc = \\\n",
    "      (sentiment_preds == sentiment_labels).float().mean().item()\n",
    "\n",
    "      metrics = {\n",
    "        # Task losses. \n",
    "        f'{split}_asr_loss': asr_metrics[f'{split}_loss'],\n",
    "        f'{split}_task_type_loss': task_type_loss,\n",
    "        f'{split}_dialog_acts_loss': dialog_acts_loss,\n",
    "        f'{split}_sentiment_loss': sentiment_loss,\n",
    "        # CER as ASR metric.\n",
    "        f'{split}_asr_cer': asr_metrics[f'{split}_cer'],\n",
    "        # Accuracy as task_type metric.\n",
    "        f'{split}_task_type_acc': task_type_acc,\n",
    "        # F1 score as dialog_acts metric.\n",
    "        f'{split}_dialog_acts_f1': dialog_acts_f1,\n",
    "        # Accuracy as sentiment metric.\n",
    "        f'{split}_sentiment_acc': sentiment_acc\n",
    "      }\n",
    "      ############################ END OF YOUR CODE ############################\n",
    "    return combined_loss, metrics\n",
    "\n",
    "  # comment out parameters for the models you don't use\n",
    "  def configure_optimizers(self):\n",
    "    parameters = chain(self.model.parameters(),\n",
    "                       self.task_type_model.parameters(),\n",
    "                       self.dialog_acts_model.parameters(),\n",
    "                       self.sentiment_model.parameters())\n",
    "    optim = torch.optim.AdamW(parameters, lr=self.lr,\n",
    "                              weight_decay=self.weight_decay)\n",
    "    return [optim], []\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss, metrics = self.get_multi_task_loss(batch, split='train')\n",
    "    self.log_dict(metrics)\n",
    "    # self.log('train_asr_loss', metrics['train_asr_loss'], prog_bar=True, \n",
    "    #          on_step=True)\n",
    "    # self.log('train_asr_cer', metrics['train_asr_cer'], prog_bar=True, \n",
    "    #          on_step=True)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss, metrics = self.get_multi_task_loss(batch, split='val')\n",
    "    return metrics\n",
    "\n",
    "  def on_validation_epoch_end(self, outputs):\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.3)\n",
    "    # No additional code is required here. :)\n",
    "    # Comment out the metrics for tasks you do not plan to use.\n",
    "    metrics = { \n",
    "      'val_asr_loss': torch.tensor([elem['val_asr_loss']\n",
    "                                    for elem in outputs]).float().mean(),\n",
    "      'val_asr_cer': torch.tensor([elem['val_asr_cer']\n",
    "                                   for elem in outputs]).float().mean(),\n",
    "      'val_task_type_loss': torch.tensor([elem['val_task_type_loss']\n",
    "                                          for elem in outputs]).float().mean(),\n",
    "      'val_task_type_acc': torch.tensor([elem['val_task_type_acc']\n",
    "                                          for elem in outputs]).float().mean(),\n",
    "      'val_dialog_acts_loss': torch.tensor([elem['val_dialog_acts_loss'] \n",
    "                                            for elem in outputs]).float().mean(),\n",
    "      'val_dialog_acts_f1': torch.tensor([elem['val_dialog_acts_f1']\n",
    "                                          for elem in outputs]).float().mean(),\n",
    "      'val_sentiment_loss': torch.tensor([elem['val_sentiment_loss']\n",
    "                                          for elem in outputs]).float().mean(),\n",
    "      'val_sentiment_acc': torch.tensor([elem['val_sentiment_acc']\n",
    "                                         for elem in outputs]).float().mean()\n",
    "      }\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    # self.log('val_asr_loss', metrics['val_asr_loss'], prog_bar=True)\n",
    "    # self.log('val_asr_cer', metrics['val_asr_cer'], prog_bar=True)\n",
    "    self.log_dict(metrics)\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    loss, metrics = self.get_multi_task_loss(batch, split='test')\n",
    "    return metrics\n",
    "\n",
    "  def on_test_epoch_end(self, outputs):\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(4.3)\n",
    "    # No additional code is required here. :)\n",
    "    # Comment out the metrics for tasks you do not plan to use.\n",
    "    metrics = { \n",
    "      'test_asr_loss': torch.tensor([elem['test_asr_loss']\n",
    "                                    for elem in outputs]).float().mean(),\n",
    "      'test_asr_cer': torch.tensor([elem['test_asr_cer']\n",
    "                                   for elem in outputs]).float().mean(),\n",
    "      'test_task_type_loss': torch.tensor([elem['test_task_type_loss']\n",
    "                                          for elem in outputs]).float().mean(),\n",
    "      'test_task_type_acc': torch.tensor([elem['test_task_type_acc']\n",
    "                                          for elem in outputs]).float().mean(),\n",
    "      'test_dialog_acts_loss': torch.tensor([elem['test_dialog_acts_loss'] \n",
    "                                             for elem in outputs]).float().mean(),\n",
    "      'test_dialog_acts_f1': torch.tensor([elem['test_dialog_acts_f1']\n",
    "                                           for elem in outputs]).float().mean(),\n",
    "      'test_sentiment_loss': torch.tensor([elem['test_sentiment_loss']\n",
    "                                           for elem in outputs]).float().mean(),\n",
    "      'test_sentiment_acc': torch.tensor([elem['test_sentiment_acc']\n",
    "                                          for elem in outputs]).float().mean()\n",
    "      }\n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    # self.log('test_asr_loss', metrics['test_asr_loss'], prog_bar=True)\n",
    "    # self.log('test_asr_cer', metrics['test_asr_cer'], prog_bar=True)\n",
    "    self.log_dict(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7x88wh0WTj4"
   },
   "source": [
    "## **Task 4.1: Training CTC-MTL [10 points]**\n",
    "\n",
    "**Training & Written Response**\n",
    "\n",
    "** Train the CTC-MTL network with the default hyperparameters we provide.**\n",
    "\n",
    "One epoch of training CTC-MTL takes 3 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n",
    "\n",
    "**NOTE: We expect this model to perform a bit worse on CER compared with the CTC-only objective. That's a trade-off when building multi-task learning approaches, so don't spend extra times trying to improve your CER for this question**\n",
    "\n",
    "** Paste screenshots from your Weights & Biases dashboard of your loss curves and CER curve in the cell marked \"Plots\". Remember to include learning curves for the auxiliary tasks!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "C4D8kJJKxWVK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Constructing HarperValleyBank train dataset...\n",
      "                                                                                                                                                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2157: UserWarning: Run (9mjc9r6u) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Constructing HarperValleyBank val dataset...\n",
      "> Constructing HarperValleyBank test dataset...\n",
      "> Constructing HarperValleyBank train dataset...\n",
      "> Constructing HarperValleyBank val dataset...\n",
      "> Constructing HarperValleyBank test dataset...\n",
      "n_classes: 25730\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:slitwk9n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ctc_mtl</strong> at: <a href='https://wandb.ai/dacphuc1993/cs224s/runs/slitwk9n' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/runs/slitwk9n</a><br/> View job at <a href='https://wandb.ai/dacphuc1993/cs224s/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyODg5MzA0MQ==/version_details/v5' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyODg5MzA0MQ==/version_details/v5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240111_111426-slitwk9n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:slitwk9n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Phuc\\Desktop\\CS224S-slp\\HW3\\wandb\\run-20240111_112854-jliq1yw1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dacphuc1993/cs224s/runs/jliq1yw1' target=\"_blank\">ctc_mtl</a></strong> to <a href='https://wandb.ai/dacphuc1993/cs224s' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dacphuc1993/cs224s' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dacphuc1993/cs224s/runs/jliq1yw1' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/runs/jliq1yw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model             | CTCEncoderDecoder    | 2.4 M \n",
      "1 | task_type_model   | TaskTypeClassifier   | 3.3 M \n",
      "2 | dialog_acts_model | DialogActsClassifier | 3.3 M \n",
      "3 | sentiment_model   | SentimentClassifier  | 3.3 M \n",
      "-----------------------------------------------------------\n",
      "12.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.3 M    Total params\n",
      "49.398    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                                                                                                                        | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                                                                                                                       | 0/2 [00:00<?, ?it/s]input shape: torch.Size([2048, 128])\n",
      "encoding shape: torch.Size([2048, 25730])\n",
      "log_probs shape: torch.Size([2048, 25730])\n",
      "prob size: torch.Size([2048, 25730])\n",
      "targets size: torch.Size([128])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([128])) that is different to the input size (torch.Size([2048, 25730])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run CTC-MTL\u001b[39;00m\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_mels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_fft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.25\u001b[39m,\n\u001b[0;32m     21\u001b[0m }\n\u001b[1;32m---> 23\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLightningCTCMTL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mctc_mtl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_asr_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(system, config, ckpt_dir, epochs, monitor_key, use_gpu, seed)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m   trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     36\u001b[0m       max_epochs\u001b[38;5;241m=\u001b[39mepochs, min_epochs\u001b[38;5;241m=\u001b[39mepochs, enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m       callbacks\u001b[38;5;241m=\u001b[39mcheckpoint_callback, logger\u001b[38;5;241m=\u001b[39mwandb_logger)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1062\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[105], line 164\u001b[0m, in \u001b[0;36mLightningCTCMTL.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m--> 164\u001b[0m   loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_multi_task_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "Cell \u001b[1;32mIn[105], line 90\u001b[0m, in \u001b[0;36mLightningCTCMTL.get_multi_task_loss\u001b[1;34m(self, batch, split)\u001b[0m\n\u001b[0;32m     88\u001b[0m sentiment_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_model(embedding\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     89\u001b[0m task_type_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type_model\u001b[38;5;241m.\u001b[39mget_loss(task_type_log_probs, task_type_labels)\n\u001b[1;32m---> 90\u001b[0m dialog_acts_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialog_acts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdialog_acts_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialog_acts_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m sentiment_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_model\u001b[38;5;241m.\u001b[39mget_loss(sentiment_log_probs, sentiment_labels)\n\u001b[0;32m     92\u001b[0m combined_loss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masr_weight \u001b[38;5;241m*\u001b[39m asr_loss \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type_weight \u001b[38;5;241m*\u001b[39m task_type_loss \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialog_acts_weight \u001b[38;5;241m*\u001b[39m dialog_acts_loss \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_weight \u001b[38;5;241m*\u001b[39m sentiment_loss\n\u001b[0;32m     97\u001b[0m )\n",
      "Cell \u001b[1;32mIn[104], line 63\u001b[0m, in \u001b[0;36mDialogActsClassifier.get_loss\u001b[1;34m(self, probs, targets)\u001b[0m\n\u001b[0;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m############################ START OF YOUR CODE ############################\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# TODO(4.2)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Hint:\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# - probs shape: (batch_size, num_dialog_acts)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# - targets shape: (batch_size, num_dialog_acts)\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m############################# END OF YOUR CODE #############################\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\functional.py:3113\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3111\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3116\u001b[0m     )\n\u001b[0;32m   3118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3119\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([2048, 25730])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Run CTC-MTL\n",
    "config = {\n",
    "  'n_mels': 128,\n",
    "  'n_fft': 256,\n",
    "  'win_length': 256,\n",
    "  'hop_length': 128,\n",
    "  'wav_max_length': 2048, \n",
    "  'transcript_max_length': 500,\n",
    "  'learning_rate': 1e-3, \n",
    "  'batch_size': 128, \n",
    "  'weight_decay': 0, \n",
    "  'encoder_num_layers': 2, \n",
    "  'encoder_hidden_dim': 256, \n",
    "  'encoder_bidirectional': True,\n",
    "  # you may wish to play with these weights; try to keep the sum\n",
    "  # of them equal to one.\n",
    "  'asr_weight': 0.25, \n",
    "  'task_type_weight': 0.25,\n",
    "  'dialog_acts_weight': 0.25,\n",
    "  'sentiment_weight': 0.25,\n",
    "}\n",
    "\n",
    "run(system=\"LightningCTCMTL\", config=config, ckpt_dir='ctc_mtl', epochs=20, \n",
    "    monitor_key='val_asr_loss', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "xrqzM4TTleFx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Phuc\\Desktop\\CS224S-slp\\HW3>doskey make=mingw32-make.exe \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# You can find the saved checkpoint here:\n",
    "!ls /content/cs224s_spring2022/trained_models/ctc_mtl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ1KkpAGlDX1"
   },
   "source": [
    "---\n",
    "\n",
    "**Plots:**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYCTmwiNkioM"
   },
   "source": [
    "** Using your plots as evidence in your description, answer the following questions:**\n",
    "\n",
    "a) Report performance metrics on each of the auxiliary tasks and the CER of your jointly trained model.\n",
    "\n",
    "b) Under the same configuration of hyperparameters, does CTC-MTL perform better than CTC? Why or why not? (Hint: Have the loss plots converged? How does multi-tasking affect the speed of learning the primary task?)\n",
    "\n",
    "c) Which tasks seem to be more difficult than others? Why might that be?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBbCzn0512GV"
   },
   "source": [
    "---\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZItBGVmaLKE"
   },
   "source": [
    "# Part 5: Joint CTC-Attention Based Neural Network\n",
    "As a practictioner, in designing a speech system you might face the choice between using CTC or LAS, and you might wonder, **why not both**? Now, we'll use even more multi-tasking to improve our performance, except with a twist: we want to regularize our attention-based network to respect CTC alignments. Essentially, we will use CTC as another auxiliary task to aid alignment for recognizing speech, based on this [paper](https://arxiv.org/pdf/1609.06773.pdf) (Kim et al.).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aKsMxwmaHMq"
   },
   "source": [
    "## CTC-Attention understanding check\n",
    "\n",
    "**Read through the paper. Let us call the proposed model CTC-LAS-MTL.**\n",
    "\n",
    "**You do not need to submit responses for the questions below. Use these to check your understanding as you work**\n",
    "\n",
    "a) What is a key advantage of CTC-LAS-MTL over LAS-MTL?\n",
    "\n",
    "\n",
    "b) What is an erroneous example utterance LAS-MTL might predict that CTC-LAS-MTL would probably not? Explain your reasoning. (Hint: Why might conditional independence sometimes be a good thing?)\n",
    "\n",
    "\n",
    "c) Describe a tradeoff of weighting the CTC loss higher than the LAS loss vs. lower than the LAS loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "xIpip7iUG0MQ"
   },
   "outputs": [],
   "source": [
    "class CTCDecoder(nn.Module):\n",
    "  \"\"\"\n",
    "  This is a small decoder (just one linear layer) that takes \n",
    "  the listener embedding from LAS and imposes a CTC \n",
    "  objective on the decoding.\n",
    "\n",
    "  NOTE: This is only to be used for the Joint CTC-Attention model.\n",
    "  \"\"\"\n",
    "  def __init__(self, listener_hidden_dim, num_class, dropout=0.5):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(listener_hidden_dim, num_class)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.listener_hidden_dim = listener_hidden_dim\n",
    "    self.num_class = num_class\n",
    "\n",
    "  def forward(self, listener_outputs):\n",
    "    batch_size, maxlen, _ = listener_outputs.size()\n",
    "    logits = self.fc(self.dropout(listener_outputs))\n",
    "    logits = logits.view(batch_size, maxlen, self.num_class)\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    return log_probs\n",
    "\n",
    "  def get_loss(\n",
    "      self, log_probs, input_lengths, labels, label_lengths, blank=0):\n",
    "    return get_ctc_loss(\n",
    "      log_probs, labels, input_lengths, label_lengths, blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TXauS2P67vx"
   },
   "source": [
    "## **Task 5.1: CTC-LAS Network [5 points]**\n",
    "\n",
    "** Read through the starter code and fill out the `forward` pass for `JointCTCAttention`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "BpSulwjvHHY1"
   },
   "outputs": [],
   "source": [
    "class JointCTCAttention(LASEncoderDecoder):\n",
    "  \"\"\"Joint CTC and LAS model that optimizes the LAS objective but \n",
    "  regularized by the conditional independence of a CTC decoder. One\n",
    "  can interpret CTC as regularizer on LAS.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, input_dim, num_class, label_maxlen, listener_hidden_dim=128, \n",
    "      listener_bidirectional=True, num_pyramid_layers=3, dropout=0, \n",
    "      speller_hidden_dim=256, speller_num_layers=1, mlp_hidden_dim=128, \n",
    "      multi_head=1, sos_index=0, sample_decode=False):\n",
    "    super().__init__(\n",
    "      input_dim,\n",
    "      num_class,\n",
    "      label_maxlen,\n",
    "      listener_hidden_dim=listener_hidden_dim,\n",
    "      listener_bidirectional=listener_bidirectional,\n",
    "      num_pyramid_layers=num_pyramid_layers,\n",
    "      dropout=dropout,\n",
    "      speller_hidden_dim=speller_hidden_dim,\n",
    "      speller_num_layers=speller_num_layers,\n",
    "      mlp_hidden_dim=mlp_hidden_dim,\n",
    "      multi_head=multi_head,\n",
    "      sos_index=sos_index,\n",
    "      sample_decode=sample_decode,\n",
    "    )\n",
    "    self.ctc_decoder = CTCDecoder(listener_hidden_dim * 2, num_class)\n",
    "    self.num_pyramid_layers = num_pyramid_layers\n",
    "    self.embedding_dim = listener_hidden_dim * 4\n",
    "\n",
    "  def forward(self, inputs, ground_truth=None, teacher_force_prob=0.9,):\n",
    "    ctc_log_probs = None\n",
    "    las_log_probs = None\n",
    "    h, c = None, None\n",
    "    ############################ START OF YOUR CODE ############################\n",
    "    # TODO(5.1)\n",
    "    # Hint:\n",
    "    # - Encode the inputs with the `listener` network and decode \n",
    "    #   transcription probabilities using both the `speller` network\n",
    "    #   and CTCDecoder network.\n",
    "    encoding, (h, c) = self.listener(inputs)\n",
    "    ctc_log_probs = self.ctc_decoder(encoding)\n",
    "    las_log_probs = self.speller(encoding)\n",
    "    \n",
    "    ############################# END OF YOUR CODE #############################\n",
    "    listener_hc = self.combine_h_and_c(h, c)\n",
    "    return ctc_log_probs, las_log_probs, listener_hc\n",
    "\n",
    "  def get_loss(\n",
    "      self, ctc_log_probs, las_log_probs, input_lengths, labels, label_lengths,\n",
    "      num_labels, pad_index=0, blank_index=0, label_smooth=0.1):\n",
    "    ctc_loss = self.ctc_decoder.get_loss(\n",
    "      ctc_log_probs,\n",
    "      # pyramid encode cuts timesteps in 1/2 each way\n",
    "      input_lengths // (2**self.num_pyramid_layers),\n",
    "      labels,\n",
    "      label_lengths,\n",
    "      blank=blank_index,\n",
    "    )\n",
    "    las_loss = super().get_loss(las_log_probs, labels, num_labels,\n",
    "                                pad_index=pad_index, label_smooth=label_smooth)\n",
    "\n",
    "    return ctc_loss, las_loss\n",
    "\n",
    "  def decode(self, log_probs, input_lengths, labels, label_lengths,\n",
    "             sos_index, eos_index, pad_index, eps_index):\n",
    "    las_log_probs = log_probs[1]\n",
    "    return super().decode(las_log_probs, input_lengths, labels, label_lengths,\n",
    "                          sos_index, eos_index, pad_index, eps_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EU4UBRT6YKN1"
   },
   "source": [
    "## **Task 5.2: Training CTC-LAS-MTL [10 points]**\n",
    "\n",
    "**Training results & written response**\n",
    "\n",
    "** Train the CTC-LAS-MTL network with the default hyperparameters we provide.**\n",
    "\n",
    "Like LAS-MTL, this model takes 6-7 minutes per epoch to train. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n",
    "\n",
    "**We encourage you to try and tune the model to improve overall task performance or CER. Report your findings if you experiment**\n",
    "\n",
    "** Paste screenshots from your Weights & Biases dashboard of your loss curves and CER curve in the cell marked \"Plots\". Remember to include learning curves for the auxiliary tasks!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "rtKpwCEGIbz5"
   },
   "outputs": [],
   "source": [
    "class LightningCTCLASMTL(LightningLASMTL):\n",
    "\n",
    "  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n",
    "               wav_max_length=200, transcript_max_length=200, \n",
    "               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n",
    "               encoder_num_layers=2, encoder_hidden_dim=256, \n",
    "               encoder_bidirectional=True, encoder_dropout=0, \n",
    "               decoder_hidden_dim=256, decoder_num_layers=1,\n",
    "               decoder_multi_head=1, decoder_mlp_dim=128,\n",
    "               asr_label_smooth=0.1, teacher_force_prob=0.9,\n",
    "               ctc_weight=0.5, asr_weight=1.0, task_type_weight=1.0, \n",
    "               dialog_acts_weight=1.0, sentiment_weight=1.0):\n",
    "    super().__init__(\n",
    "      n_mels=n_mels, \n",
    "      n_fft=n_fft,\n",
    "      win_length=win_length,\n",
    "      hop_length=hop_length,\n",
    "      wav_max_length=wav_max_length, \n",
    "      transcript_max_length=transcript_max_length,\n",
    "      learning_rate=learning_rate, \n",
    "      batch_size=batch_size, \n",
    "      weight_decay=weight_decay, \n",
    "      encoder_num_layers=encoder_num_layers, \n",
    "      encoder_hidden_dim=encoder_hidden_dim, \n",
    "      encoder_bidirectional=encoder_bidirectional,\n",
    "      encoder_dropout=encoder_dropout,\n",
    "      decoder_hidden_dim=decoder_hidden_dim,\n",
    "      decoder_num_layers=decoder_num_layers,\n",
    "      decoder_multi_head=decoder_multi_head,\n",
    "      decoder_mlp_dim=decoder_mlp_dim,\n",
    "      asr_label_smooth=asr_label_smooth,\n",
    "      teacher_force_prob=teacher_force_prob,\n",
    "      asr_weight=asr_weight, \n",
    "      task_type_weight=task_type_weight, \n",
    "      dialog_acts_weight=dialog_acts_weight,\n",
    "      sentiment_weight=sentiment_weight)\n",
    "    self.save_hyperparameters()\n",
    "    self.ctc_weight = ctc_weight\n",
    "\n",
    "  def create_model(self):\n",
    "    model = JointCTCAttention(\n",
    "      self.train_dataset.input_dim,\n",
    "      self.train_dataset.num_class,\n",
    "      self.transcript_max_length,\n",
    "      listener_hidden_dim=self.encoder_hidden_dim,\n",
    "      listener_bidirectional=self.encoder_bidirectional,\n",
    "      num_pyramid_layers=self.encoder_num_layers,\n",
    "      dropout=self.encoder_dropout,\n",
    "      speller_hidden_dim=self.decoder_hidden_dim,\n",
    "      speller_num_layers=self.decoder_num_layers,\n",
    "      mlp_hidden_dim=self.decoder_mlp_dim,\n",
    "      multi_head=self.decoder_multi_head,\n",
    "      sos_index=self.train_dataset.sos_index,\n",
    "      sample_decode=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "  def forward(self, inputs, input_lengths, labels, label_lengths):\n",
    "    ctc_log_probs, las_log_probs, embedding = self.model(\n",
    "      inputs,\n",
    "      ground_truth=labels,\n",
    "      teacher_force_prob=self.teacher_force_prob)\n",
    "    return (ctc_log_probs, las_log_probs), embedding\n",
    "\n",
    "  def get_loss(self, log_probs, input_lengths, labels, label_lengths):\n",
    "    (ctc_log_probs, las_log_probs) = log_probs\n",
    "    ctc_loss, las_loss = self.model.get_loss(\n",
    "      ctc_log_probs,\n",
    "      las_log_probs,\n",
    "      input_lengths,\n",
    "      labels,\n",
    "      label_lengths,\n",
    "      self.train_dataset.num_labels,\n",
    "      pad_index=self.train_dataset.pad_index,\n",
    "      blank_index=self.train_dataset.eps_index,\n",
    "      label_smooth=self.asr_label_smooth)\n",
    "    loss = self.ctc_weight * ctc_loss + (1 - self.ctc_weight) * las_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "r8ZrGMpCxBKN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Constructing HarperValleyBank train dataset...\n",
      "> Constructing HarperValleyBank val dataset...\n",
      "> Constructing HarperValleyBank test dataset...\n",
      "> Constructing HarperValleyBank train dataset...\n",
      "> Constructing HarperValleyBank val dataset...\n",
      "> Constructing HarperValleyBank test dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:m6hshb5a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ctc_mtl</strong> at: <a href='https://wandb.ai/dacphuc1993/cs224s/runs/m6hshb5a' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/runs/m6hshb5a</a><br/> View job at <a href='https://wandb.ai/dacphuc1993/cs224s/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyODg5MzA0MQ==/version_details/v3' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyODg5MzA0MQ==/version_details/v3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240111_084523-m6hshb5a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:m6hshb5a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Phuc\\Desktop\\CS224S-slp\\HW3\\wandb\\run-20240111_094049-52ebs0qr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dacphuc1993/cs224s/runs/52ebs0qr' target=\"_blank\">ctc_las_mtl</a></strong> to <a href='https://wandb.ai/dacphuc1993/cs224s' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dacphuc1993/cs224s' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dacphuc1993/cs224s/runs/52ebs0qr' target=\"_blank\">https://wandb.ai/dacphuc1993/cs224s/runs/52ebs0qr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model             | JointCTCAttention    | 521 K \n",
      "1 | task_type_model   | TaskTypeClassifier   | 3.3 M \n",
      "2 | dialog_acts_model | DialogActsClassifier | 3.3 M \n",
      "3 | sentiment_model   | SentimentClassifier  | 3.3 M \n",
      "-----------------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "41.916    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                                                                                                                        | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phuc\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                                                                                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x256 and 128x25730)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 32\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run Joint CTC/LAS-MTL\u001b[39;00m\n\u001b[0;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_mels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m128\u001b[39m, \n\u001b[0;32m      5\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_fft\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.25\u001b[39m,\n\u001b[0;32m     30\u001b[0m }\n\u001b[1;32m---> 32\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLightningCTCLASMTL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mctc_las_mtl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_asr_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(system, config, ckpt_dir, epochs, monitor_key, use_gpu, seed)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m   trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     36\u001b[0m       max_epochs\u001b[38;5;241m=\u001b[39mepochs, min_epochs\u001b[38;5;241m=\u001b[39mepochs, enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m       callbacks\u001b[38;5;241m=\u001b[39mcheckpoint_callback, logger\u001b[38;5;241m=\u001b[39mwandb_logger)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1062\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[43], line 164\u001b[0m, in \u001b[0;36mLightningCTCMTL.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m--> 164\u001b[0m   loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_multi_task_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "Cell \u001b[1;32mIn[43], line 86\u001b[0m, in \u001b[0;36mLightningCTCMTL.get_multi_task_loss\u001b[1;34m(self, batch, split)\u001b[0m\n\u001b[0;32m     84\u001b[0m dialog_acts_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     85\u001b[0m sentiment_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 86\u001b[0m task_type_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m dialog_acts_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialog_acts_model(embedding)\n\u001b[0;32m     88\u001b[0m sentiment_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_model(embedding)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 14\u001b[0m, in \u001b[0;36mTaskTypeClassifier.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     10\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m############################ START OF YOUR CODE ############################\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# TODO(4.2)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Hint: This is an N-way classification problem.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     15\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(inputs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m############################# END OF YOUR CODE #############################\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x256 and 128x25730)"
     ]
    }
   ],
   "source": [
    "# Run Joint CTC/LAS-MTL\n",
    "\n",
    "config = {\n",
    "  'n_mels': 128, \n",
    "  'n_fft': 256,\n",
    "  'win_length': 256,\n",
    "  'hop_length': 128,\n",
    "  'wav_max_length': 2048, \n",
    "  'transcript_max_length': 500,\n",
    "  'learning_rate': 4e-3, \n",
    "  'batch_size': 128, \n",
    "  'weight_decay': 0, \n",
    "  'encoder_num_layers': 2,    # can't shrink output too much... \n",
    "  'encoder_hidden_dim': 64, \n",
    "  'encoder_bidirectional': True,\n",
    "  'encoder_dropout': 0,\n",
    "  'decoder_hidden_dim': 128,  # must be 2 x encoder_hidden_dim\n",
    "  'decoder_num_layers': 1,\n",
    "  'decoder_multi_head': 1,\n",
    "  'decoder_mlp_dim': 64,\n",
    "  'asr_label_smooth': 0.1,\n",
    "  'teacher_force_prob': 0.9,\n",
    "  # you may wish to play with these weights; try to keep the sum\n",
    "  # of them equal to one.\n",
    "  'ctc_weight': 0.5,  # equal weight between ctc and las?\n",
    "  'asr_weight': 0.25, \n",
    "  'task_type_weight': 0.25,\n",
    "  'dialog_acts_weight': 0.25,\n",
    "  'sentiment_weight': 0.25,\n",
    "}\n",
    "\n",
    "run(system=\"LightningCTCLASMTL\", config=config, epochs=20, \n",
    "    ckpt_dir='ctc_las_mtl', monitor_key='val_asr_loss', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWm7yH7ll9tJ"
   },
   "outputs": [],
   "source": [
    "# You can find the saved checkpoint here:\n",
    "!ls /content/cs224s_spring2022/trained_models/ctc_las_mtl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QUxJkdq80Bi"
   },
   "source": [
    "---\n",
    "\n",
    "**Plots:**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iWcrpxLYck3"
   },
   "source": [
    "** Using your plots as evidence in your description, answer the following questions:**\n",
    "\n",
    "a) Report performance metrics on each of the auxiliary tasks and the CER of your jointly trained model.\n",
    "\n",
    "b) Is your best CER lower than from LAS-MTL?\n",
    "\n",
    "c) What effects do you observe CTC has on LAS in terms of its learning curve? (e.g. How does the speed of learning compare to that of LAS? In what ways could CTC be improving LAS? Please explain thoroughly.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ownof9Jt2S3r"
   },
   "source": [
    "---\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMhSTNY-Gb0-"
   },
   "source": [
    "# Part 6: One Model to Hear Them All\n",
    "Congratulations, by this point you have trained multiple end-to-end deep learning neural networks for automatic speech recognition!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8nXgReDYn4M"
   },
   "source": [
    "## **Task 6.1: Train and summarize your best model [5 Points]**\n",
    "\n",
    "**Training & Written Response**\n",
    "\n",
    "**Note:** You are welcome to conduct additional experiments in this part. Please copy cells from above into Part 7 of the notebook if you wish to utilize them.\n",
    "\n",
    "** Alter any model of your choice or training procedure to improve performance! Describe what you tried, and report performance of your best model.** Include in your answer your design choices of:\n",
    "- Type of neural network (CTC, LAS, or Joint CTC-Attention)\n",
    "- Any auxiliary task(s) and weighting of tasks you may have used\n",
    "- Training hyperparameters (e.g. learning rate)\n",
    "\n",
    "You should attempt to improve your model performance in a reasonable way given what you have observed so far. You do not need to exhaustively optimize performance though; training at least 2 new models with adjustments is enough to obtain full credit for the homework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZIdGGOo2Is7"
   },
   "source": [
    "---\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvoyMHdH7rXs"
   },
   "outputs": [],
   "source": [
    "## Code to train your best model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh7lhrPAb_u1"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLN0ItdPrtwP"
   },
   "source": [
    "**Great work!** You have completed the final assignment of the course, and in doing so you have trained deep acoustic models from scratch on a speech dataset, and built intuition for different architectures and design choices for jointly training ASR along with other tasks.\n",
    "\n",
    "**Gradescope Submission**\n",
    "- Download your Colab notebook **with all cells fully executed** as a `.ipynb` file. Zip together your `.ipynb` with any supporting files. Submit this zipped file under `Assignment 3: Code Submission`.\n",
    "- Open your `.ipynb` file locally and save it as a PDF. Submit this PDF under `Assignment 3: PDF Submission` and **tag all pages corresponding to each task**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA-eoZYubhSd"
   },
   "source": [
    "# Optional: Testing Your System!\n",
    "\n",
    "**You can test your system with your own voice samples! Make 2 short recordings of yourself: 1) one on any topic and 2) another on a topic that more closely matches utterances in the HarperValleyBank dataset.** Save/convert them as `.WAV` files and upload them to your `DATA_PATH` directory. Then use `run_inference` to get your best model's transcriptions on them.\n",
    "\n",
    "- How does it do? Are the transcripts accurate?\n",
    "- Does the model generalize? If not, why do you think that is? What changes could be made to help the model generalize better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JMjmSXy1qO4"
   },
   "source": [
    "---\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Npf3piJcidS6"
   },
   "outputs": [],
   "source": [
    "# Load your saved model weights.\n",
    "\n",
    "system = None\n",
    "wav = None\n",
    "device = torch.device('cuda')\n",
    "sr = 8000\n",
    "############################## START OF YOUR CODE ##############################\n",
    "# TODO(7.2)\n",
    "# Use system.eval() after you load your PyTorch Lightning system weights.\n",
    "# Use `librosa.load` (refer to https://librosa.org/doc/latest/index.html).\n",
    "# Use the default target sample rate of 8000.\n",
    "# Use `librosa.effects.trim` to remove leading and trailing silences.\n",
    "\n",
    "############################### END OF YOUR CODE ###############################\n",
    "\n",
    "run_inference(system, wav, device=device, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmKqOxgD4I6_"
   },
   "source": [
    "# Optional: Listen, Attend and Spell (LAS) Neural Network\n",
    "\n",
    "This is a stand alone LAS model for the dataset. It tends to overfit on the small HVB training set. You do not need this code for the homework, we leave the LAS section here as a reference for you. \n",
    "\n",
    "This experiment uses an attention-based encoder-decoder model from the paper [Listen, Attend and Spell](https://arxiv.org/pdf/1508.01211.pdf) (Chan et al.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJH-HtjHyGUt"
   },
   "source": [
    "## LAS check for understanding\n",
    "\n",
    "** Read through the paper. Think about the following questions:**\n",
    "\n",
    "a) What is a key advantage of LAS over CTC?\n",
    "\n",
    "\n",
    "b) How does LAS handle words it may not have seen during training?\n",
    "\n",
    "\n",
    "c) **Teacher forcing** is a term used to describe a training technique where an RNN feeds the previous time step ground truth as the current time step input. In LAS, when is teacher forcing used and how often? What are some possible downsides to teacher forcing?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDQFnl2udiUx"
   },
   "source": [
    "## LAS Network\n",
    "Implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Y_E08yPO4nBa"
   },
   "outputs": [],
   "source": [
    "from utils import (reshape_and_apply, label_smooth_loss)\n",
    "\n",
    "\n",
    "class Listener(nn.Module):\n",
    "  \"\"\"Stacks 3 layers of PyramidLSTMLayers to reduce resolution 8 times.\n",
    "\n",
    "  Args:\n",
    "    input_dim: Number of input features.\n",
    "    hidden_dim: Number of hidden features.\n",
    "    num_pyramid_layers: Number of stacked lstm layers. (default: 3)\n",
    "    dropout: Dropout probability. (default: 0)\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "      self, input_dim, hidden_dim, num_pyramid_layers=3, dropout=0., \n",
    "      bidirectional=True):\n",
    "    super().__init__()\n",
    "    self.rnn_layer0 = PyramidLSTMLayer(input_dim, hidden_dim, num_layers=1,\n",
    "                                        bidirectional=True, dropout=dropout)\n",
    "    for i in range(1, num_pyramid_layers):\n",
    "      setattr(\n",
    "          self, \n",
    "          f'rnn_layer{i}',\n",
    "          PyramidLSTMLayer(hidden_dim * 2, hidden_dim, num_layers=1,\n",
    "                            bidirectional=bidirectional, dropout=dropout),\n",
    "      )\n",
    "    \n",
    "    self.num_pyramid_layers = num_pyramid_layers\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    outputs, hiddens = self.rnn_layer0(inputs)\n",
    "    for i in range(1, self.num_pyramid_layers):\n",
    "      outputs, hiddens = getattr(self, f'rnn_layer{i}')(outputs)\n",
    "    return outputs, hiddens\n",
    "        \n",
    "\n",
    "class PyramidLSTMLayer(nn.Module):\n",
    "  \"\"\"A Pyramid LSTM layer is a standard LSTM layer that halves the size \n",
    "  of the input in its hidden embeddings.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_dim, hidden_dim, num_layers=1,\n",
    "                bidirectional=True, dropout=0.):\n",
    "    super().__init__()\n",
    "    self.rnn = nn.LSTM(input_dim * 2, hidden_dim, num_layers=num_layers,\n",
    "                        bidirectional=bidirectional, dropout=dropout,\n",
    "                        batch_first=True)\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_layers = num_layers\n",
    "    self.bidirectional = bidirectional\n",
    "    self.dropout = dropout\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    batch_size, maxlen, input_dim = inputs.size()\n",
    "    \n",
    "    # reduce time resolution?\n",
    "    inputs = inputs.contiguous().view(batch_size, maxlen // 2, input_dim * 2)\n",
    "    outputs, hiddens = self.rnn(inputs)\n",
    "    return outputs, hiddens\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "  \"\"\"Attention module that trains an MLP to get attention weights.\"\"\"\n",
    "  def __init__(self, input_dim, hidden_dim, multi_head=1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.phi = nn.Linear(input_dim, hidden_dim*multi_head)\n",
    "    self.psi = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    if multi_head > 1:\n",
    "      self.fc_reduce = nn.Linear(input_dim*multi_head, input_dim)\n",
    "\n",
    "    self.multi_head = multi_head\n",
    "    self.hidden_dim = hidden_dim\n",
    "  \n",
    "  def forward(self, decoder_state, listener_feat):\n",
    "    attention_score = None\n",
    "    context = None\n",
    "    input_dim = listener_feat.size(2)\n",
    "    # decoder_state: batch_size x 1 x decoder_hidden_dim\n",
    "    # listener_feat: batch_size x maxlen x input_dim\n",
    "    comp_decoder_state = F.relu(self.phi(decoder_state))\n",
    "    comp_listener_feat = F.relu(reshape_and_apply(self.psi, listener_feat))\n",
    "\n",
    "    if self.multi_head == 1:\n",
    "      energy = torch.bmm(\n",
    "          comp_decoder_state,\n",
    "          comp_listener_feat.transpose(1, 2)\n",
    "      ).squeeze(1)\n",
    "      attention_score = [F.softmax(energy, dim=-1)]\n",
    "      weights = attention_score[0].unsqueeze(2).repeat(1, 1, input_dim)\n",
    "      context = torch.sum(listener_feat * weights, dim=1)\n",
    "    else:\n",
    "      attention_score = []\n",
    "      for att_query in torch.split(comp_decoder_state, self.hidden_dim, dim=-1):\n",
    "        score = torch.softmax(\n",
    "            torch.bmm(att_query,\n",
    "                      comp_listener_feat.transpose(1, 2)).squeeze(dim=1),\n",
    "        )\n",
    "        attention_score.append(score)\n",
    "      \n",
    "      projected_src = []\n",
    "      for att_s in attention_score:\n",
    "        weights = att_s.unsqueeze(2).repeat(1, 1, input_dim)\n",
    "        proj = torch.sum(listener_feat * weights, dim=1)\n",
    "        projected_src.append(proj)\n",
    "      \n",
    "      context = self.fc_reduce(torch.cat(projected_src, dim=-1))\n",
    "\n",
    "    # context is the entries of listener input weighted by attention\n",
    "    return attention_score, context\n",
    "\n",
    "\n",
    "class Speller(nn.Module):\n",
    "  \"\"\"Decoder that uses a LSTM with attention to convert a sequence of\n",
    "  hidden embeddings to a sequence of probabilities for output classes.\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "        self, num_labels, label_maxlen, speller_hidden_dim,\n",
    "        listener_hidden_dim, mlp_hidden_dim, num_layers=1, multi_head=1,\n",
    "        sos_index=0, sample_decode=False):\n",
    "    super().__init__()\n",
    "    self.rnn = nn.LSTM(num_labels + speller_hidden_dim,\n",
    "                        speller_hidden_dim, num_layers=num_layers,\n",
    "                        batch_first=True)\n",
    "    self.attention = AttentionLayer(listener_hidden_dim * 2, mlp_hidden_dim, \n",
    "                                    multi_head=multi_head)\n",
    "    self.fc_out = nn.Linear(speller_hidden_dim*2, num_labels)\n",
    "    self.num_labels = num_labels\n",
    "    self.label_maxlen = label_maxlen\n",
    "    self.sample_decode = sample_decode\n",
    "    self.sos_index = sos_index\n",
    "\n",
    "  def step(self, inputs, last_hiddens, listener_feats):\n",
    "    outputs, cur_hiddens = self.rnn(inputs, last_hiddens)\n",
    "    attention_score, context = self.attention(outputs, listener_feats)\n",
    "    features = torch.cat((outputs.squeeze(1), context), dim=-1)\n",
    "    logits = self.fc_out(features)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    return log_probs, cur_hiddens, context, attention_score\n",
    "\n",
    "  def forward(\n",
    "      self, listener_feats, ground_truth=None, teacher_force_prob=0.9):\n",
    "    device = listener_feats.device\n",
    "    if ground_truth is None:\n",
    "      teacher_force_prob = 0\n",
    "    teacher_force = np.random.random_sample() < teacher_force_prob\n",
    "    \n",
    "    batch_size = listener_feats.size(0)\n",
    "    with torch.no_grad():\n",
    "      output_toks = torch.zeros((batch_size, 1, self.num_labels), device=device)\n",
    "      output_toks[:, 0, self.sos_index] = 1\n",
    "\n",
    "    rnn_inputs = torch.cat([output_toks, listener_feats[:, 0:1, :]], dim=-1)\n",
    "\n",
    "    hidden_state = None\n",
    "    log_probs_seq = []\n",
    "\n",
    "    if (ground_truth is None) or (not teacher_force_prob):\n",
    "      max_step = int(self.label_maxlen)\n",
    "    else:\n",
    "      max_step = int(ground_truth.size(1))\n",
    "\n",
    "    for step in range(max_step):\n",
    "      log_probs, hidden_state, context, _ = self.step(\n",
    "          rnn_inputs, hidden_state, listener_feats)\n",
    "      log_probs_seq.append(log_probs.unsqueeze(1))\n",
    "\n",
    "      if teacher_force:\n",
    "        gt_tok = ground_truth[:, step:step+1].float()\n",
    "        output_tok = torch.zeros_like(log_probs)\n",
    "        for idx, i in enumerate(gt_tok):\n",
    "            output_tok[idx, int(i.item())] = 1\n",
    "        output_tok = output_tok.unsqueeze(1)\n",
    "      else:\n",
    "        if self.sample_decode:\n",
    "          probs = torch.exp(log_probs)\n",
    "          sampled_tok = Categorical(probs).sample()\n",
    "        else:  # Pick max probability (greedy decoding)\n",
    "          output_tok = torch.zeros_like(log_probs)\n",
    "          sampled_tok = log_probs.topk(1)[1]\n",
    "\n",
    "        output_tok = torch.zeros_like(log_probs)\n",
    "        for idx, i in enumerate(sampled_tok):\n",
    "          output_tok[idx, int(i.item())] = 1\n",
    "        output_tok = output_tok.unsqueeze(1)\n",
    "\n",
    "      rnn_inputs = torch.cat([output_tok, context.unsqueeze(1)], dim=-1)\n",
    "\n",
    "    # batch_size x maxlen x num_labels\n",
    "    log_probs_seq = torch.cat(log_probs_seq, dim=1)\n",
    "\n",
    "    return log_probs_seq.contiguous()\n",
    "\n",
    "\n",
    "class LASEncoderDecoder(nn.Module):\n",
    "  def __init__(\n",
    "      self, input_dim, num_class, label_maxlen, listener_hidden_dim=128, \n",
    "      listener_bidirectional=True, num_pyramid_layers=3, dropout=0, \n",
    "      speller_hidden_dim=256, speller_num_layers=1, mlp_hidden_dim=128, \n",
    "      multi_head=1, sos_index=0, sample_decode=False):\n",
    "    super().__init__()\n",
    "    # Encoder.\n",
    "    self.listener = Listener(input_dim, listener_hidden_dim,\n",
    "                              num_pyramid_layers=num_pyramid_layers,\n",
    "                              dropout=dropout,\n",
    "                              bidirectional=listener_bidirectional)\n",
    "    # Decoder.\n",
    "    self.speller = Speller(num_class, label_maxlen, speller_hidden_dim,\n",
    "                            listener_hidden_dim, mlp_hidden_dim,\n",
    "                            num_layers=speller_num_layers, \n",
    "                            multi_head=multi_head,\n",
    "                            sos_index=sos_index,\n",
    "                            sample_decode=sample_decode)\n",
    "    self.embedding_dim = listener_hidden_dim * 4\n",
    "\n",
    "  def combine_h_and_c(self, h, c):\n",
    "    batch_size = h.size(1)\n",
    "    h = h.permute(1, 0, 2).contiguous()\n",
    "    c = c.permute(1, 0, 2).contiguous()\n",
    "    h = h.view(batch_size, -1)\n",
    "    c = c.view(batch_size, -1)\n",
    "    return torch.cat([h, c], dim=1)\n",
    "\n",
    "  def forward(\n",
    "      self, inputs, ground_truth=None, teacher_force_prob=0.9):\n",
    "    log_probs = None\n",
    "    h, c = None, None\n",
    "    # this is the main model connection for forward prop\n",
    "    outputs, (h, c) = self.listener(inputs)\n",
    "    log_probs = self.speller(outputs, ground_truth=ground_truth, teacher_force_prob=teacher_force_prob)\n",
    "\n",
    "    combined_h_and_c = self.combine_h_and_c(h, c)\n",
    "    return log_probs, combined_h_and_c\n",
    "\n",
    "  def get_loss(\n",
    "      self, log_probs, labels, num_labels, pad_index=0, label_smooth=0.1):\n",
    "    batch_size = log_probs.size(0)\n",
    "    labels_maxlen = labels.size(1)\n",
    "\n",
    "    if label_smooth == 0.0:\n",
    "      loss = F.nll_loss(log_probs.view(batch_size * labels_maxlen, -1),\n",
    "                        labels.long().view(batch_size * labels_maxlen),\n",
    "                        ignore_index=pad_index)\n",
    "    else:\n",
    "      # label_smooth_loss is the sample as F.nll_loss but with a temperature\n",
    "      # parameter that makes the log probability distribution \"sharper\".\n",
    "      loss = label_smooth_loss(log_probs, labels.float(), num_labels,\n",
    "                                smooth_param=label_smooth)\n",
    "    return loss\n",
    "\n",
    "  def decode(self, log_probs, input_lengths, labels, label_lengths,\n",
    "             sos_index, eos_index, pad_index, eps_index):\n",
    "    # Use greedy decoding.\n",
    "    decoded = torch.argmax(log_probs, dim=2)\n",
    "    batch_size = decoded.size(0)\n",
    "    # Collapse each decoded sequence using CTC rules.\n",
    "    hypotheses = []\n",
    "    hypothesis_lengths = []\n",
    "    references = []\n",
    "    reference_lengths = []\n",
    "    for i in range(batch_size):\n",
    "      decoded_i = decoded[i]\n",
    "      hypothesis_i = []\n",
    "      for tok in decoded_i:\n",
    "        if tok.item() == sos_index:\n",
    "          continue\n",
    "        if tok.item() == pad_index:\n",
    "          continue\n",
    "        if tok.item() == eos_index:\n",
    "          # once we reach an EOS token, we are done generating.\n",
    "          break\n",
    "        hypothesis_i.append(tok.item())\n",
    "      hypotheses.append(hypothesis_i)\n",
    "      hypothesis_lengths.append(len(hypothesis_i))\n",
    "\n",
    "      if labels is not None:\n",
    "        label_i = labels[i]\n",
    "        reference_i = [tok.item() for tok in labels[i]\n",
    "                      if tok.item() != sos_index and \n",
    "                          tok.item() != eos_index and \n",
    "                          tok.item() != pad_index]\n",
    "        references.append(reference_i)\n",
    "        reference_lengths.append(len(reference_i))\n",
    "    \n",
    "    if labels is None: # Run at inference time.\n",
    "      references, reference_lengths = None, None\n",
    "\n",
    "    return hypotheses, hypothesis_lengths, references, reference_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "zPSgNAJT4crM"
   },
   "outputs": [],
   "source": [
    "class LightningLASMTL(LightningCTCMTL):\n",
    "  \"\"\"Train a Listen-Attend-Spell model along with the Multi-Task Objevtive.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n",
    "               wav_max_length=200, transcript_max_length=200, \n",
    "               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n",
    "               encoder_num_layers=2, encoder_hidden_dim=256, \n",
    "               encoder_bidirectional=True, encoder_dropout=0, \n",
    "               decoder_hidden_dim=256, decoder_num_layers=1,\n",
    "               decoder_multi_head=1, decoder_mlp_dim=128,\n",
    "               asr_label_smooth=0.1, teacher_force_prob=0.9,\n",
    "               asr_weight=1.0, task_type_weight=1.0, \n",
    "               dialog_acts_weight=1.0, sentiment_weight=1.0):\n",
    "    self.encoder_dropout = encoder_dropout\n",
    "    self.decoder_hidden_dim = decoder_hidden_dim\n",
    "    self.decoder_num_layers = decoder_num_layers\n",
    "    self.decoder_mlp_dim = decoder_mlp_dim\n",
    "    self.decoder_multi_head = decoder_multi_head\n",
    "    self.asr_label_smooth = asr_label_smooth\n",
    "    self.teacher_force_prob = teacher_force_prob\n",
    "\n",
    "    super().__init__(\n",
    "      n_mels=n_mels, n_fft=n_fft,\n",
    "      win_length=win_length, hop_length=hop_length,\n",
    "      wav_max_length=wav_max_length, \n",
    "      transcript_max_length=transcript_max_length,\n",
    "      learning_rate=learning_rate, \n",
    "      batch_size=batch_size, \n",
    "      weight_decay=weight_decay, \n",
    "      encoder_num_layers=encoder_num_layers, \n",
    "      encoder_hidden_dim=encoder_hidden_dim, \n",
    "      encoder_bidirectional=encoder_bidirectional,\n",
    "      asr_weight=asr_weight, \n",
    "      task_type_weight=task_type_weight, \n",
    "      dialog_acts_weight=dialog_acts_weight,\n",
    "      sentiment_weight=sentiment_weight)\n",
    "    self.save_hyperparameters()\n",
    "\n",
    "  def create_model(self):\n",
    "    model = LASEncoderDecoder(\n",
    "      self.train_dataset.input_dim,\n",
    "      self.train_dataset.num_class,\n",
    "      self.transcript_max_length,\n",
    "      listener_hidden_dim=self.encoder_hidden_dim,\n",
    "      listener_bidirectional=self.encoder_bidirectional, \n",
    "      num_pyramid_layers=self.encoder_num_layers,\n",
    "      dropout=self.encoder_dropout,\n",
    "      speller_hidden_dim=self.decoder_hidden_dim,\n",
    "      speller_num_layers=self.decoder_num_layers,\n",
    "      mlp_hidden_dim=self.decoder_mlp_dim,\n",
    "      multi_head=self.decoder_multi_head,\n",
    "      sos_index=self.train_dataset.sos_index,\n",
    "      sample_decode=False)\n",
    "    return model\n",
    "\n",
    "  def create_datasets(self):\n",
    "    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n",
    "    train_dataset = HarperValleyBankMTL(\n",
    "      root, split='train', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "      win_length=self.win_length, hop_length=self.hop_length, \n",
    "      wav_max_length=self.wav_max_length,\n",
    "      transcript_max_length=self.transcript_max_length,\n",
    "      append_eos_token=True)  # LAS adds a EOS token to the end of a sequence\n",
    "    val_dataset = HarperValleyBankMTL(\n",
    "      root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "      win_length=self.win_length, hop_length=self.hop_length,\n",
    "      wav_max_length=self.wav_max_length,\n",
    "      transcript_max_length=self.transcript_max_length,\n",
    "      append_eos_token=True) \n",
    "    test_dataset = HarperValleyBankMTL(\n",
    "      root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n",
    "      win_length=self.win_length, hop_length=self.hop_length,\n",
    "      wav_max_length=self.wav_max_length,\n",
    "      transcript_max_length=self.transcript_max_length,\n",
    "      append_eos_token=True) \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "  def forward(self, inputs, input_lengths, labels, label_lengths):\n",
    "    log_probs, embedding = self.model(\n",
    "      inputs,\n",
    "      ground_truth=labels,\n",
    "      teacher_force_prob=self.teacher_force_prob,\n",
    "    )\n",
    "    return log_probs, embedding\n",
    "\n",
    "  def get_loss(self, log_probs, input_lengths, labels, label_lengths):\n",
    "    loss = self.model.get_loss(log_probs, labels,\n",
    "      self.train_dataset.num_labels,\n",
    "      pad_index=self.train_dataset.pad_index,\n",
    "      label_smooth=self.asr_label_smooth)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg1VaakFVtBq"
   },
   "source": [
    "## Training LAS-MTL \n",
    "\n",
    "**This section will train the LAS-MTL network with the default hyperparameters we provide.**\n",
    "\n",
    "LAS is more expensive than CTC to train. Each epoch takes roughly 6 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJjrPvUWxMLD"
   },
   "outputs": [],
   "source": [
    "# Run LAS-MTL\n",
    "\n",
    "config = {\n",
    "  'n_mels': 128, \n",
    "  'n_fft': 256,\n",
    "  'win_length': 256,\n",
    "  'hop_length': 128,\n",
    "  'wav_max_length': 512, \n",
    "  'transcript_max_length': 200,\n",
    "  'learning_rate': 4e-3,      # faster learning rate\n",
    "  'batch_size': 128, \n",
    "  'weight_decay': 0, \n",
    "  'encoder_num_layers': 2, \n",
    "  'encoder_hidden_dim': 64, \n",
    "  'encoder_bidirectional': True,\n",
    "  'encoder_dropout': 0,\n",
    "  'decoder_hidden_dim': 128,  # must be 2 x encoder_hidden_dim\n",
    "  'decoder_num_layers': 1,\n",
    "  'decoder_multi_head': 1,\n",
    "  'decoder_mlp_dim': 64,\n",
    "  'asr_label_smooth': 0.1,\n",
    "  'teacher_force_prob': 0.9,\n",
    "  # You may wish to adjust these weights.\n",
    "  # Keep the sum of them equal to one.\n",
    "  'asr_weight': 0.25, \n",
    "  'task_type_weight': 0.25,\n",
    "  'dialog_acts_weight': 0.25,\n",
    "  'sentiment_weight': 0.25,\n",
    "}\n",
    "\n",
    "run(system=\"LightningLASMTL\", config=config, epochs=20, ckpt_dir='las_mtl', \n",
    "    monitor_key='val_asr_loss', use_gpu=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YA-eoZYubhSd"
   ],
   "name": "CS224S_HW3_2022_E2E",
   "provenance": [
    {
     "file_id": "1dPQL8Jp82owJcEoo8SnJ-P12ODuY6s6-",
     "timestamp": 1650953820102
    },
    {
     "file_id": "1Srrk-wxDFxtvKnAjzPWtWKG-A9weHDD4",
     "timestamp": 1649198669533
    },
    {
     "file_id": "1yMCxBCJN9uQm5j2c5XioGN8h4V-M4VTi",
     "timestamp": 1614325884607
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
