{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS224S_HW3_2022_E2E","provenance":[{"file_id":"1dPQL8Jp82owJcEoo8SnJ-P12ODuY6s6-","timestamp":1650953820102},{"file_id":"1Srrk-wxDFxtvKnAjzPWtWKG-A9weHDD4","timestamp":1649198669533},{"file_id":"1yMCxBCJN9uQm5j2c5XioGN8h4V-M4VTi","timestamp":1614325884607}],"collapsed_sections":["YA-eoZYubhSd"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QgfqwLKSPaAw"},"source":["# CS224S Assignment 3: Deep Learning for End-to-End Speech Recognition\n","\n","---\n"]},{"cell_type":"markdown","source":["This notebook is worth 70 / 100 possible points for homework 3. You should be able to train all models in Colab. We encourage you to read general PyTorch / Lightning tutorials as necessary as you work. "],"metadata":{"id":"zXsGVZtG-dzr"}},{"cell_type":"markdown","metadata":{"id":"rh8A_5L1dq6-"},"source":["**Note:** You will need to make a copy of this Colab notebook in your Google Drive before you can edit it.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"M9ahWfoydgHv"},"source":["#@title Setup Cell\n","\n","#@markdown **Mount Google Drive.** We will use your `cs224s_spring2022`\n","#@markdown directory in your Google Drive to store all relevant files,\n","#@markdown including `utils.py`.\n","\n","# Do not modify.\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs224s_spring2022'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","SYM_PATH = '/content/cs224s_spring2022'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH\n","\n","#@markdown **Data.** It takes ~5-10 minutes to download the dataset the first\n","#@markdown time you run this cell. Afterwards it will stay saved in\n","#@markdown `cs224s_spring2022/data/harper_valley_bank_minified`.\n","\n","\n","DATA_PATH = '{}/data'.format(SYM_PATH)\n","if not os.path.exists(DATA_PATH):\n","  %mkdir $DATA_PATH\n","%cd $DATA_PATH\n","if not os.path.exists(os.path.join(DATA_PATH, 'harper_valley_bank_minified')):\n","  !wget -q http://web.stanford.edu/class/cs224s/download/harper_valley_bank_minified.zip\n","  !unzip -q harper_valley_bank_minified.zip\n","  %rm harper_valley_bank_minified.zip\n","\n","#@markdown **Experiments.** Model checkpoints will be saved in your\n","#@markdown `cs224s_spring2022/trained_models` directory.\n","MODEL_PATH = '{}/trained_models'.format(SYM_PATH)\n","if not os.path.exists(MODEL_PATH):\n","  %mkdir $MODEL_PATH\n","\n","%cd $SYM_PATH\n","if not os.path.exists(os.path.join(SYM_PATH, 'utils.py')):\n","  !wget -q http://web.stanford.edu/class/cs224s/download/utils.py\n","\n","#@markdown **Note on Sessions.** You have to run this cell each new session\n","#@markdown (sessions either expire after some time or after you close the notebook).\n","#@markdown You may have to periodically go to **Runtime** > **Factory reset runtime**\n","#@markdown if you are experiencing Colab environment issues. Requirements must be installed each new session.\n","\n","!pip -q install pytorch_lightning\n","!pip install wandb -qqq\n","\n","from collections import OrderedDict\n","from itertools import chain\n","\n","import h5py\n","import math\n","import json\n","import torch\n","import wandb\n","import numpy as np\n","import pytorch_lightning as pl\n","from glob import glob\n","import librosa\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import random\n","from sklearn.metrics import f1_score\n","from typing import *\n","from IPython.display import Audio\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.loggers import WandbLogger\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7eUiyEhfnYV"},"source":["# Part 1: ML Speech Data Pipeline\n","\n","HarperValleyBank consists of 23 hours of audio from 1,446 human-human conversations between 59 unique speakers. For your convenience, we store in `harper_valley_bank_minified` all utterance audio waveforms as `numpy` arrays in `data.h5` and all transcripts and labels as `numpy` arrays in `labels.npz`.\n","\n","Our custom dataset class `HarperValleyBank` should inherit `torch.utils.data.Dataset` and overwrite the following methods:\n","- `__len__` so that `len(dataset)` returns the size of the dataset.\n","- `__getitem__` to support the indexing such that `dataset[i]` can be used to get the `i`th dataset sample.\n","\n","There are a few special features that the `HarperValleyBank` class should exhibit.\n","- **Fixed-length data.** Both the extracted audio features and the character labels will inherently be sequences of different lengths. However, in order to store data in a minibatch during training, we need to make the lengths uniform. To do so, we can first enforce a maximum length for audio waves and a maximum length for labels (note that these two maximum lengths are not necessarily the same). We have preprocessed all sequences to be cropped by single utterances as opposed to conversations. Next, we can crop and pad each sequence with a pad token (e.g. `3`) such that all audio sequences and all label sequences are their respective maximum lengths. We will also store the actual lengths of each sequence so that the model does not learn from the padded indices.\n","- **Sequence representation.** We are training a character-level model, so the ASR model is responsible for predicting each spoken character. Therefore, we must convert our transcript text to a list of indices representing 34 possible characters (see the global variable `VOCAB`) and a few domain-specific tokens (see the global variable `SILENT_VOCAB` e.g. `[laughter]`). Think of each character as its own class.\n","```\n","Raw utterance:  hi this is an example .\n","List of characters: ['h', 'i', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ' ', '.']\n","List of indices: [18, 19, 3, 30, 18, 19, 29, 3, 19, 29, 3, 11, 24, 3, 15, 34, 11, 23, 26, 22, 15, 3, 6]\n","```\n","- **Special tokens.** Although this next part is provided, it is worth pointing out. Aside from the padding index, there are three special tokens in our vocabulary:\n","    - A blank token (`epsilon`, represented by index `0`) which designates a padded index and plays a special role in CTC.\n","    - A start-of-sentence token (`SOS`, represented by index `1`) which designates the start of a sentence.\n","    - An end-of-sentence token (`EOS`, represented by index `2`) which designates the end of a sentence.\n","```\n","Example label sequence: [18, 19, 3, 30, 18]\n","Add an END token: [18, 19, 3, 30, 18, 2]\n","```\n","Suppose the maximum label sequence has length 10.\n","```\n","Padded label sequence: [18, 19, 3, 30, 18, 2, 0, 0, 0, 0]\n","Label sequence length: 6\n","```\n","\n","**It may be helpful to first read through the `HarperValleyBank` starter code and `utils.py` to get familiar with the data pipeline.**\n","\n","Below, we provide a cell for you to index into the raw data and listen to randomly chosen samples."]},{"cell_type":"code","metadata":{"id":"iGmsjnx_karw"},"source":["root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n","waveform_h5 = h5py.File(os.path.join(root, 'data.h5'), 'r')\n","waveform_data = waveform_h5.get('waveforms')\n","label_data = np.load(os.path.join(root, 'labels.npz'))\n","assert len(waveform_data) == len(label_data['human_transcripts'])\n","index = random.randint(0, len(waveform_data) - 1)\n","w = waveform_data[f'{index}'][:]\n","t = label_data['human_transcripts'][index]\n","\n","print('index {}: \"{}\"\\n'.format(index, t))\n","Audio(w, rate=8000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3aU-u6laAzt"},"source":["## **Task 1.1: Set up primary task data (5 Points)**\n","\n","**Implementation**\n","\n","Notice that the `__getitem__` returns 4 objects:\n","- `inputs`: the padded log-Mel spectrogram features.\n","- `input_lengths`: the true length of the unpadded spectrogram features.\n","- `labels`: the padded character labels.\n","- `label_lengths`: the true length of the unpadded character labels.\n","\n","These objects will be used for what we call our *primary* task: speech recognition. In later parts, we will use *auxiliary* tasks to perform multi-task learning toward boosting speech recognition.\n","\n","**→ Implement the `get_primary_task_data` method.** This will be used in the `__getitem__` method of `HarperValleyBank` and later its subclass for multi-task learning, and it is responsible for extracting log-Mel spectrogram features from the raw audio clips. Do not modify other methods. You should pass the sanity check at the end.\n"]},{"cell_type":"code","metadata":{"id":"QQ2x5IuxfuZf"},"source":["from utils import (\n","  prune_transcripts, pad_wav, pad_transcript_label, get_transcript_labels,\n","  get_cer_per_sample)\n","\n","\n","# HarperValleyBank character vocabulary\n","VOCAB = [' ', \"'\", '~', '-', '.', '<', '>', '[', ']', 'a', 'b', 'c', 'd', 'e',\n","         'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's',\n","         't', 'u', 'v', 'w', 'x', 'y', 'z']\n","         \n","SILENT_VOCAB = ['[baby]', '[ringing]', '[laughter]', '[kids]', '[music]', \n","                '[noise]', '[unintelligible]', '[dogs]', '[cough]']\n","\n","\n","class HarperValleyBank(Dataset):\n","  \"\"\"Dataset to be used to train CTC, LAS, and MTL.\n","  \n","  Args:\n","    root: string\n","          path to the data files.\n","    split: string (default: train)\n","            choices: train | val | test\n","            which split of data to load\n","    n_mels: integer (default: 128)\n","            number of mel frequencies\n","    n_fft: integer (default: 256)\n","            number of fourier components\n","    win_length: integer (default: 256)\n","                should be <= n_fft\n","    hop_length: integer (default: 128)\n","                number of frames to skip in between\n","    wav_max_length: integer (default: 200)\n","                    maximum number of timesteps in a waveform\n","    transcript_max_length: integer (default: 200)\n","                            maximum number of timesteps in a transcript\n","    append_eos_token: boolean (default: False)\n","                      add EOS token to the end of every transcription\n","                      this is used for LAS (and LAS+CTC models)\n","  \"\"\"\n","  def __init__(\n","      self, root, split='train', n_mels=128, n_fft=256, win_length=256, \n","      hop_length=128, wav_max_length=200, transcript_max_length=200, \n","      append_eos_token=False):\n","    super().__init__()\n","    print(f'> Constructing HarperValleyBank {split} dataset...')\n","\n","    self.label_data = np.load(os.path.join(root, 'labels.npz'))   \n","    self.root = root\n","    self.wav_max_length = wav_max_length\n","    self.transcript_max_length = transcript_max_length\n","\n","    self.input_dim = n_mels\n","    self.n_mels = n_mels\n","    self.n_fft = n_fft\n","    self.win_length = win_length\n","    self.hop_length = hop_length\n","\n","    # Prune away very short examples.\n","    # This returns a list of indices of examples longer than 3 words.\n","    valid_indices = prune_transcripts(self.label_data['human_transcripts'])\n","\n","    # Decides which indices belong to which split.\n","    train_indices, val_indices, test_indices = self.split_data(valid_indices)\n","\n","    if split == 'train':\n","      indices = train_indices\n","    elif split == 'val':\n","      indices = val_indices\n","    elif split == 'test':\n","      indices = test_indices\n","    else:\n","      raise Exception(f'Split {split} not supported.')\n","\n","    raw_human_transcripts = self.label_data['human_transcripts'].tolist()\n","    human_transcript_labels = get_transcript_labels(\n","      raw_human_transcripts, VOCAB, SILENT_VOCAB)\n","  \n","    # Increment all indices by 4 to reserve the following special tokens:\n","    #   0 for epsilon\n","    #   1 for start-of-sentence (SOS)\n","    #   2 for end-of-sentence (EOS)\n","    #   3 for padding \n","    num_special_tokens = 4\n","    human_transcript_labels = [list(np.array(lab) + num_special_tokens) \n","                                for lab in human_transcript_labels]\n","    # CTC doesn't use SOS nor EOS; LAS doesn't use EPS but add anyway.\n","    eps_index, sos_index, eos_index, pad_index = 0, 1, 2, 3\n","\n","    if append_eos_token:\n","      # Ensert an EOS token to the end of all the labels.\n","      # This is important for the LAS objective.\n","      human_transcript_labels_ = []\n","      for i in range(len(human_transcript_labels)):\n","        new_label_i = human_transcript_labels[i] + [eos_index]\n","        human_transcript_labels_.append(new_label_i)\n","      human_transcript_labels = human_transcript_labels_\n","    self.human_transcript_labels = human_transcript_labels\n","  \n","    # Include epsilon, SOS, and EOS tokens.\n","    self.num_class = len(VOCAB) + len(SILENT_VOCAB) + num_special_tokens\n","    self.num_labels = self.num_class  # These are interchangeable.\n","    self.eps_index = eps_index\n","    self.sos_index = sos_index\n","    self.eos_index = eos_index\n","    self.pad_index = pad_index # Use this index for padding.\n","\n","    self.indices = indices\n","\n","  def indices_to_chars(self, indices):\n","    # indices: list of integers in vocab\n","    # add special characters in front (since we did this above)\n","    full_vocab = ['<eps>', '<sos>', '<eos>', '<pad>'] + VOCAB + SILENT_VOCAB\n","    chars = [full_vocab[ind] for ind in indices]\n","    return chars\n","\n","  def split_data(self, valid_indices, train_ratio = 0.8, val_ratio = 0.1):\n","    \"\"\"Splits data into train, val, and test sets based on speaker. When \n","    evaluating methods on the test split, we measure how well they generalize\n","    to new (unseen) speakers.\n","    \n","    Concretely, this stores and returns indices belonging to each split.\n","    \"\"\"\n","    # Fix seed so everyone reproduces the same splits.\n","    rs = np.random.RandomState(42)\n","\n","    speaker_ids = self.label_data['speaker_ids']\n","    unique_speaker_ids = sorted(list(set(speaker_ids)))\n","    unique_speaker_ids = np.array(unique_speaker_ids)\n","\n","    # Shuffle so the speaker IDs are distributed.\n","    rs.shuffle(unique_speaker_ids)\n","\n","    num_speaker = len(unique_speaker_ids)\n","    num_train = int(train_ratio * num_speaker)\n","    num_val = int(val_ratio * num_speaker)\n","    num_test = num_speaker - num_train - num_val\n","\n","    train_speaker_ids = unique_speaker_ids[:num_train]\n","    val_speaker_ids = unique_speaker_ids[num_train:num_train+num_val]\n","    test_speaker_ids = unique_speaker_ids[num_train+num_val:]\n","\n","    train_speaker_dict = dict(zip(train_speaker_ids, ['train'] * num_train))\n","    val_speaker_dict = dict(zip(val_speaker_ids, ['val'] * num_val))\n","    test_speaker_dict = dict(zip(test_speaker_ids, ['test'] * num_test))\n","    speaker_dict = {**train_speaker_dict, **val_speaker_dict, \n","                    **test_speaker_dict} \n","\n","    train_indices, val_indices, test_indices = [], [], []\n","    for i in range(len(speaker_ids)):\n","      speaker_id = speaker_ids[i]\n","      if speaker_dict[speaker_id] == 'train':\n","          train_indices.append(i)\n","      elif speaker_dict[speaker_id] == 'val':\n","          val_indices.append(i)\n","      elif speaker_dict[speaker_id] == 'test':\n","          test_indices.append(i)\n","      else:\n","          raise Exception('split not recognized.')\n","\n","    train_indices = np.array(train_indices)\n","    val_indices = np.array(val_indices)\n","    test_indices = np.array(test_indices)\n","\n","    # Make sure to only keep \"valid indices\" i.e. those with more than 4 \n","    # words in the transcription.\n","    train_indices = np.intersect1d(train_indices, valid_indices)\n","    val_indices = np.intersect1d(val_indices, valid_indices)\n","    test_indices = np.intersect1d(test_indices, valid_indices)\n","\n","    return train_indices, val_indices, test_indices\n","\n","  def get_primary_task_data(self, index):\n","    \"\"\"Returns audio and transcript information for a single utterance.\n","\n","    Args:\n","      index: Index of an utterance.\n","\n","    Returns:\n","      log melspectrogram, wav length, transcript label, transcript length\n","    \"\"\"\n","    input_feature = None\n","    input_length = None\n","    human_transcript_label = None\n","    human_transcript_length = None\n","\n","    wav = self.waveform_data[f'{index}'][:] # An h5py file uses string keys.\n","    sr = 8000 # We fix the sample rate for you.\n","\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(1.1)\n","    # - Compute the mel spectrogram of the audio crop.\n","    # - Convert the mel spectrogram to log space and normalize it.\n","    # - This is your primary task feature. Note that models will expect feature\n","    #   inputs of shape (T, n_mels).\n","    # - Pad the feature so that all features are fixed-length and\n","    #   convert it into a tensor.\n","    # - Likewise, retrieve and pad the corresponding transcript label sequence.\n","    #\n","    # Hint:\n","    # - Refer to https://librosa.org/doc/latest/index.html.\n","    # - Use `librosa.feature.melspectrogram` and `librosa.util.normalize`.\n","    # - Make sure to use our provided sr, n_mels, n_fft, win_length, \n","    # - and hop_length\n","    # - utils.py has helpful padding functions.\n","\n","    \n","\n","    ############################# END OF YOUR CODE #############################\n","\n","    return input_feature, input_length, human_transcript_label, human_transcript_length\n","\n","  def load_waveforms(self):\n","    # Make a file pointer to waveforms file.\n","    waveform_h5 = h5py.File(os.path.join(self.root, 'data.h5'), 'r')\n","    self.waveform_data = waveform_h5.get('waveforms')\n","\n","  def __getitem__(self, index):\n","    \"\"\"Serves primary task data for a single utterance.\"\"\"\n","    if not hasattr(self, 'waveform_data'):\n","      # Do this in __getitem__ function so we enable multiprocessing.\n","      self.load_waveforms()\n","    index = int(self.indices[index])\n","    return self.get_primary_task_data(index)\n","\n","  def __len__(self):\n","    \"\"\"Returns total number of utterances in the dataset.\"\"\"\n","    return len(self.indices)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LsxCZGiucJRc"},"source":["**Sanity check.** Let's check that your dataset implementation is correct. This will be important to properly run our experiments in later parts. In particular, make sure your `__getitem__` and `__len__` are implemented correctly."]},{"cell_type":"code","metadata":{"id":"lTi-avLRcIuE"},"source":[" # Do not modify.\n","root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n","train_dataset = HarperValleyBank(root, split='train')\n","val_dataset = HarperValleyBank(root, split='val')\n","test_dataset = HarperValleyBank(root, split='test')\n","\n","assert len(train_dataset) == 10402\n","assert len(val_dataset) == 679\n","assert len(test_dataset) == 2854 \n","\n","input, input_length, label, label_length = train_dataset.__getitem__(224)\n","assert input.size() == torch.Size([train_dataset.wav_max_length, train_dataset.n_mels])\n","assert input_length == 92\n","assert label_length == 26\n","print('\\nValidated dataset class implementation!')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WrjMYi5U6RkM"},"source":["# Part 2: Connectionist Temporal Classification (CTC) Neural Network\n","\n","Our first experiment will be a [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) (Graves et al.) model trained on our primary task of speech recognition.\n","\n","As an overview, given an input matrix of shape `batch_size x sequence_length x feature_dim`, the network encodes the input speech features with an LSTM, producing a tensor of shape `batch_size x sequence_length x hidden_dim`. Using an additional linear layer, we transform this to `batch_size x sequence_length x vocab_size`, representing the probability of transcribing each character in the vocabulary at each time step. This is directly given to the CTC loss function.\n","\n","We will use [Weights & Biases](https://wandb.ai) to log loss curves and character error rates (CER) in the cloud. You can create a free account [here](https://wandb.ai/site)."]},{"cell_type":"markdown","metadata":{"id":"mMBltzWIRWRI"},"source":["## **CTC Network**\n","\n","**Implementation**\n","\n","You will use the CTC objective to train your network. Previously, you implemented the CTC loss function from scratch. For this assignment, you may use PyTorch's implementation. Filling out this section will be necessary to carry out later experiments.\n","\n","**→ Fill out `get_ctc_loss` using `F.ctc_loss`.**\n","\n","**→ Read through the starter code and fill out the `forward` pass of `CTCEncoderDecoder`.**"]},{"cell_type":"code","metadata":{"id":"ua4VUy5fwlEZ"},"source":["def get_ctc_loss(\n","    log_probs, targets, input_lengths, target_lengths, blank=0):\n","  \"\"\"Connectionist Temporal Classification objective function.\"\"\"\n","  ctc_loss = None\n","  log_probs = log_probs.contiguous()\n","  targets = targets.long()\n","  input_lengths = input_lengths.long()\n","  target_lengths = target_lengths.long()\n","  ############################ START OF YOUR CODE ############################\n","  # TODO(2.1)\n","  # Hint:\n","  # - `F.ctc_loss`: https://pytorch.org/docs/stable/nn.functional.html#ctc-loss\n","  # - log_probs is passed in with shape (batch_size, input_length, num_classes).\n","  # - Notice that `F.ctc_loss` expects log_probs of shape\n","  #   (input_length, batch_size, num_classes)\n","  # - Turn on zero_infinity.\n","  \n","  \n","  ############################# END OF YOUR CODE #############################\n","  return ctc_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fb6IkAvR5xkY"},"source":["class CTCEncoderDecoder(nn.Module):\n","  \"\"\"\n","  Encoder-Decoder model trained with CTC objective.\n","\n","  Args:\n","    input_dim: integer\n","                number of input features\n","    num_class: integer\n","                size of transcription vocabulary\n","    num_layers: integer (default: 2)\n","                number of layers in encoder LSTM\n","    hidden_dim: integer (default: 128)\n","                number of hidden dimensions for encoder LSTM\n","    bidirectional: boolean (default: True)\n","                    is the encoder LSTM bidirectional?\n","  \"\"\"\n","  def __init__(\n","      self, input_dim, num_class, num_layers=2, hidden_dim=128,\n","      bidirectional=True):\n","    super().__init__()\n","    # Note: `batch_first=True` argument implies the inputs to the LSTM should\n","    # be of shape (batch_size x T x D) instead of (T x batch_size x D).\n","    self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n","                            bidirectional=bidirectional, batch_first=True)\n","    self.decoder = nn.Linear(hidden_dim * 2, num_class)\n","    self.input_dim = input_dim\n","    self.num_class = num_class\n","    self.num_layers = num_layers\n","    self.hidden_dim = hidden_dim\n","    self.embedding_dim = hidden_dim * num_layers * 2 * \\\n","                          (2 if bidirectional else 1)\n","\n","  def combine_h_and_c(self, h, c):\n","    \"\"\"Combine the signals from RNN hidden and cell states.\"\"\"\n","    batch_size = h.size(1)\n","    h = h.permute(1, 0, 2).contiguous()\n","    c = c.permute(1, 0, 2).contiguous()\n","    h = h.view(batch_size, -1)\n","    c = c.view(batch_size, -1)\n","    return torch.cat([h, c], dim=1)  # just concatenate\n","\n","  def forward(self, inputs, input_lengths):\n","    batch_size, max_length, _ = inputs.size()\n","    # `torch.nn.utils.rnn.pack_padded_sequence` collapses padded sequences\n","    # to a contiguous chunk\n","    inputs = torch.nn.utils.rnn.pack_padded_sequence(\n","        inputs, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n","    log_probs = None\n","    h, c = None, None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(2.1)\n","    # Hint:\n","    # - Refer to https://pytorch.org/docs/stable/nn.html\n","    # - Use `self.encoder` to get the encodings output which is of shape\n","    #   (batch_size, max_length, num_directions*hidden_dim) and the\n","    #   hidden states and cell states which are both of shape\n","    #   (batch_size, num_layers*num_directions, hidden_dim)\n","    # - Pad outputs with `0.` using `torch.nn.utils.rnn.pad_packed_sequence`\n","    #   (turn on batch_first and set total_length as max_length).\n","    # - Apply 50% dropout.\n","    # - Use `self.decoder` to take the embeddings sequence and return\n","    #   probabilities for each character.\n","    # - Make sure to then convert to log probabilities.\n","\n","    \n","    ############################# END OF YOUR CODE #############################\n","    \n","    # The extracted embedding is not used for the ASR task but will be\n","    # needed for other auxiliary tasks.\n","    embedding = self.combine_h_and_c(h, c)\n","    return log_probs, embedding\n","\n","  def get_loss(\n","      self, log_probs, targets, input_lengths, target_lengths, blank=0):\n","    return get_ctc_loss(\n","        log_probs, targets, input_lengths, target_lengths, blank)\n","\n","  def decode(self, log_probs, input_lengths, labels, label_lengths,\n","             sos_index, eos_index, pad_index, eps_index):\n","    # Use greedy decoding.\n","    decoded = torch.argmax(log_probs, dim=2)\n","    batch_size = decoded.size(0)\n","    # Collapse each decoded sequence using CTC rules.\n","    hypotheses = []\n","    for i in range(batch_size):\n","      hypotheses_i = self.ctc_collapse(decoded[i], input_lengths[i].item(),\n","                                       blank_index=eps_index)\n","      hypotheses.append(hypotheses_i)\n","\n","    hypothesis_lengths = input_lengths.cpu().numpy().tolist()\n","    if labels is None: # Run at inference time.\n","      references, reference_lengths = None, None\n","    else:\n","      references = labels.cpu().numpy().tolist()\n","      reference_lengths = label_lengths.cpu().numpy().tolist()\n","\n","    return hypotheses, hypothesis_lengths, references, reference_lengths\n","\n","  def ctc_collapse(self, seq, seq_len, blank_index=0):\n","    result = []\n","    for i, tok in enumerate(seq[:seq_len]):\n","      if tok.item() != blank_index:  # remove blanks\n","        if i != 0 and tok.item() == seq[i-1].item():  # remove dups\n","          pass\n","        else:\n","          result.append(tok.item())\n","    return result\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1nnZCcWpLTzm"},"source":["## **Introduction to PyTorch Lightning**\n","\n","**Walkthrough**\n","\n","*This section is a walkthrough and will not require any code or answers.* We will use [PyTorch Lightning](https://www.pytorchlightning.ai/), a lightweight wrapper framework for PyTorch, to run our experiments. You can learn more about the lightning toolkit [here](https://github.com/PyTorchLightning/pytorch-lightning). As a short introduction, Pytorch Lightning is a scaffold for training deep learning models. It handles a lot of the usual pipeline for you (e.g. looping over the training set, calling your optimizer). It has several callback handlers you can overwrite to specify your model."]},{"cell_type":"code","metadata":{"id":"2fD95O2MLRWN"},"source":["# Do not modify.\n","\n","class LightningCTC(pl.LightningModule):\n","  \"\"\"PyTorch Lightning class for training a CTC model.\n","\n","  Args:\n","    n_mels: number of mel frequencies. (default: 128)          \n","    n_fft: number of fourier features. (default: 256)          \n","    win_length: number of frames in a window. (default: 256)              \n","    hop_length: number of frames to hop in computing spectrogram. (default: 128)               \n","    wav_max_length: max number of timesteps in a waveform spectrogram. (default: 200)                  \n","    transcript_max_length: max number of characters in decoded transcription. (default: 200)                         \n","    learning_rate: learning rate for Adam optimizer. (default: 1e-3)                  \n","    batch_size: batch size used in optimization and evaluation. (default: 256)               \n","    weight_decay: weight decay for Adam optimizer. (default: 1e-5)               \n","    encoder_num_layers: number of layers in LSTM encoder. (default: 2)                       \n","    encoder_hidden_dim: number of hidden dimensions in LSTM encoder. (default: 256)\n","    encoder_bidirectional: directionality of LSTM encoder. (default: True)                         \n","  \"\"\"\n","  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n","               wav_max_length=200, transcript_max_length=200, \n","               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n","               encoder_num_layers=2, encoder_hidden_dim=256, \n","               encoder_bidirectional=True):\n","    super().__init__()\n","    self.save_hyperparameters()\n","    self.n_mels = n_mels\n","    self.n_fft = n_fft\n","    self.win_length = win_length\n","    self.hop_length = hop_length\n","    self.lr = learning_rate\n","    self.batch_size = batch_size\n","    self.weight_decay = weight_decay\n","    self.wav_max_length = wav_max_length\n","    self.transcript_max_length = transcript_max_length\n","    self.train_dataset, self.val_dataset, self.test_dataset = \\\n","      self.create_datasets()\n","    self.encoder_num_layers = encoder_num_layers\n","    self.encoder_hidden_dim = encoder_hidden_dim\n","    self.encoder_bidirectional = encoder_bidirectional\n","\n","    # Instantiate the CTC encoder/decoder.\n","    self.model = self.create_model()\n","\n","  def create_model(self):\n","    model = CTCEncoderDecoder(\n","      self.train_dataset.input_dim,\n","      self.train_dataset.num_class,\n","      num_layers=self.encoder_num_layers,\n","      hidden_dim=self.encoder_hidden_dim,\n","      bidirectional=self.encoder_bidirectional)\n","    return model\n","\n","  def create_datasets(self):\n","    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n","    train_dataset = HarperValleyBank(\n","        root, split='train', n_mels=self.n_mels, n_fft=self.n_fft, \n","        win_length=self.win_length, hop_length=self.hop_length,\n","        wav_max_length=self.wav_max_length,\n","        transcript_max_length=self.transcript_max_length,\n","        append_eos_token=False)\n","    val_dataset = HarperValleyBank(\n","        root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n","        win_length=self.win_length, hop_length=self.hop_length, \n","        wav_max_length=self.wav_max_length,\n","        transcript_max_length=self.transcript_max_length,\n","        append_eos_token=False) \n","    test_dataset = HarperValleyBank(\n","        root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n","        win_length=self.win_length, hop_length=self.hop_length,\n","        wav_max_length=self.wav_max_length,\n","        transcript_max_length=self.transcript_max_length,\n","        append_eos_token=False) \n","    return train_dataset, val_dataset, test_dataset\n","\n","  def configure_optimizers(self):\n","    optim = torch.optim.AdamW(self.model.parameters(),\n","                              lr=self.lr, weight_decay=self.weight_decay)\n","    return [optim], [] # <-- put scheduler in here if you want to use one\n","\n","  def get_loss(self, log_probs, input_lengths, labels, label_lengths):\n","    loss = self.model.get_loss(log_probs, labels, input_lengths, label_lengths,\n","                                blank=self.train_dataset.eps_index)\n","    return loss\n","\n","  def forward(self, inputs, input_lengths, labels, label_lengths):\n","    log_probs, embedding = self.model(inputs, input_lengths)\n","    return log_probs, embedding\n","\n","  def get_primary_task_loss(self, batch, split='train'):\n","    \"\"\"Returns ASR model losses, metrics, and embeddings for a batch.\"\"\"\n","    inputs, input_lengths = batch[0], batch[1]\n","    labels, label_lengths = batch[2], batch[3]\n","\n","    if split == 'train':\n","      log_probs, embedding = self.forward(\n","          inputs, input_lengths, labels, label_lengths)\n","    else:\n","      # do not pass labels to not teacher force after training\n","      log_probs, embedding = self.forward(\n","          inputs, input_lengths, None, None)\n","\n","    loss = self.get_loss(log_probs, input_lengths, labels, label_lengths)\n","\n","    # Compute CER (no gradient necessary).\n","    with torch.no_grad():\n","      hypotheses, hypothesis_lengths, references, reference_lengths = \\\n","        self.model.decode(\n","            log_probs, input_lengths, labels, label_lengths,\n","            self.train_dataset.sos_index,\n","            self.train_dataset.eos_index,\n","            self.train_dataset.pad_index,\n","            self.train_dataset.eps_index)\n","      cer_per_sample = get_cer_per_sample(\n","          hypotheses, hypothesis_lengths, references, reference_lengths)\n","      cer = cer_per_sample.mean()\n","      metrics = {f'{split}_loss': loss, f'{split}_cer': cer}\n","\n","    return loss, metrics, embedding\n","\n","  # Overwrite TRAIN\n","  def training_step(self, batch, batch_idx):\n","    loss, metrics, _ = self.get_primary_task_loss(batch, split='train')\n","    self.log_dict(metrics)\n","    # self.log('train_loss', loss, prog_bar=True, on_step=True)\n","    # self.log('train_cer', metrics['train_cer'], prog_bar=True, on_step=True)\n","    return loss\n","\n","  # Overwrite VALIDATION: get next minibatch\n","  def validation_step(self, batch, batch_idx):\n","    loss, metrics, _ = self.get_primary_task_loss(batch, split='val')\n","    return metrics\n","\n","  def test_step(self, batch, batch_idx):\n","    _, metrics, _ = self.get_primary_task_loss(batch, split='test')\n","    return metrics\n","\n","  # Overwrite: e.g. accumulate stats (avg over CER and loss)\n","  def validation_epoch_end(self, outputs):\n","    \"\"\"Called at the end of validation step to aggregate outputs.\"\"\"\n","    # outputs is list of metrics from every validation_step (over a\n","    # validation epoch).\n","    metrics = { \n","      # important that these are torch Tensors!\n","      'val_loss': torch.tensor([elem['val_loss']\n","                                for elem in outputs]).float().mean(),\n","      'val_cer': torch.tensor([elem['val_cer']\n","                                for elem in outputs]).float().mean()\n","    }\n","    # self.log('val_loss', metrics['val_loss'], prog_bar=True)\n","    # self.log('val_cer', metrics['val_cer'], prog_bar=True)\n","    self.log_dict(metrics)\n","\n","  def test_epoch_end(self, outputs):\n","    metrics = { \n","      'test_loss': torch.tensor([elem['test_loss']\n","                                  for elem in outputs]).float().mean(),\n","      'test_cer': torch.tensor([elem['test_cer']\n","                                for elem in outputs]).float().mean()\n","    }\n","    self.log_dict(metrics)\n","    \n","  def train_dataloader(self):\n","    # - important to shuffle to not overfit!\n","    # - drop the last batch to preserve consistent batch sizes\n","    loader = DataLoader(self.train_dataset, batch_size=self.batch_size,\n","                        shuffle=True, pin_memory=True, drop_last=True)\n","    return loader\n","\n","  def val_dataloader(self):\n","    loader = DataLoader(self.val_dataset, batch_size=self.batch_size,\n","                        shuffle=False, pin_memory=True)\n","    return loader\n","\n","  def test_dataloader(self):\n","    loader = DataLoader(self.test_dataset, batch_size=self.batch_size,\n","                        shuffle=False, pin_memory=True)\n","    return loader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XY6Sl-rH950z"},"source":["## **Task 2.1: Train a network with CTC [15 Points]**\n","\n","**Training & Written Response**\n","\n","Go to **Runtime** > **Change runtime type** and set **Hardware accelerator** to **GPU**.\n","\n","This section will be graded based on 1) your model's performance in regards to loss plots and CER plots and 2) your response for qualitative assessments of your plots.\n","\n","**→ Train the CTC network with the default hyperparameters we provide.** \n","\n","With batch size 128, one epoch of optimizing CTC takes roughly 3 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n","\n","**→ Paste screenshots from your Weights & Biases dashboard of your loss curve and CER curve in the cell marked \"Plots\".**"]},{"cell_type":"code","metadata":{"id":"viZWdpyhJkX0"},"source":["WANDB_NAME = '' # Fill in your Weights & Biases ID here.\n","\n","def run(system, config, ckpt_dir, epochs=1, monitor_key='val_loss', \n","        use_gpu=False, seed=1337):\n","  random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  np.random.seed(seed)\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","  SystemClass = globals()[system]\n","  system = SystemClass(**config)\n","\n","  checkpoint_callback = ModelCheckpoint(\n","    dirpath=os.path.join(MODEL_PATH, ckpt_dir),\n","    save_top_k=1,\n","    verbose=True,\n","    monitor=monitor_key, \n","    mode='min')\n","  \n","  wandb.init(project='cs224s', entity=WANDB_NAME, name=ckpt_dir, \n","             config=config, sync_tensorboard=True)\n","  wandb_logger = WandbLogger()\n","  \n","  if use_gpu:\n","    trainer = pl.Trainer(\n","        gpus=1, max_epochs=epochs, min_epochs=epochs, enable_checkpointing=True,\n","        callbacks=checkpoint_callback, logger=wandb_logger)\n","  else:\n","    trainer = pl.Trainer(\n","        max_epochs=epochs, min_epochs=epochs, enable_checkpointing=True,\n","        callbacks=checkpoint_callback, logger=wandb_logger)\n","  \n","  trainer.fit(system)\n","  result = trainer.test()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2H9tt8gSKVH0"},"source":["config = {\n","    'n_mels': 128, \n","    'n_fft': 256,\n","    'win_length': 256,\n","    'hop_length': 128,\n","    'wav_max_length': 512, \n","    'transcript_max_length': 200,\n","    'learning_rate': 1e-3, \n","    'batch_size': 128, \n","    'weight_decay': 0, \n","    'encoder_num_layers': 2, \n","    'encoder_hidden_dim': 256, \n","    'encoder_bidirectional': True,\n","}\n","\n","# NOTES:\n","# -----\n","# - PyTorch Lightning will run 2 steps of validation prior to the first \n","#   epoch to sanity check that validation works (otherwise you \n","#   might waste an epoch training and error).\n","# - The progress bar updates very slowly, the model is likely \n","#   training even if it doesn't look like it is. \n","# - Wandb will generate a URL for you where all the metrics will be logged.\n","# - Every validation loop, the best performing model is saved.\n","# - After training, the system will evaluate performance on the test set.\n","# run(system=\"LightningCTC\", config=config, ckpt_dir='ctc', epochs=20, use_gpu=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgZmM-cKtpaG"},"source":["# You can find the saved checkpoint here:\n","!ls /content/cs224s_spring2022/trained_models/ctc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ig-B8k-muLt0"},"source":["# How to load the checkpoint into a CTC system:\n","#   LightningCTC.load_from_checkpoint(...) # Fill in your checkpoint path.\n","# To resume training, use pl.Trainer as in the `run` fucntion above. For example:\n","#   system = LightningCTC.load_from_checkpoint(...)\n","#   trainer = pl.Trainer(gpus=1, ...)\n","#   trainer.fit(system)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_zuwGLr0Pi7"},"source":["---\n","\n","**Plots:** \n","\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"-IXjPxEYMOUB"},"source":["**→ Using your plots as evidence in your description, answer the following questions:**\n","\n","a) What is the model's best test CER?\n","\n","b) Does the model learn and converge? What do you notice about CTC loss early in training?\n","\n","c) Does the model overfit? Despite the small dataset size, why might CTC not overfit?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L5Fkkc9O1SYv"},"source":["---\n","\n","**Answer:**\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ngMF5nRLXLJ1"},"source":["# Part 3: Analysis\n","\n","While looking at validation and test CER is a good way to judge how a model is performing, it is also important to look at specific examples it does well on or fails on, in order to build an intuition for why it fails."]},{"cell_type":"markdown","metadata":{"id":"pfYm97bGZpyZ"},"source":["## **Task 3.1: Lowest and Highest CER Examples [5 Points]**\n","\n","**Implementation & Written Response**\n","\n","**→ Now we will find and examine a test utterance your model transcribes well and a test utterance it transcribes poorly.** Fill out `get_low_high_cer_wav` to get the lowest and highest CERs and their corresponding utterances in your test set."]},{"cell_type":"code","metadata":{"id":"UrF1Mt6w1g55"},"source":["from tqdm import tqdm\n","\n","def get_low_high_cer_wav(system, device=None):\n","  \"\"\"Gets the test set sample with lowest CER and the sample with highest CER.\n","\n","  Args:\n","    system: Subclassed LightningModule for your model.\n","    device: Instance of torch.device(...) [default: None]\n","\n","  Returns:\n","    lowest CER (float), audio of the lowest CER utterance (ndarray),\n","    highest CER (float), audio of the highest CER utterance (ndarray)\n","  \"\"\"\n","  # Init values.\n","  low_cer = float('inf')\n","  low_idx = 0\n","  high_cer = float('-inf')\n","  high_idx = 0\n","\n","  test_dataloader = system.test_dataloader()\n","  index_lookup = system.test_dataset.indices\n","\n","  pbar = tqdm(total=len(test_dataloader))\n","  for i, batch in enumerate(test_dataloader):\n","    input_features, input_lengths = batch[0], batch[1]\n","    labels, label_lengths = batch[2], batch[3]\n","    batch_size = input_features.size(0)\n","    if device is not None:\n","      input_features = input_features.to(device)\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(3.1)\n","    # Hint:\n","    # - Use `get_cer_per_sample`, which gets a numpy array of\n","    #   CERs for each sample in a batch\n","    # - Use `index_lookup` to map a sample's test set index to\n","    #   its index in the full dataset.\n","    \n","    \n","\n","    ############################# END OF YOUR CODE #############################\n","    pbar.update()\n","  pbar.close()\n","\n","  # Retrieve ndarray wav data from the original h5py file.\n","  system.test_dataset.load_waveforms()\n","  waveform_data = system.test_dataset.waveform_data\n","  low_wav = waveform_data[f'{low_idx}'][:]\n","  high_wav = waveform_data[f'{high_idx}'][:]\n","\n","  return low_cer, low_wav, high_cer, high_wav"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ie7HDMvVqgSP"},"source":["checkpoint_path = None\n","############################## START OF YOUR CODE ##############################\n","# TODO(3.1)\n","# Add your CTC checkpoint path.\n","\n","\n","############################## END OF YOUR CODE ################################\n","\n","device = torch.device('cuda')\n","system = LightningCTC.load_from_checkpoint(checkpoint_path)\n","system = system.to(device)\n","system.eval()\n","low_cer, low_wav, high_cer, high_wav = get_low_high_cer_wav(system, device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wygAmfkc-H5n"},"source":["print('Utterance with lowest CER: {}\\n'.format(low_cer))\n","Audio(low_wav, rate=8000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvsavm_F_BjF"},"source":["print('Utterance with highest CER: {}\\n'.format(high_cer))\n","Audio(high_wav, rate=8000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0DpQsZADTfKA"},"source":["**→ What are the lowest and highest CERs? Why do you think CTC got these CERs for these utterances?**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7VFJdCCr1yFk"},"source":["---\n","\n","**Answer:**\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5TtjP0eSZ_tI"},"source":["## **Task 3.2: Run inference using your model [5 points]**\n","\n","**Implementation**\n","\n","**→ Similar to `get_low_high_cer_wav`, we'll run inference on a single audio file and see what the model transcribes.** Fill in `run_inference` to have your system decode test utterances from a `.WAV` file. We will later run this function in Parts 5 and 7 to qualitatively evaluate systems."]},{"cell_type":"code","metadata":{"id":"aBJwFvv2aHsK"},"source":["def run_inference(\n","    system, wav, device=None, sr=8000, n_mels=128, n_fft=256, win_length=256, \n","    hop_length=128, wav_max_length=512, labels=None, label_lengths=None):\n","  \"\"\"Run your system on a .WAV file and returns a string utterance.\n","    \n","  Args:\n","    system: a pl.LightningModule for your chosen model.\n","    wav: a .WAV file of an utterance\n","    device: GPU -> torch.device('cuda')\n","\n","  Returns:\n","    A string for the utterance transcribed by your model.\n","  \"\"\"\n","  input_feature = None\n","  ############################# START OF YOUR CODE #############################\n","  # TODO(3.2)\n","  # Extract features from the utterance. This is similar to what you implemented\n","  # in `get_primary_task_data`.\n","  # Hint:\n","  # - Make sure to put the extracted features into a batch of size 1.\n","\n","  \n","  ############################## END OF YOUR CODE ##############################\n","\n","  input_lengths = torch.LongTensor([input_length])\n","  # Whether or not to use GPU.\n","  if device is not None:\n","    input_feature = input_feature.to(device)\n","    input_lengths = input_lengths.to(device)\n","    if labels is not None:  # to test teacher-forcing\n","      labels = labels.to(device)\n","      labels_lengths = label_lengths.to(device)\n","\n","  utterance = None\n","  ############################# START OF YOUR CODE #############################\n","  # TODO(3.2)\n","  # Run your system on the utterance input feature to get log probabilities\n","  # and decode the log probabilities into indices. Then turn those indices into\n","  # characters.\n","\n","  \n","  ############################## END OF YOUR CODE ##############################\n","  return utterance\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kg_FAAx04iP9"},"source":["# Part 4: Leveraging Auxiliary Tasks for Multi-Task Learing\n","\n","When designing a speech system, we might care about more than just the transcription. As a bank, we might want to know the intent of the caller, for example.\n","\n","Our dataset includes the dialog action, the intent of the caller, and the sentiment of the caller. In the spirit of an end-to-end system, we will expand the CTC model to make predictions for auxiliary tasks. It is up to you which tasks you decide to multi-task on!\n"]},{"cell_type":"markdown","metadata":{"id":"UO-ng6Sgbv6d"},"source":["## **Task 4.1 Working with auxiliary task data [5 Points]**\n","\n","**Implementation**\n","\n","**→ Fill in `__getitem__`. Add one or more auxiliary tasks to your training.** We include `get_auxiliary_labels` for you."]},{"cell_type":"code","metadata":{"id":"Co1TnD6lf-Kn"},"source":["class HarperValleyBankMTL(HarperValleyBank):\n","  \"\"\"Like the HarperValleyBank dataset but returns labels for task type, \n","  dialog actions, and sentiment: our three auxiliary tasks.\n","\n","  See `HarperValleyBank` class for description. \n","  \"\"\"\n","  def __init__(\n","    self, root, split='train', n_mels=128, n_fft=128, win_length=256, \n","    hop_length=128, wav_max_length=200, transcript_max_length=200, \n","    append_eos_token=False):\n","    super().__init__(\n","      root, split=split, n_mels=n_mels, n_fft=n_fft,\n","      win_length=win_length, hop_length=hop_length,\n","      wav_max_length=wav_max_length, \n","      transcript_max_length=transcript_max_length,\n","      append_eos_token=append_eos_token)\n","    self.auxiliary_labels = self.get_auxiliary_labels()\n","\n","  def get_auxiliary_labels(self):\n","    \"\"\"Returns auxiliary task labels.\n","    \n","    This function will take the raw auxiliary tasks and convert them\n","    integers labels (for neural networks).\n","\n","    These include: `task_type`, `dialogue_acts`, and `sentiment`.\n","    \"\"\"\n","    # task_types: each element is a string representing a conversation-level\n","    #             label. So all utterances in the same conversation share \n","    #             the same label.\n","    task_types = self.label_data['task_types']\n","    # dialog_acts: each element is a comma-separated string of dialog actions \n","    #              that describe the current utterance\n","    dialog_acts = self.label_data['dialog_acts']\n","    dialog_acts = [acts.split(',') for acts in dialog_acts]\n","    # sentiments: each element is a 3 dimensional vector that sums to 1 \n","    #             representing the probabilities for \n","    #             \"negative\", \"neutral\", and \"positive\"\n","    sentiment_labels = self.label_data['sentiments']\n","\n","    # Get label vocabularies.\n","    task_type_vocab = sorted(set(task_types))\n","    dialog_acts_vocab = sorted(set([item for sublist in dialog_acts\n","                                    for item in sublist]))\n","\n","    task_type_labels = [task_type_vocab.index(t) for t in task_types]\n","\n","    # dialog_acts_labels: list of 1-hot vectors\n","    dialog_acts_labels = []\n","    for acts in dialog_acts:\n","      onehot = [0 for _ in range(len(dialog_acts_vocab))]\n","      for act in acts:\n","        onehot[dialog_acts_vocab.index(act)] = 1\n","      dialog_acts_labels.append(onehot)\n","\n","    # Store number of classes for each auxiliary task.\n","    # Note: \n","    #   - task_type is a N-way classification problem.\n","    #   - dialog_acts is a set of binary classification problems. \n","    #       (more than one dialog action may be \"on\" for an utterance)            \n","    #   - sentiment is a regression problem (match given probabilities).\n","    self.task_type_num_class = len(task_type_vocab)\n","    self.dialog_acts_num_class = len(dialog_acts_vocab)\n","    self.sentiment_num_class = 3\n","    \n","    return task_type_labels, dialog_acts_labels, sentiment_labels\n","    \n","  def __getitem__(self, index):\n","    \"\"\"Serves multi-task data for a single utterance.\"\"\"\n","    if not hasattr(self, 'waveform_data'):\n","      self.load_waveforms()\n","\n","    index = int(self.indices[index])\n","\n","    primary_task_data = self.get_primary_task_data(index)\n","    auxiliary_task_data = None\n","    \n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.1)\n","    # Get auxiliary task label(s) for this index using\n","    # `self.auxiliary_task_labels`. Populate the object `auxiliary_task_data` \n","    # as a tuple of auxiliary task labels. Make sure to cast appropriate \n","    # torch tensor types for the different labels.\n","\n","    \n","\n","    ############################# END OF YOUR CODE #############################\n","    if not isinstance(auxiliary_task_data, tuple):\n","      auxiliary_task_data = (auxiliary_task_data,)\n","\n","    return primary_task_data + auxiliary_task_data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MND0Dny4FBrN"},"source":["## **Task 4.2: Implement auxiliary task heads [2 points]**\n","\n","**Implementation**\n","\n","**→ Fill out the `*Classifier` classes for your multi-tasking design choice.** These classifiers only require a single layer in depth. You will use these in `LightningCTCMTL`."]},{"cell_type":"code","metadata":{"id":"2-cjrZkscJUp"},"source":["class TaskTypeClassifier(nn.Module):\n","  def __init__(self, input_dim, n_classes):\n","    super().__init__()\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","\n","    ############################# END OF YOUR CODE #############################\n","\n","  def forward(self, inputs):\n","    log_probs = None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","    # Hint: This is an N-way classification problem.\n","    \n","    ############################# END OF YOUR CODE #############################\n","    return log_probs\n","  \n","  def get_loss(self, probs, targets):\n","    loss = None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2) \n","    \n","    ############################# END OF YOUR CODE #############################\n","    return loss\n","\n","\n","class DialogActsClassifier(nn.Module):\n","  def __init__(self, input_dim, n_classes):\n","    super().__init__()\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","    \n","    ############################# END OF YOUR CODE #############################\n","\n","  def forward(self, inputs):\n","    probs = None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","    # Hint: One person can have multiple dialog actions.\n","    \n","    ############################# END OF YOUR CODE #############################\n","    return probs\n","\n","  def get_loss(self, probs, targets):\n","    loss = None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","    # Hint:\n","    # - probs shape: (batch_size, num_dialog_acts)\n","    # - targets shape: (batch_size, num_dialog_acts)\n","    \n","    ############################# END OF YOUR CODE #############################\n","    return loss\n","\n","\n","class SentimentClassifier(nn.Module):\n","  def __init__(self, input_dim, n_classes):\n","    super().__init__()\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","    \n","    ############################# END OF YOUR CODE #############################\n","\n","  def forward(self, inputs):\n","    probs = None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","    # Hint:\n","    # - Sentiment is measured as a log probability distribution among multiple\n","    #   possible sentiments.\n","    \n","    ############################# END OF YOUR CODE #############################\n","    return probs\n","\n","  def get_loss(self, pred_probs, target_probs):\n","    loss = None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.2)\n","    # Hint:\n","    # - As usual, the predictions are probabilities. But the labels for\n","    #   sentiment are themselves probabilities. Since the targets are not be\n","    #   single numbers, we cannot just use `F.cross_entropy`.\n","    # - Therefore, you will need to implement cross entropy manually.\n","    #     Refer to wikipedia: https://en.wikipedia.org/wiki/Cross_entropy\n","    # - pred_logits shape: (batch_size, num_sentiment_class)\n","    # - target_logits shape: (batch_size, num_sentiment_class)\n","    \n","    ############################# END OF YOUR CODE #############################\n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfsmjWi3DADg"},"source":["## **Task 4.3: Implement multi-task learning loss [3 points]**\n","\n","**Implementation**\n","\n","**Metrics.** We provide code for computing the metrics for each possible auxiliary task. For task type classification and sentiment classification, you should use accuracy. For dialog acts classification, you should use F1 score (as it is very unbalanced).\n","\n","**→ Instantiate your auxiliary task models in `__init__` for `LightningCTCMTL` and implement/modify `get_multi_task_loss`.** You can additively combine loss functions and use weighting parameters in your addition to trade off the importance of different tasks during training.\n","\n","**Note:** In general, your choice of weighting parameters is often an important design decision for your system! It is even possible to negatively weight certain tasks that may otherwise bias the primary task prediction through simple correlation. This turns those auxiliary tasks into adversarial tasks (that your model should *not* do well on) and can lead to more robust performance on certain edge cases.\n","\n","**Note:** In the Lightning class below, these weights are expressed as `asr_weight`, `task_type_weight`, `dialog_acts_weight`, and `sentiment_weight`. You should choose them to sum to 1.\n","\n","**→ Modify `validation_epoch_end` and `test_epoch_end`.** Leave in the necessary code to store metrics for your model's auxiliary tasks."]},{"cell_type":"code","metadata":{"id":"M5cllqMeW_pb"},"source":["class LightningCTCMTL(LightningCTC):\n","  \"\"\"PyTorch Lightning class for training CTC with multi-task learning.\"\"\"\n","  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n","               wav_max_length=200, transcript_max_length=200, \n","               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n","               encoder_num_layers=2, encoder_hidden_dim=256, \n","               encoder_bidirectional=True, asr_weight=1.0, task_type_weight=1.0, \n","               dialog_acts_weight=1.0, sentiment_weight=1.0):\n","    super().__init__(\n","      n_mels=n_mels, hop_length=hop_length, \n","      wav_max_length=wav_max_length, \n","      transcript_max_length=transcript_max_length,\n","      learning_rate=learning_rate, \n","      batch_size=batch_size, \n","      weight_decay=weight_decay, \n","      encoder_num_layers=encoder_num_layers, \n","      encoder_hidden_dim=encoder_hidden_dim, \n","      encoder_bidirectional=encoder_bidirectional)\n","    self.save_hyperparameters()\n","    self.asr_weight = asr_weight\n","    self.task_type_weight = task_type_weight\n","    self.dialog_acts_weight = dialog_acts_weight\n","    self.sentiment_weight = sentiment_weight\n","\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.3)\n","    # Instantiate your auxiliary task models here.\n","    \n","    \n","    ############################# END OF YOUR CODE #############################\n","\n","  def create_datasets(self):\n","    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n","    train_dataset = HarperValleyBankMTL(\n","      root, split='train', n_mels=self.n_mels, n_fft=self.n_fft,\n","      win_length=self.win_length, hop_length=self.hop_length, \n","      wav_max_length=self.wav_max_length,\n","      transcript_max_length=self.transcript_max_length,\n","      append_eos_token=False)\n","    val_dataset = HarperValleyBankMTL(\n","      root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n","      win_length=self.win_length, hop_length=self.hop_length, \n","      wav_max_length=self.wav_max_length,\n","      transcript_max_length=self.transcript_max_length,\n","      append_eos_token=False) \n","    test_dataset = HarperValleyBankMTL(\n","      root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n","      win_length=self.win_length, hop_length=self.hop_length, \n","      wav_max_length=self.wav_max_length,\n","      transcript_max_length=self.transcript_max_length,\n","      append_eos_token=False) \n","    return train_dataset, val_dataset, test_dataset\n","\n","  def get_multi_task_loss(self, batch, split='train'):\n","    \"\"\"Gets losses and metrics for all task heads.\"\"\"\n","    # Compute loss on the primary ASR task.\n","    asr_loss, asr_metrics, embedding = self.get_primary_task_loss(batch, split)\n","    \n","    # Note: Not all of these have to be used (it is up to your design)\n","    task_type_labels = None\n","    dialog_acts_labels = None\n","    sentiment_labels = None\n","    task_type_log_probs = None\n","    dialog_acts_probs = None\n","    sentiment_log_probs = None\n","    task_type_loss = None\n","    dialog_acts_loss = None\n","    sentiment_loss = None\n","    combined_loss = None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.3)\n","    # Implement multi-task learning by combining multiple objectives.\n","    # Define `combined_loss` here. \n","\n","    \n","\n","    ############################ END OF YOUR CODE ##############################\n","\n","    with torch.no_grad():\n","      ############################ START OF YOUR CODE ##########################\n","      # TODO(4.3)\n","      # No additional code is required here. :)\n","      # We provide how to compute metrics for all possible auxiliary tasks and\n","      # store them in your metrics dictionary. Comment out the metrics for tasks\n","      # you do not plan to use.\n","\n","      # TASK_TYPE: Compare predicted task type to true task type.\n","      task_type_preds = torch.argmax(task_type_log_probs, dim=1)\n","      task_type_acc = \\\n","        (task_type_preds == task_type_labels).float().mean().item()\n","\n","      # DIALOG_ACTS: Compare predicted dialog actions to true dialog actions.\n","      dialog_acts_preds = torch.round(dialog_acts_probs)\n","      dialog_acts_f1 = f1_score(dialog_acts_labels.cpu().numpy().reshape(-1),\n","                                dialog_acts_preds.cpu().numpy().reshape(-1))\n","\n","\n","      # SENTIMENT: Compare largest predicted sentiment to largest true sentim\n","      sentiment_preds = torch.argmax(sentiment_log_probs, dim=1)\n","      sentiment_labels = torch.argmax(sentiment_labels, dim=1)\n","      sentiment_acc = \\\n","      (sentiment_preds == sentiment_labels).float().mean().item()\n","\n","      metrics = {\n","        # Task losses. \n","        f'{split}_asr_loss': asr_metrics[f'{split}_loss'],\n","        f'{split}_task_type_loss': task_type_loss,\n","        f'{split}_dialog_acts_loss': dialog_acts_loss,\n","        f'{split}_sentiment_loss': sentiment_loss,\n","        # CER as ASR metric.\n","        f'{split}_asr_cer': asr_metrics[f'{split}_cer'],\n","        # Accuracy as task_type metric.\n","        f'{split}_task_type_acc': task_type_acc,\n","        # F1 score as dialog_acts metric.\n","        f'{split}_dialog_acts_f1': dialog_acts_f1,\n","        # Accuracy as sentiment metric.\n","        f'{split}_sentiment_acc': sentiment_acc\n","      }\n","      ############################ END OF YOUR CODE ############################\n","    return combined_loss, metrics\n","\n","  # comment out parameters for the models you don't use\n","  def configure_optimizers(self):\n","    parameters = chain(self.model.parameters(),\n","                       self.task_type_model.parameters(),\n","                       self.dialog_acts_model.parameters(),\n","                       self.sentiment_model.parameters())\n","    optim = torch.optim.AdamW(parameters, lr=self.lr,\n","                              weight_decay=self.weight_decay)\n","    return [optim], []\n","\n","  def training_step(self, batch, batch_idx):\n","    loss, metrics = self.get_multi_task_loss(batch, split='train')\n","    self.log_dict(metrics)\n","    # self.log('train_asr_loss', metrics['train_asr_loss'], prog_bar=True, \n","    #          on_step=True)\n","    # self.log('train_asr_cer', metrics['train_asr_cer'], prog_bar=True, \n","    #          on_step=True)\n","    return loss\n","\n","  def validation_step(self, batch, batch_idx):\n","    loss, metrics = self.get_multi_task_loss(batch, split='val')\n","    return metrics\n","\n","  def validation_epoch_end(self, outputs):\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.3)\n","    # No additional code is required here. :)\n","    # Comment out the metrics for tasks you do not plan to use.\n","    metrics = { \n","      'val_asr_loss': torch.tensor([elem['val_asr_loss']\n","                                    for elem in outputs]).float().mean(),\n","      'val_asr_cer': torch.tensor([elem['val_asr_cer']\n","                                   for elem in outputs]).float().mean(),\n","      'val_task_type_loss': torch.tensor([elem['val_task_type_loss']\n","                                          for elem in outputs]).float().mean(),\n","      'val_task_type_acc': torch.tensor([elem['val_task_type_acc']\n","                                          for elem in outputs]).float().mean(),\n","      'val_dialog_acts_loss': torch.tensor([elem['val_dialog_acts_loss'] \n","                                            for elem in outputs]).float().mean(),\n","      'val_dialog_acts_f1': torch.tensor([elem['val_dialog_acts_f1']\n","                                          for elem in outputs]).float().mean(),\n","      'val_sentiment_loss': torch.tensor([elem['val_sentiment_loss']\n","                                          for elem in outputs]).float().mean(),\n","      'val_sentiment_acc': torch.tensor([elem['val_sentiment_acc']\n","                                         for elem in outputs]).float().mean()\n","      }\n","    ############################# END OF YOUR CODE #############################\n","    # self.log('val_asr_loss', metrics['val_asr_loss'], prog_bar=True)\n","    # self.log('val_asr_cer', metrics['val_asr_cer'], prog_bar=True)\n","    self.log_dict(metrics)\n","\n","  def test_step(self, batch, batch_idx):\n","    loss, metrics = self.get_multi_task_loss(batch, split='test')\n","    return metrics\n","\n","  def test_epoch_end(self, outputs):\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(4.3)\n","    # No additional code is required here. :)\n","    # Comment out the metrics for tasks you do not plan to use.\n","    metrics = { \n","      'test_asr_loss': torch.tensor([elem['test_asr_loss']\n","                                    for elem in outputs]).float().mean(),\n","      'test_asr_cer': torch.tensor([elem['test_asr_cer']\n","                                   for elem in outputs]).float().mean(),\n","      'test_task_type_loss': torch.tensor([elem['test_task_type_loss']\n","                                          for elem in outputs]).float().mean(),\n","      'test_task_type_acc': torch.tensor([elem['test_task_type_acc']\n","                                          for elem in outputs]).float().mean(),\n","      'test_dialog_acts_loss': torch.tensor([elem['test_dialog_acts_loss'] \n","                                             for elem in outputs]).float().mean(),\n","      'test_dialog_acts_f1': torch.tensor([elem['test_dialog_acts_f1']\n","                                           for elem in outputs]).float().mean(),\n","      'test_sentiment_loss': torch.tensor([elem['test_sentiment_loss']\n","                                           for elem in outputs]).float().mean(),\n","      'test_sentiment_acc': torch.tensor([elem['test_sentiment_acc']\n","                                          for elem in outputs]).float().mean()\n","      }\n","    ############################# END OF YOUR CODE #############################\n","    # self.log('test_asr_loss', metrics['test_asr_loss'], prog_bar=True)\n","    # self.log('test_asr_cer', metrics['test_asr_cer'], prog_bar=True)\n","    self.log_dict(metrics)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7x88wh0WTj4"},"source":["## **Task 4.1: Training CTC-MTL [10 points]**\n","\n","**Training & Written Response**\n","\n","**→ Train the CTC-MTL network with the default hyperparameters we provide.**\n","\n","One epoch of training CTC-MTL takes 3 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n","\n","**NOTE: We expect this model to perform a bit worse on CER compared with the CTC-only objective. That's a trade-off when building multi-task learning approaches, so don't spend extra times trying to improve your CER for this question**\n","\n","**→ Paste screenshots from your Weights & Biases dashboard of your loss curves and CER curve in the cell marked \"Plots\". Remember to include learning curves for the auxiliary tasks!**"]},{"cell_type":"code","metadata":{"id":"C4D8kJJKxWVK"},"source":["# Run CTC-MTL\n","\n","config = {\n","  'n_mels': 128,\n","  'n_fft': 256,\n","  'win_length': 256,\n","  'hop_length': 128,\n","  'wav_max_length': 512, \n","  'transcript_max_length': 200,\n","  'learning_rate': 1e-3, \n","  'batch_size': 128, \n","  'weight_decay': 0, \n","  'encoder_num_layers': 2, \n","  'encoder_hidden_dim': 256, \n","  'encoder_bidirectional': True,\n","  # you may wish to play with these weights; try to keep the sum\n","  # of them equal to one.\n","  'asr_weight': 0.25, \n","  'task_type_weight': 0.25,\n","  'dialog_acts_weight': 0.25,\n","  'sentiment_weight': 0.25,\n","}\n","\n","run(system=\"LightningCTCMTL\", config=config, ckpt_dir='ctc_mtl', epochs=20, \n","    monitor_key='val_asr_loss', use_gpu=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrqzM4TTleFx"},"source":["# You can find the saved checkpoint here:\n","!ls /content/cs224s_spring2022/trained_models/ctc_mtl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQ1KkpAGlDX1"},"source":["---\n","\n","**Plots:**\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"YYCTmwiNkioM"},"source":["**→ Using your plots as evidence in your description, answer the following questions:**\n","\n","a) Report performance metrics on each of the auxiliary tasks and the CER of your jointly trained model.\n","\n","b) Under the same configuration of hyperparameters, does CTC-MTL perform better than CTC? Why or why not? (Hint: Have the loss plots converged? How does multi-tasking affect the speed of learning the primary task?)\n","\n","c) Which tasks seem to be more difficult than others? Why might that be?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FBbCzn0512GV"},"source":["---\n","\n","**Answer:**\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jZItBGVmaLKE"},"source":["# Part 5: Joint CTC-Attention Based Neural Network\n","As a practictioner, in designing a speech system you might face the choice between using CTC or LAS, and you might wonder, **why not both**? Now, we'll use even more multi-tasking to improve our performance, except with a twist: we want to regularize our attention-based network to respect CTC alignments. Essentially, we will use CTC as another auxiliary task to aid alignment for recognizing speech, based on this [paper](https://arxiv.org/pdf/1609.06773.pdf) (Kim et al.).\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8aKsMxwmaHMq"},"source":["## CTC-Attention understanding check\n","\n","**Read through the paper. Let us call the proposed model CTC-LAS-MTL.**\n","\n","**You do not need to submit responses for the questions below. Use these to check your understanding as you work**\n","\n","a) What is a key advantage of CTC-LAS-MTL over LAS-MTL?\n","\n","\n","b) What is an erroneous example utterance LAS-MTL might predict that CTC-LAS-MTL would probably not? Explain your reasoning. (Hint: Why might conditional independence sometimes be a good thing?)\n","\n","\n","c) Describe a tradeoff of weighting the CTC loss higher than the LAS loss vs. lower than the LAS loss.\n"]},{"cell_type":"code","metadata":{"id":"xIpip7iUG0MQ"},"source":["class CTCDecoder(nn.Module):\n","  \"\"\"\n","  This is a small decoder (just one linear layer) that takes \n","  the listener embedding from LAS and imposes a CTC \n","  objective on the decoding.\n","\n","  NOTE: This is only to be used for the Joint CTC-Attention model.\n","  \"\"\"\n","  def __init__(self, listener_hidden_dim, num_class, dropout=0.5):\n","    super().__init__()\n","    self.fc = nn.Linear(listener_hidden_dim, num_class)\n","    self.dropout = nn.Dropout(dropout)\n","    self.listener_hidden_dim = listener_hidden_dim\n","    self.num_class = num_class\n","\n","  def forward(self, listener_outputs):\n","    batch_size, maxlen, _ = listener_outputs.size()\n","    logits = self.fc(self.dropout(listener_outputs))\n","    logits = logits.view(batch_size, maxlen, self.num_class)\n","    log_probs = F.log_softmax(logits, dim=2)\n","    return log_probs\n","\n","  def get_loss(\n","      self, log_probs, input_lengths, labels, label_lengths, blank=0):\n","    return get_ctc_loss(\n","      log_probs, labels, input_lengths, label_lengths, blank)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TXauS2P67vx"},"source":["## **Task 5.1: CTC-LAS Network [5 points]**\n","\n","**→ Read through the starter code and fill out the `forward` pass for `JointCTCAttention`.**"]},{"cell_type":"markdown","metadata":{"id":"EU4UBRT6YKN1"},"source":["## **Task 5.2: Training CTC-LAS-MTL [10 points]**\n","\n","**Training results & written response**\n","\n","**→ Train the CTC-LAS-MTL network with the default hyperparameters we provide.**\n","\n","Like LAS-MTL, this model takes 6-7 minutes per epoch to train. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint.\n","\n","**We encourage you to try and tune the model to improve overall task performance or CER. Report your findings if you experiment**\n","\n","**→ Paste screenshots from your Weights & Biases dashboard of your loss curves and CER curve in the cell marked \"Plots\". Remember to include learning curves for the auxiliary tasks!**"]},{"cell_type":"code","metadata":{"id":"r8ZrGMpCxBKN"},"source":["# Run Joint CTC/LAS-MTL\n","\n","config = {\n","  'n_mels': 128, \n","  'n_fft': 256,\n","  'win_length': 256,\n","  'hop_length': 128,\n","  'wav_max_length': 512, \n","  'transcript_max_length': 200,\n","  'learning_rate': 4e-3, \n","  'batch_size': 128, \n","  'weight_decay': 0, \n","  'encoder_num_layers': 2,    # can't shrink output too much... \n","  'encoder_hidden_dim': 64, \n","  'encoder_bidirectional': True,\n","  'encoder_dropout': 0,\n","  'decoder_hidden_dim': 128,  # must be 2 x encoder_hidden_dim\n","  'decoder_num_layers': 1,\n","  'decoder_multi_head': 1,\n","  'decoder_mlp_dim': 64,\n","  'asr_label_smooth': 0.1,\n","  'teacher_force_prob': 0.9,\n","  # you may wish to play with these weights; try to keep the sum\n","  # of them equal to one.\n","  'ctc_weight': 0.5,  # equal weight between ctc and las?\n","  'asr_weight': 0.25, \n","  'task_type_weight': 0.25,\n","  'dialog_acts_weight': 0.25,\n","  'sentiment_weight': 0.25,\n","}\n","\n","run(system=\"LightningCTCLASMTL\", config=config, epochs=20, \n","    ckpt_dir='ctc_las_mtl', monitor_key='val_asr_loss', use_gpu=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWm7yH7ll9tJ"},"source":["# You can find the saved checkpoint here:\n","!ls /content/cs224s_spring2022/trained_models/ctc_las_mtl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7QUxJkdq80Bi"},"source":["---\n","\n","**Plots:**\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"9iWcrpxLYck3"},"source":["**→ Using your plots as evidence in your description, answer the following questions:**\n","\n","a) Report performance metrics on each of the auxiliary tasks and the CER of your jointly trained model.\n","\n","b) Is your best CER lower than from LAS-MTL?\n","\n","c) What effects do you observe CTC has on LAS in terms of its learning curve? (e.g. How does the speed of learning compare to that of LAS? In what ways could CTC be improving LAS? Please explain thoroughly.)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ownof9Jt2S3r"},"source":["---\n","\n","**Answer:**\n","\n","\n","---"]},{"cell_type":"code","metadata":{"id":"BpSulwjvHHY1"},"source":["class JointCTCAttention(LASEncoderDecoder):\n","  \"\"\"Joint CTC and LAS model that optimizes the LAS objective but \n","  regularized by the conditional independence of a CTC decoder. One\n","  can interpret CTC as regularizer on LAS.\n","  \"\"\"\n","\n","  def __init__(\n","      self, input_dim, num_class, label_maxlen, listener_hidden_dim=128, \n","      listener_bidirectional=True, num_pyramid_layers=3, dropout=0, \n","      speller_hidden_dim=256, speller_num_layers=1, mlp_hidden_dim=128, \n","      multi_head=1, sos_index=0, sample_decode=False):\n","    super().__init__(\n","      input_dim,\n","      num_class,\n","      label_maxlen,\n","      listener_hidden_dim=listener_hidden_dim,\n","      listener_bidirectional=listener_bidirectional,\n","      num_pyramid_layers=num_pyramid_layers,\n","      dropout=dropout,\n","      speller_hidden_dim=speller_hidden_dim,\n","      speller_num_layers=speller_num_layers,\n","      mlp_hidden_dim=mlp_hidden_dim,\n","      multi_head=multi_head,\n","      sos_index=sos_index,\n","      sample_decode=sample_decode,\n","    )\n","    self.ctc_decoder = CTCDecoder(listener_hidden_dim * 2, num_class)\n","    self.num_pyramid_layers = num_pyramid_layers\n","    self.embedding_dim = listener_hidden_dim * 4\n","\n","  def forward(\n","      self, inputs, ground_truth=None, teacher_force_prob=0.9,):\n","    ctc_log_probs = None\n","    las_log_probs = None\n","    h, c = None, None\n","    ############################ START OF YOUR CODE ############################\n","    # TODO(5.1)\n","    # Hint:\n","    # - Encode the inputs with the `listener` network and decode \n","    #   transcription probabilities using both the `speller` network\n","    #   and CTCDecoder network.\n","\n","    \n","    \n","    ############################# END OF YOUR CODE #############################\n","    listener_hc = self.combine_h_and_c(h, c)\n","    return ctc_log_probs, las_log_probs, listener_hc\n","\n","  def get_loss(\n","      self, ctc_log_probs, las_log_probs, input_lengths, labels, label_lengths,\n","      num_labels, pad_index=0, blank_index=0, label_smooth=0.1):\n","    ctc_loss = self.ctc_decoder.get_loss(\n","      ctc_log_probs,\n","      # pyramid encode cuts timesteps in 1/2 each way\n","      input_lengths // (2**self.num_pyramid_layers),\n","      labels,\n","      label_lengths,\n","      blank=blank_index,\n","    )\n","    las_loss = super().get_loss(las_log_probs, labels, num_labels,\n","                                pad_index=pad_index, label_smooth=label_smooth)\n","\n","    return ctc_loss, las_loss\n","\n","  def decode(self, log_probs, input_lengths, labels, label_lengths,\n","             sos_index, eos_index, pad_index, eps_index):\n","    las_log_probs = log_probs[1]\n","    return super().decode(las_log_probs, input_lengths, labels, label_lengths,\n","                          sos_index, eos_index, pad_index, eps_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtKpwCEGIbz5"},"source":["class LightningCTCLASMTL(LightningLASMTL):\n","\n","  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n","               wav_max_length=200, transcript_max_length=200, \n","               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n","               encoder_num_layers=2, encoder_hidden_dim=256, \n","               encoder_bidirectional=True, encoder_dropout=0, \n","               decoder_hidden_dim=256, decoder_num_layers=1,\n","               decoder_multi_head=1, decoder_mlp_dim=128,\n","               asr_label_smooth=0.1, teacher_force_prob=0.9,\n","               ctc_weight=0.5, asr_weight=1.0, task_type_weight=1.0, \n","               dialog_acts_weight=1.0, sentiment_weight=1.0):\n","    super().__init__(\n","      n_mels=n_mels, \n","      n_fft=n_fft,\n","      win_length=win_length,\n","      hop_length=hop_length,\n","      wav_max_length=wav_max_length, \n","      transcript_max_length=transcript_max_length,\n","      learning_rate=learning_rate, \n","      batch_size=batch_size, \n","      weight_decay=weight_decay, \n","      encoder_num_layers=encoder_num_layers, \n","      encoder_hidden_dim=encoder_hidden_dim, \n","      encoder_bidirectional=encoder_bidirectional,\n","      encoder_dropout=encoder_dropout,\n","      decoder_hidden_dim=decoder_hidden_dim,\n","      decoder_num_layers=decoder_num_layers,\n","      decoder_multi_head=decoder_multi_head,\n","      decoder_mlp_dim=decoder_mlp_dim,\n","      asr_label_smooth=asr_label_smooth,\n","      teacher_force_prob=teacher_force_prob,\n","      asr_weight=asr_weight, \n","      task_type_weight=task_type_weight, \n","      dialog_acts_weight=dialog_acts_weight,\n","      sentiment_weight=sentiment_weight)\n","    self.save_hyperparameters()\n","    self.ctc_weight = ctc_weight\n","\n","  def create_model(self):\n","    model = JointCTCAttention(\n","      self.train_dataset.input_dim,\n","      self.train_dataset.num_class,\n","      self.transcript_max_length,\n","      listener_hidden_dim=self.encoder_hidden_dim,\n","      listener_bidirectional=self.encoder_bidirectional,\n","      num_pyramid_layers=self.encoder_num_layers,\n","      dropout=self.encoder_dropout,\n","      speller_hidden_dim=self.decoder_hidden_dim,\n","      speller_num_layers=self.decoder_num_layers,\n","      mlp_hidden_dim=self.decoder_mlp_dim,\n","      multi_head=self.decoder_multi_head,\n","      sos_index=self.train_dataset.sos_index,\n","      sample_decode=False)\n","\n","    return model\n","\n","  def forward(self, inputs, input_lengths, labels, label_lengths):\n","    ctc_log_probs, las_log_probs, embedding = self.model(\n","      inputs,\n","      ground_truth=labels,\n","      teacher_force_prob=self.teacher_force_prob)\n","    return (ctc_log_probs, las_log_probs), embedding\n","\n","  def get_loss(self, log_probs, input_lengths, labels, label_lengths):\n","    (ctc_log_probs, las_log_probs) = log_probs\n","    ctc_loss, las_loss = self.model.get_loss(\n","      ctc_log_probs,\n","      las_log_probs,\n","      input_lengths,\n","      labels,\n","      label_lengths,\n","      self.train_dataset.num_labels,\n","      pad_index=self.train_dataset.pad_index,\n","      blank_index=self.train_dataset.eps_index,\n","      label_smooth=self.asr_label_smooth)\n","    loss = self.ctc_weight * ctc_loss + (1 - self.ctc_weight) * las_loss\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMhSTNY-Gb0-"},"source":["# Part 6: One Model to Hear Them All\n","Congratulations, by this point you have trained multiple end-to-end deep learning neural networks for automatic speech recognition!\n"]},{"cell_type":"markdown","metadata":{"id":"U8nXgReDYn4M"},"source":["## **Task 6.1: Train and summarize your best model [5 Points]**\n","\n","**Training & Written Response**\n","\n","**Note:** You are welcome to conduct additional experiments in this part. Please copy cells from above into Part 7 of the notebook if you wish to utilize them.\n","\n","**→ Alter any model of your choice or training procedure to improve performance! Describe what you tried, and report performance of your best model.** Include in your answer your design choices of:\n","- Type of neural network (CTC, LAS, or Joint CTC-Attention)\n","- Any auxiliary task(s) and weighting of tasks you may have used\n","- Training hyperparameters (e.g. learning rate)\n","\n","You should attempt to improve your model performance in a reasonable way given what you have observed so far. You do not need to exhaustively optimize performance though; training at least 2 new models with adjustments is enough to obtain full credit for the homework.\n"]},{"cell_type":"markdown","metadata":{"id":"DZIdGGOo2Is7"},"source":["---\n","\n","**Answer:**\n","\n","\n","---\n","\n"]},{"cell_type":"code","source":["## Code to train your best model here"],"metadata":{"id":"bvoyMHdH7rXs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jh7lhrPAb_u1"},"source":["# Submission"]},{"cell_type":"markdown","metadata":{"id":"MLN0ItdPrtwP"},"source":["**Great work!** You have completed the final assignment of the course, and in doing so you have trained deep acoustic models from scratch on a speech dataset, and built intuition for different architectures and design choices for jointly training ASR along with other tasks.\n","\n","**Gradescope Submission**\n","- Download your Colab notebook **with all cells fully executed** as a `.ipynb` file. Zip together your `.ipynb` with any supporting files. Submit this zipped file under `Assignment 3: Code Submission`.\n","- Open your `.ipynb` file locally and save it as a PDF. Submit this PDF under `Assignment 3: PDF Submission` and **tag all pages corresponding to each task**."]},{"cell_type":"markdown","metadata":{"id":"YA-eoZYubhSd"},"source":["# Optional: Testing Your System!\n","\n","**You can test your system with your own voice samples! Make 2 short recordings of yourself: 1) one on any topic and 2) another on a topic that more closely matches utterances in the HarperValleyBank dataset.** Save/convert them as `.WAV` files and upload them to your `DATA_PATH` directory. Then use `run_inference` to get your best model's transcriptions on them.\n","\n","- How does it do? Are the transcripts accurate?\n","- Does the model generalize? If not, why do you think that is? What changes could be made to help the model generalize better?"]},{"cell_type":"markdown","metadata":{"id":"4JMjmSXy1qO4"},"source":["---\n","\n","**Answer:**\n","\n","\n","---"]},{"cell_type":"code","metadata":{"id":"Npf3piJcidS6"},"source":["# Load your saved model weights.\n","\n","system = None\n","wav = None\n","device = torch.device('cuda')\n","sr = 8000\n","############################## START OF YOUR CODE ##############################\n","# TODO(7.2)\n","# Use system.eval() after you load your PyTorch Lightning system weights.\n","# Use `librosa.load` (refer to https://librosa.org/doc/latest/index.html).\n","# Use the default target sample rate of 8000.\n","# Use `librosa.effects.trim` to remove leading and trailing silences.\n","\n","############################### END OF YOUR CODE ###############################\n","\n","run_inference(system, wav, device=device, sr=sr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmKqOxgD4I6_"},"source":["# Optional: Listen, Attend and Spell (LAS) Neural Network\n","\n","This is a stand alone LAS model for the dataset. It tends to overfit on the small HVB training set. You do not need this code for the homework, we leave the LAS section here as a reference for you. \n","\n","This experiment uses an attention-based encoder-decoder model from the paper [Listen, Attend and Spell](https://arxiv.org/pdf/1508.01211.pdf) (Chan et al.). "]},{"cell_type":"markdown","metadata":{"id":"nJH-HtjHyGUt"},"source":["## LAS check for understanding\n","\n","**→ Read through the paper. Think about the following questions:**\n","\n","a) What is a key advantage of LAS over CTC?\n","\n","\n","b) How does LAS handle words it may not have seen during training?\n","\n","\n","c) **Teacher forcing** is a term used to describe a training technique where an RNN feeds the previous time step ground truth as the current time step input. In LAS, when is teacher forcing used and how often? What are some possible downsides to teacher forcing?\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GDQFnl2udiUx"},"source":["## LAS Network\n","Implementation below"]},{"cell_type":"code","metadata":{"id":"Y_E08yPO4nBa"},"source":["from utils import (reshape_and_apply, label_smooth_loss)\n","\n","\n","class Listener(nn.Module):\n","  \"\"\"Stacks 3 layers of PyramidLSTMLayers to reduce resolution 8 times.\n","\n","  Args:\n","    input_dim: Number of input features.\n","    hidden_dim: Number of hidden features.\n","    num_pyramid_layers: Number of stacked lstm layers. (default: 3)\n","    dropout: Dropout probability. (default: 0)\n","  \"\"\"\n","  def __init__(\n","      self, input_dim, hidden_dim, num_pyramid_layers=3, dropout=0., \n","      bidirectional=True):\n","    super().__init__()\n","    self.rnn_layer0 = PyramidLSTMLayer(input_dim, hidden_dim, num_layers=1,\n","                                        bidirectional=True, dropout=dropout)\n","    for i in range(1, num_pyramid_layers):\n","      setattr(\n","          self, \n","          f'rnn_layer{i}',\n","          PyramidLSTMLayer(hidden_dim * 2, hidden_dim, num_layers=1,\n","                            bidirectional=bidirectional, dropout=dropout),\n","      )\n","    \n","    self.num_pyramid_layers = num_pyramid_layers\n","\n","  def forward(self, inputs):\n","    outputs, hiddens = self.rnn_layer0(inputs)\n","    for i in range(1, self.num_pyramid_layers):\n","      outputs, hiddens = getattr(self, f'rnn_layer{i}')(outputs)\n","    return outputs, hiddens\n","        \n","\n","class PyramidLSTMLayer(nn.Module):\n","  \"\"\"A Pyramid LSTM layer is a standard LSTM layer that halves the size \n","  of the input in its hidden embeddings.\n","  \"\"\"\n","  def __init__(self, input_dim, hidden_dim, num_layers=1,\n","                bidirectional=True, dropout=0.):\n","    super().__init__()\n","    self.rnn = nn.LSTM(input_dim * 2, hidden_dim, num_layers=num_layers,\n","                        bidirectional=bidirectional, dropout=dropout,\n","                        batch_first=True)\n","    self.input_dim = input_dim\n","    self.hidden_dim = hidden_dim\n","    self.num_layers = num_layers\n","    self.bidirectional = bidirectional\n","    self.dropout = dropout\n","\n","  def forward(self, inputs):\n","    batch_size, maxlen, input_dim = inputs.size()\n","    \n","    # reduce time resolution?\n","    inputs = inputs.contiguous().view(batch_size, maxlen // 2, input_dim * 2)\n","    outputs, hiddens = self.rnn(inputs)\n","    return outputs, hiddens\n","\n","\n","class AttentionLayer(nn.Module):\n","  \"\"\"Attention module that trains an MLP to get attention weights.\"\"\"\n","  def __init__(self, input_dim, hidden_dim, multi_head=1):\n","    super().__init__()\n","\n","    self.phi = nn.Linear(input_dim, hidden_dim*multi_head)\n","    self.psi = nn.Linear(input_dim, hidden_dim)\n","\n","    if multi_head > 1:\n","      self.fc_reduce = nn.Linear(input_dim*multi_head, input_dim)\n","\n","    self.multi_head = multi_head\n","    self.hidden_dim = hidden_dim\n","  \n","  def forward(self, decoder_state, listener_feat):\n","    attention_score = None\n","    context = None\n","    input_dim = listener_feat.size(2)\n","    # decoder_state: batch_size x 1 x decoder_hidden_dim\n","    # listener_feat: batch_size x maxlen x input_dim\n","    comp_decoder_state = F.relu(self.phi(decoder_state))\n","    comp_listener_feat = F.relu(reshape_and_apply(self.psi, listener_feat))\n","\n","    if self.multi_head == 1:\n","      energy = torch.bmm(\n","          comp_decoder_state,\n","          comp_listener_feat.transpose(1, 2)\n","      ).squeeze(1)\n","      attention_score = [F.softmax(energy, dim=-1)]\n","      weights = attention_score[0].unsqueeze(2).repeat(1, 1, input_dim)\n","      context = torch.sum(listener_feat * weights, dim=1)\n","    else:\n","      attention_score = []\n","      for att_query in torch.split(comp_decoder_state, self.hidden_dim, dim=-1):\n","        score = torch.softmax(\n","            torch.bmm(att_query,\n","                      comp_listener_feat.transpose(1, 2)).squeeze(dim=1),\n","        )\n","        attention_score.append(score)\n","      \n","      projected_src = []\n","      for att_s in attention_score:\n","        weights = att_s.unsqueeze(2).repeat(1, 1, input_dim)\n","        proj = torch.sum(listener_feat * weights, dim=1)\n","        projected_src.append(proj)\n","      \n","      context = self.fc_reduce(torch.cat(projected_src, dim=-1))\n","\n","    # context is the entries of listener input weighted by attention\n","    return attention_score, context\n","\n","\n","class Speller(nn.Module):\n","  \"\"\"Decoder that uses a LSTM with attention to convert a sequence of\n","  hidden embeddings to a sequence of probabilities for output classes.\n","  \"\"\"\n","  def __init__(\n","        self, num_labels, label_maxlen, speller_hidden_dim,\n","        listener_hidden_dim, mlp_hidden_dim, num_layers=1, multi_head=1,\n","        sos_index=0, sample_decode=False):\n","    super().__init__()\n","    self.rnn = nn.LSTM(num_labels + speller_hidden_dim,\n","                        speller_hidden_dim, num_layers=num_layers,\n","                        batch_first=True)\n","    self.attention = AttentionLayer(listener_hidden_dim * 2, mlp_hidden_dim, \n","                                    multi_head=multi_head)\n","    self.fc_out = nn.Linear(speller_hidden_dim*2, num_labels)\n","    self.num_labels = num_labels\n","    self.label_maxlen = label_maxlen\n","    self.sample_decode = sample_decode\n","    self.sos_index = sos_index\n","\n","  def step(self, inputs, last_hiddens, listener_feats):\n","    outputs, cur_hiddens = self.rnn(inputs, last_hiddens)\n","    attention_score, context = self.attention(outputs, listener_feats)\n","    features = torch.cat((outputs.squeeze(1), context), dim=-1)\n","    logits = self.fc_out(features)\n","    log_probs = torch.log_softmax(logits, dim=-1)\n","\n","    return log_probs, cur_hiddens, context, attention_score\n","\n","  def forward(\n","      self, listener_feats, ground_truth=None, teacher_force_prob=0.9):\n","    device = listener_feats.device\n","    if ground_truth is None:\n","      teacher_force_prob = 0\n","    teacher_force = np.random.random_sample() < teacher_force_prob\n","    \n","    batch_size = listener_feats.size(0)\n","    with torch.no_grad():\n","      output_toks = torch.zeros((batch_size, 1, self.num_labels), device=device)\n","      output_toks[:, 0, self.sos_index] = 1\n","\n","    rnn_inputs = torch.cat([output_toks, listener_feats[:, 0:1, :]], dim=-1)\n","\n","    hidden_state = None\n","    log_probs_seq = []\n","\n","    if (ground_truth is None) or (not teacher_force_prob):\n","      max_step = int(self.label_maxlen)\n","    else:\n","      max_step = int(ground_truth.size(1))\n","\n","    for step in range(max_step):\n","      log_probs, hidden_state, context, _ = self.step(\n","          rnn_inputs, hidden_state, listener_feats)\n","      log_probs_seq.append(log_probs.unsqueeze(1))\n","\n","      if teacher_force:\n","        gt_tok = ground_truth[:, step:step+1].float()\n","        output_tok = torch.zeros_like(log_probs)\n","        for idx, i in enumerate(gt_tok):\n","            output_tok[idx, int(i.item())] = 1\n","        output_tok = output_tok.unsqueeze(1)\n","      else:\n","        if self.sample_decode:\n","          probs = torch.exp(log_probs)\n","          sampled_tok = Categorical(probs).sample()\n","        else:  # Pick max probability (greedy decoding)\n","          output_tok = torch.zeros_like(log_probs)\n","          sampled_tok = log_probs.topk(1)[1]\n","\n","        output_tok = torch.zeros_like(log_probs)\n","        for idx, i in enumerate(sampled_tok):\n","          output_tok[idx, int(i.item())] = 1\n","        output_tok = output_tok.unsqueeze(1)\n","\n","      rnn_inputs = torch.cat([output_tok, context.unsqueeze(1)], dim=-1)\n","\n","    # batch_size x maxlen x num_labels\n","    log_probs_seq = torch.cat(log_probs_seq, dim=1)\n","\n","    return log_probs_seq.contiguous()\n","\n","\n","class LASEncoderDecoder(nn.Module):\n","  def __init__(\n","      self, input_dim, num_class, label_maxlen, listener_hidden_dim=128, \n","      listener_bidirectional=True, num_pyramid_layers=3, dropout=0, \n","      speller_hidden_dim=256, speller_num_layers=1, mlp_hidden_dim=128, \n","      multi_head=1, sos_index=0, sample_decode=False):\n","    super().__init__()\n","    # Encoder.\n","    self.listener = Listener(input_dim, listener_hidden_dim,\n","                              num_pyramid_layers=num_pyramid_layers,\n","                              dropout=dropout,\n","                              bidirectional=listener_bidirectional)\n","    # Decoder.\n","    self.speller = Speller(num_class, label_maxlen, speller_hidden_dim,\n","                            listener_hidden_dim, mlp_hidden_dim,\n","                            num_layers=speller_num_layers, \n","                            multi_head=multi_head,\n","                            sos_index=sos_index,\n","                            sample_decode=sample_decode)\n","    self.embedding_dim = listener_hidden_dim * 4\n","\n","  def combine_h_and_c(self, h, c):\n","    batch_size = h.size(1)\n","    h = h.permute(1, 0, 2).contiguous()\n","    c = c.permute(1, 0, 2).contiguous()\n","    h = h.view(batch_size, -1)\n","    c = c.view(batch_size, -1)\n","    return torch.cat([h, c], dim=1)\n","\n","  def forward(\n","      self, inputs, ground_truth=None, teacher_force_prob=0.9):\n","    log_probs = None\n","    h, c = None, None\n","    # this is the main model connection for forward prop\n","    outputs, (h, c) = self.listener(inputs)\n","    log_probs = self.speller(outputs, ground_truth=ground_truth, teacher_force_prob=teacher_force_prob)\n","\n","    combined_h_and_c = self.combine_h_and_c(h, c)\n","    return log_probs, combined_h_and_c\n","\n","  def get_loss(\n","      self, log_probs, labels, num_labels, pad_index=0, label_smooth=0.1):\n","    batch_size = log_probs.size(0)\n","    labels_maxlen = labels.size(1)\n","\n","    if label_smooth == 0.0:\n","      loss = F.nll_loss(log_probs.view(batch_size * labels_maxlen, -1),\n","                        labels.long().view(batch_size * labels_maxlen),\n","                        ignore_index=pad_index)\n","    else:\n","      # label_smooth_loss is the sample as F.nll_loss but with a temperature\n","      # parameter that makes the log probability distribution \"sharper\".\n","      loss = label_smooth_loss(log_probs, labels.float(), num_labels,\n","                                smooth_param=label_smooth)\n","    return loss\n","\n","  def decode(self, log_probs, input_lengths, labels, label_lengths,\n","             sos_index, eos_index, pad_index, eps_index):\n","    # Use greedy decoding.\n","    decoded = torch.argmax(log_probs, dim=2)\n","    batch_size = decoded.size(0)\n","    # Collapse each decoded sequence using CTC rules.\n","    hypotheses = []\n","    hypothesis_lengths = []\n","    references = []\n","    reference_lengths = []\n","    for i in range(batch_size):\n","      decoded_i = decoded[i]\n","      hypothesis_i = []\n","      for tok in decoded_i:\n","        if tok.item() == sos_index:\n","          continue\n","        if tok.item() == pad_index:\n","          continue\n","        if tok.item() == eos_index:\n","          # once we reach an EOS token, we are done generating.\n","          break\n","        hypothesis_i.append(tok.item())\n","      hypotheses.append(hypothesis_i)\n","      hypothesis_lengths.append(len(hypothesis_i))\n","\n","      if labels is not None:\n","        label_i = labels[i]\n","        reference_i = [tok.item() for tok in labels[i]\n","                      if tok.item() != sos_index and \n","                          tok.item() != eos_index and \n","                          tok.item() != pad_index]\n","        references.append(reference_i)\n","        reference_lengths.append(len(reference_i))\n","    \n","    if labels is None: # Run at inference time.\n","      references, reference_lengths = None, None\n","\n","    return hypotheses, hypothesis_lengths, references, reference_lengths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPSgNAJT4crM"},"source":["class LightningLASMTL(LightningCTCMTL):\n","  \"\"\"Train a Listen-Attend-Spell model along with the Multi-Task Objevtive.\n","  \"\"\"\n","\n","  def __init__(self, n_mels=128, n_fft=256, win_length=256, hop_length=128, \n","               wav_max_length=200, transcript_max_length=200, \n","               learning_rate=1e-3, batch_size=256, weight_decay=1e-5, \n","               encoder_num_layers=2, encoder_hidden_dim=256, \n","               encoder_bidirectional=True, encoder_dropout=0, \n","               decoder_hidden_dim=256, decoder_num_layers=1,\n","               decoder_multi_head=1, decoder_mlp_dim=128,\n","               asr_label_smooth=0.1, teacher_force_prob=0.9,\n","               asr_weight=1.0, task_type_weight=1.0, \n","               dialog_acts_weight=1.0, sentiment_weight=1.0):\n","    self.encoder_dropout = encoder_dropout\n","    self.decoder_hidden_dim = decoder_hidden_dim\n","    self.decoder_num_layers = decoder_num_layers\n","    self.decoder_mlp_dim = decoder_mlp_dim\n","    self.decoder_multi_head = decoder_multi_head\n","    self.asr_label_smooth = asr_label_smooth\n","    self.teacher_force_prob = teacher_force_prob\n","\n","    super().__init__(\n","      n_mels=n_mels, n_fft=n_fft,\n","      win_length=win_length, hop_length=hop_length,\n","      wav_max_length=wav_max_length, \n","      transcript_max_length=transcript_max_length,\n","      learning_rate=learning_rate, \n","      batch_size=batch_size, \n","      weight_decay=weight_decay, \n","      encoder_num_layers=encoder_num_layers, \n","      encoder_hidden_dim=encoder_hidden_dim, \n","      encoder_bidirectional=encoder_bidirectional,\n","      asr_weight=asr_weight, \n","      task_type_weight=task_type_weight, \n","      dialog_acts_weight=dialog_acts_weight,\n","      sentiment_weight=sentiment_weight)\n","    self.save_hyperparameters()\n","\n","  def create_model(self):\n","    model = LASEncoderDecoder(\n","      self.train_dataset.input_dim,\n","      self.train_dataset.num_class,\n","      self.transcript_max_length,\n","      listener_hidden_dim=self.encoder_hidden_dim,\n","      listener_bidirectional=self.encoder_bidirectional, \n","      num_pyramid_layers=self.encoder_num_layers,\n","      dropout=self.encoder_dropout,\n","      speller_hidden_dim=self.decoder_hidden_dim,\n","      speller_num_layers=self.decoder_num_layers,\n","      mlp_hidden_dim=self.decoder_mlp_dim,\n","      multi_head=self.decoder_multi_head,\n","      sos_index=self.train_dataset.sos_index,\n","      sample_decode=False)\n","    return model\n","\n","  def create_datasets(self):\n","    root = os.path.join(DATA_PATH, 'harper_valley_bank_minified')\n","    train_dataset = HarperValleyBankMTL(\n","      root, split='train', n_mels=self.n_mels, n_fft=self.n_fft,\n","      win_length=self.win_length, hop_length=self.hop_length, \n","      wav_max_length=self.wav_max_length,\n","      transcript_max_length=self.transcript_max_length,\n","      append_eos_token=True)  # LAS adds a EOS token to the end of a sequence\n","    val_dataset = HarperValleyBankMTL(\n","      root, split='val', n_mels=self.n_mels, n_fft=self.n_fft,\n","      win_length=self.win_length, hop_length=self.hop_length,\n","      wav_max_length=self.wav_max_length,\n","      transcript_max_length=self.transcript_max_length,\n","      append_eos_token=True) \n","    test_dataset = HarperValleyBankMTL(\n","      root, split='test', n_mels=self.n_mels, n_fft=self.n_fft,\n","      win_length=self.win_length, hop_length=self.hop_length,\n","      wav_max_length=self.wav_max_length,\n","      transcript_max_length=self.transcript_max_length,\n","      append_eos_token=True) \n","    return train_dataset, val_dataset, test_dataset\n","\n","  def forward(self, inputs, input_lengths, labels, label_lengths):\n","    log_probs, embedding = self.model(\n","      inputs,\n","      ground_truth=labels,\n","      teacher_force_prob=self.teacher_force_prob,\n","    )\n","    return log_probs, embedding\n","\n","  def get_loss(self, log_probs, input_lengths, labels, label_lengths):\n","    loss = self.model.get_loss(log_probs, labels,\n","      self.train_dataset.num_labels,\n","      pad_index=self.train_dataset.pad_index,\n","      label_smooth=self.asr_label_smooth)\n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eg1VaakFVtBq"},"source":["## Training LAS-MTL \n","\n","**This section will train the LAS-MTL network with the default hyperparameters we provide.**\n","\n","LAS is more expensive than CTC to train. Each epoch takes roughly 6 minutes. We recommend to train for at least 15-20 epochs, although we do not guarantee this is enough to converge. If your notebook resets, you can continue training from an old checkpoint."]},{"cell_type":"code","metadata":{"id":"MJjrPvUWxMLD"},"source":["# Run LAS-MTL\n","\n","config = {\n","  'n_mels': 128, \n","  'n_fft': 256,\n","  'win_length': 256,\n","  'hop_length': 128,\n","  'wav_max_length': 512, \n","  'transcript_max_length': 200,\n","  'learning_rate': 4e-3,      # faster learning rate\n","  'batch_size': 128, \n","  'weight_decay': 0, \n","  'encoder_num_layers': 2, \n","  'encoder_hidden_dim': 64, \n","  'encoder_bidirectional': True,\n","  'encoder_dropout': 0,\n","  'decoder_hidden_dim': 128,  # must be 2 x encoder_hidden_dim\n","  'decoder_num_layers': 1,\n","  'decoder_multi_head': 1,\n","  'decoder_mlp_dim': 64,\n","  'asr_label_smooth': 0.1,\n","  'teacher_force_prob': 0.9,\n","  # You may wish to adjust these weights.\n","  # Keep the sum of them equal to one.\n","  'asr_weight': 0.25, \n","  'task_type_weight': 0.25,\n","  'dialog_acts_weight': 0.25,\n","  'sentiment_weight': 0.25,\n","}\n","\n","run(system=\"LightningLASMTL\", config=config, epochs=20, ckpt_dir='las_mtl', \n","    monitor_key='val_asr_loss', use_gpu=True)"],"execution_count":null,"outputs":[]}]}