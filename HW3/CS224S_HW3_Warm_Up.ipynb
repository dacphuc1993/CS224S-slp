{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS224S_HW3_Warm_Up","provenance":[{"file_id":"1ZnsXIscJE4Kh1FRo13np105JtDBj9pqz","timestamp":1650953831691},{"file_id":"1GzwnxXCd8-pA7MXwJqWl_XSII9JRmPQQ","timestamp":1650216239996}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YLKl4jO26g3n"},"source":["#CS224S Assignment 3 Warm Up\n","##HarperValleyBank Data Exploration & CTC Implementation\n"]},{"cell_type":"markdown","source":["This notebook is worth 30 points out of the total 100 points for Homework 3. Don't forget to work on the neural network training Colab after this one! See the assignment handout on the course website for submission instructions. "],"metadata":{"id":"fsDEnqEEtFMp"}},{"cell_type":"markdown","metadata":{"id":"rh8A_5L1dq6-"},"source":["## Setup for Google Drive and Required Libraries\n","\n","You will need to make a copy of this Colab notebook in your Google Drive before you can edit the homework files.\n","\n","You can do so with **File &rarr; Save a copy in Drive**.\n"]},{"cell_type":"code","metadata":{"id":"M9ahWfoydgHv"},"source":["#@markdown Your work will be stored in a folder called `cs224s_spring2022` by default to prevent Colab instance timeouts from deleting your edits and requiring you to redownload your data.\n","#@markdown \n","#@markdown When asked, authenticate connecting your Drive to the notebook and give all the necessary permissions.\n","\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#@title set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs224s_spring2022'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","SYM_PATH = '/content/cs224s_spring2022'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3CzA2i4g_Jk"},"source":["#@title Download HarperValleyBank dataset\n","#@markdown It takes ~10 minutes to download the dataset. You only need to do this once!\n","\n","DATA_PATH = '{}/data'.format(SYM_PATH)\n","if not os.path.exists(DATA_PATH):\n","  %mkdir $DATA_PATH\n","%cd $DATA_PATH\n","if not os.path.exists(os.path.join(DATA_PATH, 'harpervalleybank')):\n","  !wget -q http://web.stanford.edu/class/cs224s/download/harpervalleybank.zip\n","  !unzip -q harpervalleybank.zip\n","  %rm harpervalleybank.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjUD2H-wyEYA"},"source":["#@title Import packages\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","import json\n","import numpy as np\n","from scipy.fftpack import fft\n","from scipy import signal\n","from scipy.io import wavfile\n","import librosa\n","import matplotlib.pyplot as plt\n","from typing import Tuple\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8yumkFH7EVL"},"source":["## Part 1:  HarperValleyBank Dataset Exploration\n","\n","Let's first explore the [HarperValleyBank](https://arxiv.org/abs/2010.13929) dataset! The dataset primarily consists of simulated telephone/app-based consumer to banker interactions. For any new dataset, it is generally a good idea to explore the \"shape\" and \"properties\" of the data. This will help greatly when debugging unexpected behavior in your speech system.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qa4lP4mUem9Y"},"source":["## Getting Corpus Statistics\n","\n","For any new dataset, it is generally a good idea to explore the \"shape\" and \"properties\" of the data. This will help greatly when debugging unexpected behavior in your speech system. \n","\n","The structure of the HarperValleyBank dataset is:\n","```\n","data\n","    audio\n","        agent\n","            <sid1>.wav\n","            <sid2>.wav\n","            ...\n","        caller\n","            <sid1>.wav\n","            <sid2>.wav\n","            ...\n","    metadata\n","        <sid1>.json\n","        <sid2>.json\n","        ...\n","    transcript\n","        <sid1>.json\n","        <sid2>.json\n","        ...\n","```\n","Every consumer-banker conversation has an id referred to as it' `sid ` .  All associated files are named based on that sid.\n","Each conversation has four associated files, two audio files, one transcript file and one metadata file.\n","The audio for each conversation is divided in to two single channel wav files, available under the audio/agent and audio/caller directories.\n","\n","##### <ins>**Task 1.1.1**</ins>: **Number of conversations** **(2 points)**\n","\n","Please fill out the function `number_of_conversations` to get the number of conversations in the dataset. (This part could be done with 1 line of code)\n"]},{"cell_type":"code","metadata":{"id":"X1YetAUB1UxB"},"source":["agent_audio_path = '/content/cs224s_spring2022/data/harpervalleybank/audio/agent/'\n","caller_audio_path = '/content/cs224s_spring2022/data/harpervalleybank/audio/caller/'\n","transcript_path = '/content/cs224s_spring2022/data/harpervalleybank/transcript/'\n","metadata_path = '/content/cs224s_spring2022/data/harpervalleybank/metadata/'\n","\n","def number_of_conversations(path: str) -> int:\n","  \"\"\"Gets number of conversations in the dataset.\n","\n","  Args:\n","    path: Path to relevant directory.\n","\n","  Returns:\n","    Number of conversations.\n","  \"\"\"\n","  #############################\n","  #### YOUR CODE GOES HERE ####\n","  pass\n","  #############################   \n","\n","path = transcript_path \n","print('Number of conversations: ' + str(number_of_conversations(path)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sp5LIYu17jAN"},"source":["[link text](https://)##### <ins>**Task 1.1</ins>: Plot call duration (3 points)**\n","\n","Please fill out the function `recording_time` to get the durations and total duration of recording in seconds and plot a histogram with duration of recording (in seconds) as x-axis and count of conversations as y-axis. You can use `wavfile.read()` method to read files. \n","\n","For a single conversation, there are two audio files: `caller` and `agent` audio files. Both audio files for the same conversation have the same duration of recording because the other person's voice is replaced with silence. Thus, please choose either caller or agent directory to get the total duration or recording. \n"]},{"cell_type":"code","metadata":{"id":"DrIXf9Kf7mBT"},"source":["def recording_time(path_dir: str) -> Tuple[float, np.ndarray]:\n","  \"\"\"Gets duration of recording\n","  Args:\n","    path_dir: Path to relevant directory.\n","\n","  Returns:\n","    Duration of total recording in seconds (float),\n","    Numpy array of duration of recording for all conversations\n","  \n","  \"\"\"\n","  \n","  #############################\n","  #### YOUR CODE GOES HERE ####\n","  pass\n","  #############################   \n","\n","path_agent = agent_audio_path \n","path_caller = caller_audio_path \n","print(\"Duration of recordings in seconds \" + str(recording_time(path_agent)[0]))\n","\n","# Plot histogram: X - Duration of recording of individual file, Y - Count of audio files\n","#############################\n","#### YOUR CODE GOES HERE ####\n","\n","\n","#############################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZh40UnlBgev"},"source":["## Transcript Analysis\n","\n","**Transcripts:** The transcript json files are lists of segments where each segment is a json object with the following schema:\n","\n","```json\n","{\n","    \"channel_index\": 2,\n","    \"dialog_acts\": [\n","        \"gridspace_greeting\"\n","    ],\n","    \"duration_ms\": 2280,\n","    \"emotion\": {\n","        \"neutral\": 0.33766093850135803,\n","        \"negative\": 0.024230705574154854,\n","        \"positive\": 0.6381083130836487\n","    },\n","    \"human_transcript\": \"hello this is harper valley national bank\",\n","    \"index\": 1,\n","    \"offset_ms\": 5990,\n","    \"speaker_role\": \"agent\",\n","    \"start_ms\": 3990,\n","    \"start_timestamp_ms\": 1591056064136,\n","    \"transcript\": \"hello this is harper valley national bank\",\n","    \"word_durations_ms\": [\n","        330,\n","        150,\n","        120,\n","        330,\n","        270,\n","        420,\n","        330\n","    ],\n","    \"word_offsets_ms\": [\n","        0,\n","        660,\n","        810,\n","        930,\n","        1260,\n","        1530,\n","        1950\n","    ]\n","}\n","```\n","The fields we will closely analyze for inference are:\n","\n","- **\"human_transcript\":** Corrected version of the machine genereated \"transcript\".\n","\n","- **\"emotion\":** Softmax output of Gridspace's Emotion model, determining whether the emotional valence of the segment was positive, negative, or neutral.\n","\n","- **\"dialog_acts\":**. List of tags assigned to each utterance corresponding to types of conversational move represented in the utterance. There are 16 total dialog actions, and more than one can be present in an utterance. The 16 possible actions are: “yes” response, greeting, response, data confirmation, procedure explanation, data question, closing, data communication, “bear with me” response, acknowledgement, data response, filler disfluency, thanks, open question, problem description, and other.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pW59u7UqBwRo"},"source":["##### <ins>**Task 1.2**</ins>**: Transcript Statistics (3 points)**\n","\n","Load all transcript json files using `json.load()` and fill out the function `transcript_statistics` to get the following statistics: \n","- Total number of utterances\n","- Mean number of agent utterances per conversation\n","- Mean number of caller utterances per conversation\n","- Total number of words: Keep in mind that \"[\", \"]\", \"<\", \">\" aren't considered as words. \n","- Number of unique words \n","\n","You can get the text of the audio file by looking at the **\"human_transcript\"** field for each transcript. You can use `word_tokenize` function from `nltk.tokenize`.  "]},{"cell_type":"code","metadata":{"id":"S61cpfB5Gn9O"},"source":["def transcript_statistics(\n","    path_transcript: str) -> Tuple[int, float, float, int, int]:\n","  \"\"\"Get transcript statistics.\n","\n","  Args:\n","    path_transcript: Path to transcript directory.\n","  \n","  Returns:\n","    Total number of utterances, Mean number of agent utterances per conversation,\n","    Mean number of caller utterances per conversation, total number of words, \n","    Number of unique words \n","  \"\"\"\n","  #############################\n","  #### YOUR CODE GOES HERE ####\n","  pass\n","\n","  #############################   "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transcript_statistics(transcript_path)"],"metadata":{"id":"VPkn43CnBRBj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cimtUp3PJRkc"},"source":["##### <ins>**Task 1.3**</ins>**: Distribution of Utterances (3 points)**\n","\n","Plot the distribution of number of utterances per conversation and tell us the average number of utterances per conversation. (`X`: Number of utterances in a transcript, `Y`: Number of transcipts with `x` number of utterances)"]},{"cell_type":"code","metadata":{"id":"A1zyb1n0JZgr"},"source":["num_utterances = []\n","#############################\n","#### YOUR CODE GOES HERE ####\n","\n","############################# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vP_9B4nDSwuJ"},"source":["##### <ins>**Task 1.4**</ins>**: Distribution of Tasks (2 points)**\n","\n","Plot histogram of the distribution of tasks. The metadata files describe the call center scenario for each converstaion. We will look at **tasks** field for further inference. \n","\n","**tasks:** field indicates the customer’s goal/intent in the conversation. There are 8 tasks: _order checks, check balance, replace card, reset password, get branch hours, pay bill, schedule appointment, and transfer money_. (`X`: Eight tasks, `Y`: Number of conversations) "]},{"cell_type":"code","metadata":{"id":"N7vVC_qJTqpT"},"source":["task_names = ['order checks', 'check balance', 'replace card', 'reset password', 'get branch hours', 'pay bill', 'schedule appointment', 'transfer money']\n","\n","#############################\n","#### YOUR CODE GOES HERE ####\n","\n","pass\n","############################# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dGzcWYiJSy4e"},"source":["##### <ins>**Task 1.5**</ins>**: Distribution of Dialog Actions (2 points)**\n","\n","Plot histogram the distribution of dialog actions for utterances. Use **\"dialog_acts\"** field from each transcript json files. What can you infer from this plot? Please remove \"gridspace_\" from the labels. (e.g. \"greeting\" instead of \"gridspace_greeting\")\n","\n","(`X`: Dialogue action classes, `Y`: Number of utterances) "]},{"cell_type":"code","metadata":{"id":"q_nsJaYZS9yP"},"source":["#############################\n","#### YOUR CODE GOES HERE ####\n","\n","pass\n","############################# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ucjiVjaJPrl-"},"source":["##### <ins>**Task 1.6**</ins>**: Distribution of Emotions (2 points)**\n","\n","Plot boxplots that show distribution of probabilities for each emotion category across utterances. Use **\"emotion\"** field from each transcript json files. What can you infer from this plot? (`X`: Emotion Classes, `Y`: Probability) \n","\n"]},{"cell_type":"code","metadata":{"id":"jz4W3TJpPrAl"},"source":["#############################\n","#### YOUR CODE GOES HERE ####\n","\n","pass\n","############################# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UFKjz0Ytsn3a"},"source":["# Part 2: Implementing CTC Loss\n","\n","We introduce the Connectionist Temporal Classification (CTC) objective, which is a popular objective used to train neural networks to do speech recognition. Critically, it does not require you to know the alignments between inputs and outputs.\n","\n","#### **Summary of CTC**\n","\n","We highlight some of the main features of the CTC objective.\n","\n","- Given a sequence of inputs `x_1, x_2, .., x_T`, an ASR model will map each of these to a probability of an alphabet of C tokens: `p_1, p_2, ..., p_T`. For example, if we are decoding to three possible characters, the probabilities for timestep `t` could look like `p_t = [0.3, 0.2, 0.5]`.\n","\n","- We also have a sequence of symbol targets `y_1, y_2, ..., y_S` (note `S <= T`). Each `y_s` is a number from `0` to `C-1` (e.g. one of the characters).\n","\n","- The main strategy is to decode each `x_t` to a predicted character `y'_t`. For example, `x_1, x_2, ..., x_6 -> c c a a a t`. This mapping is called an **alignment**. Then we can collapse repetitions to get `c a t` as the predicted output sequence. This simple strategy has some problems: how do you handle silences or repeated characters?\n","\n","- CTC adds a special `blank` token. We'll call this `eps`. Now consider this example, `x_1, x_2, ..., x_12 -> h h e eps eps l l l eps l l o`. Now we can collapse everything in between each `eps` to get `h e eps l eps l o`. Removing blank tokens, we get the prediction `hello`.\n","\n","- For an output sequence, there are many \"valid\" alignments. For example, given an input sequence `x_1, ..., x_6` and an output sequence `c a t`, valid alignments include `eps c c eps a t`, `c c a a t t`, or `c a eps eps eps t`. Example of invalid alignments include `c eps c eps a t`, `c c a a t` (too short if the input sequence has 6 tokens), and `c eps eps eps t t`.\n","\n","- Let `A` represent all valid alignments of an output sequence to an input sequence. A simplified pseudocode for the CTC objective might look like:\n","\n","```\n","all_log_prob = 0\n","\n","for each (a_1, a_2, ..., a_S) in A:\n","\n","    log_prob = 0\n","\n","    for t in 1 to T:\n","        # The alignment a_s has a specific output character for input position t\n","        log_prob_t = log p(a_(s,t) | x_1, ..., x_T)\n","        # compute the joint probability by multiplying independent time steps\n","        # Adding in log space to avoid underflow\n","        log_prob += log_prob_t\n","\n","    all_log_prob += log_prob\n","```\n","That is, the CTC loss computes the probability of all possible alignments. Please note that the pseudocode above is for intuition. In practice, it is often too slow to enumerate over `A` explicitly. \n","\n","As an example, for an output sequence of length 50 (without any repeated characters) and an input sequence of length 100, the number of unique alignments is almost 10^40. \n","\n","#### **Dynamic Programming in CTC**\n","\n","In class, we introduced the \"forward algorithm\" to tractably compute likelihoods for HMMs. We can do something similar to score alignments more efficiently than manually enumerating over the full set.\n","\n","In other words, we can do dynamic programming. Since many alignments share partial sub-sequences, we can  store the `log_prob` for all sub-sequences we have seen so far. This allows us to reuse computation when computing the likelihood for a new sequence.\n","\n","The logic is as follows:\n","\n","Recall `X` is the input sequence of maximum length `T` and `Y` is the output sequence of maximum length `S`. Build a new sequence `Z` that adds a blank token between every character. \n","\n","```\n","Y = [y_1, y_2, ..., y_S]\n","Z = [eps, y_1, eps, y_2, ..., eps, y_S, eps]\n","  = [z_1, z_2, z_3, z_4, ..., z_2S-1, z_2S, z_2S+1]\n","```\n","Note that the length of `Z` is now `2S+1`.\n","\n","**Step 1: Make a Cache.** \n","\n","Instantiate a matrix of size `T x (2S+1)`. Call this matrix `C`. The index `C[t][s]` represents a probability score for the subsequence `z_1, ..., z_s` after observing `x_1, ..., x_t`. That is, `C[t][s] = p(y_1, ..., y_s/2 | x_1, ..., x_t)`.\n","\n","**Step 2: Make an Update Rule.**\n","\n","The goal of dynamic programming is to reuse `C[t-1]` in computing `C[t]`. To do that, we need to define an update rule. There are two cases depending on what  the token `z_s` is.\n","\n","- <ins>Case 1</ins>: `z_s` is a blank token OR `z_s = z_s-2`. This is the standard forward algorithm update. We build `p(z_1, ..., z_s|x_1, ..., x_t)` using two parts: `p(z_1, ..., z_s|x_1, ..., x_t-1)` and `p(z_1, ..., z_s-1|x_1, ..., x_t-1)`. Assuming an increasing order of `t` and `s`, these two parts will already have been computed. (If `s = 0` you can ignore the `C[t-1][s-1]` term. If `t = 0`, set `C[t][s] = p(z_s | x_1, ..., x_t)`.)\n","```\n","C[t][s] = (C[t-1][s-1] + C[t-1][s]) * p(z_s | x_1, ..., x_t)\n","```\n","In other words, `C[t-1][s-1]` and `C[t-1][s]` are known. The prediction for `p(z_s | x_1, ..., x_t)` is known (e.g. `log_probs`).\n","\n","- <ins>Case 2</ins>: `z_s` is not a blank token AND `z_s != z_s-2`. The tricky part is to notice that `z_s-1` is a blank token. Since our Markov assumption says `y_t` should depend on `y_t-1`, which means `z_s` should depend on both `z_s-1` and `z_s-2` (since we artificially added `z_s-1`). Similarly, ignore terms that do not exist.\n","```\n","C[t][s] = (C[t-1][s-2] + C[t-1][s-1] + C[t-1][s]) * p(z_s | x_1, ..., x_t)\n","```\n","\n","Now, we still need two for loops to loop over `1 ... T` and `1 ... S` but this is usually much cheaper than looping over alignments.\n","\n","```\n","C = init_cache(T, S)\n","for t in 1 to T:\n","  for s in 1 to 2S+1:\n","    C = do_update(t, s, C)\n","\n","p_y_given_x = C[-1][-1] + C[-1][-2]  # sum the probability of the last epsilon and last non-epsilon tokens\n","```\n","\n","For more information, refer to this [blog](https://distill.pub/2017/ctc/) or the original [paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf) by Alex Graves.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RBsN5YSz98CA"},"source":["Please write a function in PyTorch that given a minibatch of model predicted probabilities and a minibatch of output sequences, computes the CTC objective.\n","\n","**Note:** You cannot use the built-in `F.ctc_loss` in PyTorch. We will use this fast library for experiments in HW4.\n","\n"]},{"cell_type":"code","metadata":{"id":"-zSVCxbF-ydG"},"source":["def ctc_loss(\n","    log_probs: torch.FloatTensor, targets: torch.LongTensor,\n","    input_lengths: torch.LongTensor, target_lengths: torch.LongTensor,\n","    blank: int = 0) -> torch.Tensor:\n","  \"\"\"Connectionist Temporal Classification implementation.\n","\n","  Args:\n","    log_probs: The log-beliefs returned by an ASR model.\n","      This is `log p(a_t | x_1, ..., x_T)`.\n","      (Shape: T x batch_size x C, where T is a maximum input sequence length and\n","      C is the alphabet size (including blank))\n","\n","    targets: Sequence of contiguous output labels (no blanks).\n","      This is `y_1, ..., y_S`.\n","      (Shape: batch_size x S, where S is a maximum output sequence length)\n","\n","    input_lengths: Lengths of each example in minibatch (<= T).\n","      (Shape: batch_size)\n","\n","    target_lengths: Lengths of each target in minibatch (<= S).\n","      (Shape: batch_size)\n","\n","    blank: The \"epsilon\" token that is used to represent silence.\n","      (integer <= C, default 0)\n","\n","  Returns:\n","    CTC loss averaged over minibatch.\n","  \"\"\"\n","  #############################\n","  #### YOUR CODE GOES HERE ####\n","  \n","  pass\n","  #############################\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jB1nmtAptMT"},"source":["Here is a good sanity check. Test you code by checking below that `test` and `soln` are roughly equal. It's okay if your solution is much slower since the Pytorch one is coded in C. However, we will deduct points if you enumerate over all alignments as this is too slow for practical use.\n","\n"]},{"cell_type":"code","metadata":{"id":"FxFcWI49psc_"},"source":["import torch\n","import torch.nn.functional as F\n","\n","log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n","targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n","input_lengths = torch.full((16,), 50, dtype=torch.long)\n","target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n","\n","est = ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0)\n","soln = torch.mean(F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='none'))\n","\n","print(est.detach().numpy(), soln.detach().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"baSDEXtDqm59"},"source":["##### <ins>**Task 2.1**</ins>**: Implement CTC on your own and demonstrate your implementation [15 points]**\n","\n","Print your loss function results on the log_probs and targets we provide. You can load the test minibatches by calling `get_test_minibatches()`.\n"]},{"cell_type":"code","metadata":{"id":"N0gCS7qn3GhB"},"source":["import torch\n","import torch.nn.functional as F\n","import random\n","\n","def get_test_minibatches() -> list:\n","  \"\"\"Get minibatches to test implementation\n","  Returns:\n","    lists of log_probs, targets, input_lengths, target_lengths\n","  \"\"\"\n","  torch.manual_seed(224)\n","  np.random.seed(224)\n","  random.seed(224)\n","\n","  T = np.array([])\n","  C = np.array([])\n","  N = np.array([])  \n","  S = 40  # Target sequence length of longest target in batch (padding length)\n","  for i in range(10):\n","    T = np.append(T, random.randint(i+50, i+80))\n","    C = np.append(C, random.randint(int(0.4* T[i]), int(0.8*T[i])))\n","    N = np.append(N, random.randint(int(0.7 *C[i]), int(0.9*C[i])))\n","      \n","  log_probs = []\n","  input_lengths = []\n","  target_lengths = [] # 10 x N[i]\n","  targets = []\n","  for i in range(10):\n","    log_probs.append(torch.randn(int(T[i]), int(N[i]), int(C[i])).log_softmax(2).detach().requires_grad_())\n","    input_lengths.append(torch.full((int(N[i]),), fill_value = int(T[i]), dtype=torch.long))\n","    target_lengths.append(torch.randint(low = 1,high = S, size = (int(N[i]),), dtype=torch.long))\n","    targets.append(torch.randint(low = 1, high = int(C[i]), size = (int(N[i]), S), dtype=torch.long))\n","\n","  return log_probs, targets, input_lengths, target_lengths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BT0wjqLZ5Y6O"},"source":["log_probs, targets, input_lengths, target_lengths = get_test_minibatches()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qeFS6QdamUWw"},"source":["#############################\n","#### YOUR CODE GOES HERE ####\n","\n","pass\n","############################# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ve2yJL3BiPK5"},"source":["This is the end of Part 1 & 2. Great work! "]}]}