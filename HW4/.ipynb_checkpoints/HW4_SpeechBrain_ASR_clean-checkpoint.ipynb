{"cells":[{"cell_type":"markdown","metadata":{"id":"_JBERAXX86Ic"},"source":["# CS224S Assignment 4 SpeechBrain ASR\n","\n","This notebook is worth 60 / 100 possible points for homework 4. You should be able to train all models in Colab. We encourage you to read the speechbrain [tutorial](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing#scrollTo=J6N0Fb51pFnZ) on ASR systems for further context.\n","\n","**Goals**\n","In this part of the homework, you will:\n","1. Run an existing SpeechBrain ASR model on a new dataset (HarperValleyBank)\n","2. Evaluate word error rate (WER) and perform comparative error analysis to help adapt an existing model to new data and understand error patterns.\n","3. Fine tune an end-to-end neural ASR system on a domain-specific dataset to improve performance. \n","\n","**Note:** You will need to make a copy of this Colab notebook in your Google Drive before you can edit it. Please make sure to change your Runtime to GPU."]},{"cell_type":"markdown","metadata":{"id":"b5qc5LC2cWtS"},"source":["# Dependencies "]},{"cell_type":"markdown","metadata":{"id":"oTYjUyWirYR3"},"source":["If you have any issues using `gdown` the same data and config files are available directly via Google Drive [here](https://drive.google.com/drive/folders/1yKSJ6fSSs24c2JL8kpqa311YTELWZ2p7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etpTb5AW8uM0"},"outputs":[],"source":["# setup\n","!gdown 1k2XrxbCz76MG1LptUpcFgocelLMGk5Mu\n","!unzip -q hvb.zip\n","!mv content/data /content/\n","!rm -r /content/content\n","\n","!gdown 1v_3Kl8OrUd6_1_D0ZGoYVFEuOKhZ7YMo # train.py\n","!gdown 17cQIpx5kLLMCD23EDaE0EYg2E9LPqMCF # train.yaml\n","!gdown 1CWYOD2PC97gXguW4krc9122HKAraHkYS # inference.yaml\n","\n","!pip install speechbrain -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30NIWqcyAD-G"},"outputs":[],"source":["import speechbrain as sb\n","from speechbrain.pretrained import EncoderDecoderASR\n","import json\n","import torchaudio\n","import torch\n","from torch import nn\n","from tqdm import tqdm\n","from collections import Counter\n","from IPython.display import Audio\n","from scipy.io import wavfile\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"YwgCA2Oc9h_h"},"source":["# Part 1: Evaluate a pretrained CRDNN model\n","\n","We continue to use the Harper Valley Bank dataset as in HW3. This time, we aim to build and evaluate our own ASR system with the SpeechBrain framework.\n","\n","Here, we load a CRDNN model pretrained on LibriSpeech. SpeechBrain has some utils with built-in options to source pre-trained models from a repository on HuggingFace. In this homework we will load a model from HuggingFace, use it for inference, and then fine tune the model on our dataset. \n","\n","For now, we'll run the model for inference on our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqTF_KNI_mGI"},"outputs":[],"source":["crdnn = EncoderDecoderASR.from_hparams(\n","    source='speechbrain/asr-crdnn-rnnlm-librispeech',\n","    savedir='asr-crdnn-rnnlm-librispeech',\n","    run_opts={'device': 'cuda'}\n",")"]},{"cell_type":"markdown","metadata":{"id":"n9Z1gOWCJCCT"},"source":["The manifests we prepared for using with SpeechBrain are jsons with the structure\n","```\n","{\n","    \"15748\": {\n","        \"wav\": \"/content/data/segments/15748.wav\",\n","        \"length\": 1.86,\n","        \"words\": \"WHAT DAY WOULD YOU LIKE FOR YOUR APPOINTMENT\"\n","    },\n","    ...\n","}\n","```\n","\n","We first load them and define a function to batch them into a format that our `EncoderDecoderASR` object can ingest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NH2kZ0PbAsb1"},"outputs":[],"source":["TEST_SIZE = 200 # for faster processing\n","\n","with open('data/test_manifest.json', 'r') as f:\n","    test_manifest = json.load(f)\n","test_manifest = {\n","    k: v for k, v in list(test_manifest.items())[:TEST_SIZE]\n","}\n","\n","def batchify(manifest, batch_size):\n","    keys = list(manifest.keys())\n","    wav_paths = list(map(lambda x: x['wav'], manifest.values()))\n","    iterable = zip(keys, wav_paths)\n","    num_examples = len(manifest)\n","    for i in range(0, num_examples, batch_size):\n","        batch_wavs = nn.utils.rnn.pad_sequence([\n","            torchaudio.load(path)[0].squeeze(0)\n","            for path in wav_paths[i:min(i + batch_size, num_examples)]\n","        ], batch_first=True)\n","        batch_keys = keys[i:min(i + batch_size, num_examples)]\n","        batch_wav_lens = torch.tensor([\n","            manifest[key]['length'] for key in batch_keys\n","        ])\n","        batch_wav_lens = batch_wav_lens / batch_wav_lens.max()\n","        yield batch_keys, batch_wavs, batch_wav_lens"]},{"cell_type":"markdown","metadata":{"id":"tdRtje1OLiG5"},"source":["Next, we feed our test examples through the ASR model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YCNyfAzFh-y"},"outputs":[],"source":["true_dict = {key: test_manifest[key]['words'] for key in test_manifest}\n","\n","def inference(model, test_manifest, batch_size=8):\n","    torch.cuda.empty_cache()\n","    pred_dict = {}\n","    for keys, wavs, wav_lens in tqdm(batchify(test_manifest, batch_size), total=round(len(test_manifest) / batch_size + 0.5)):\n","        transcriptions, _ = model.transcribe_batch(wavs.to(device), wav_lens.to(device))\n","        for key, transcription in zip(keys, transcriptions):\n","            pred_dict[key] = transcription\n","    return pred_dict\n","\n","pred_dict = inference(crdnn, test_manifest)"]},{"cell_type":"markdown","metadata":{"id":"Yxyq8kjpmhR3"},"source":["## **Task: Evaluate WER of pre-trained model inferences on the new dataset (5 points)**"]},{"cell_type":"markdown","metadata":{"id":"Dbnw0_ZKL9TQ"},"source":["Finally, we check the word error rate on our test set. Note that we want WORD error rate, so we need to split our transcripts into lists of words. \n","\n","You don't need to implement anything new here. Just follow along and ensure you can run the code to obtain WER on the results you just generated. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFnB0JOfPZoj"},"outputs":[],"source":["# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JU-xiNDdSabz"},"outputs":[],"source":["# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"]},{"cell_type":"markdown","metadata":{"id":"c0aOgXMKSowv"},"source":["We expect WER of the off the shelf system to be somewhat high on HVB data (around 74%+ WER). That is quite high! Note that we already re-sampled the audio to 16kHz to make the HVB audio features more similar to the training inputs of the pre-trained model. \n","\n","Often times ASR errors have specific error modes or correlations -- let's see if we can understand where our pre-trained system is failing on HVB data. We can start to investigate where our system is making mistakes by checking some of the top missed utterances."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCOAzZofSoOH"},"outputs":[],"source":["def summarize(detail_dict, true_dict, pred_dict):\n","    print(f\"{detail_dict['key']}: {detail_dict['WER']}\")\n","    print(f\"\\tTrue: {true_dict[detail_dict['key']]}\")\n","    print(f\"\\tPred: {pred_dict[detail_dict['key']]}\")\n","\n","for wer_dict in sb.utils.edit_distance.top_wer_utts(details_by_utterance, 10)[0]:\n","    summarize(wer_dict, true_dict, pred_dict)"]},{"cell_type":"markdown","metadata":{"id":"RrJ-gKFXUHEf"},"source":["Seems that our predictions keep outputting the same word over and over. Let's see why"]},{"cell_type":"markdown","metadata":{"id":"uw4ifE5tjiQE"},"source":["## **Task: Plot spectrograms and listen to mis-transcribed examples (10 points)** "]},{"cell_type":"markdown","metadata":{"id":"LrHqM-WfzFes"},"source":["Select some examples the model mis-transcribes, and try to build a hypothesis around what in the data is associated with the model making mistkes. \n","Visualize at least 3 example mistake types:\n","- Repeated wrong word\n","- A few correct words but clearly wrong transcript\n","- Another example with transcription error of your choice\n","\n","For each example:\n","- Play the audio\n","- Print the true and predicted transcripts\n","- Plot a log spectrogram\n","- Comment (in text) on why you think the model may have made a mistake on this audio. It could include issues like channel quality, background noise, long sections of silence / endpointing issues, etc. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUtl24ogVcja"},"outputs":[],"source":["Audio(test_manifest['5788']['wav'])"]},{"cell_type":"markdown","metadata":{"id":"8q3EtR_WWBjA"},"source":["Another example code snippet to help you start. See HW1 for log spectrogram plotting if you need help."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVNDEHXvWF-D"},"outputs":[],"source":["example = details_by_utterance[0]\n","summarize(example, true_dict, pred_dict)\n","Audio(test_manifest[example['key']]['wav'])\n"]},{"cell_type":"markdown","metadata":{"id":"A4OjAs6oWnK-"},"source":["(Hint: In our initial checks, poor audio quality seems to be associated with repeated word errors.)"]},{"cell_type":"markdown","metadata":{"id":"m0ZXob8IlyPc"},"source":["## **Task: Filtering out bad data (10 points)**"]},{"cell_type":"markdown","metadata":{"id":"81R1WB7pylmK"},"source":["We've just identified that utterances with many repeated word predictions might be associated with audio and endpointing quality in the HVB dataset. To avoid seeing only this error mode when looking at test results, let's try filtering out from our dev set utterances with repeated word predictions.\n","\n","Your task is to define a simple heuristic that ignores utterances from dev set computation. You may achieve this by creating a filtered dev set, and then re-running analysis on it. \n","\n","Report WER overall and top WER utterances on your reduced dev set. This should have errors where the predictor is closer to the true transcript than before. "]},{"cell_type":"markdown","metadata":{"id":"SgD0lyxljLRj"},"source":["Below is some helper code to filter out (e.g. repeated words in predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rg4QjzXFjvNm"},"outputs":[],"source":["def repeated_words(transcript):\n","    \"\"\"\n","    Function that returns True or False if `transcript` suffers from the\n","    repeated words error or not.\n","    \"\"\"\n","    #############################\n","    #### YOUR CODE GOES HERE ####\n","\n","    #############################"]},{"cell_type":"markdown","metadata":{"id":"dqbNUvrLlJq8"},"source":["And let's filter our predictions based on this and check what proportion of examples this kind of error affects"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D94EvIellHtd"},"outputs":[],"source":["pred_dict_filtered = {\n","    k: v for k, v in pred_dict.items() if not repeated_words(v)\n","}\n","true_dict_filtered = {k: true_dict[k] for k in pred_dict_filtered}\n","\n","print(1 - len(pred_dict_filtered) / len(pred_dict))"]},{"cell_type":"markdown","metadata":{"id":"FDrIrPA4lfV9"},"source":["And we finally print our WER stats again on this filtered set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcecMNeHl8GV"},"outputs":[],"source":["details_by_utterance_filtered = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict_filtered.items()},\n","    {k: v.split() for k, v in pred_dict_filtered.items()},\n",")\n","sb.utils.edit_distance.wer_summary(details_by_utterance_filtered)"]},{"cell_type":"markdown","metadata":{"id":"HnSdRne_nIgA"},"source":["More reasonable, but still not great! Remember to finish displaying top WER utterances on your reduced set as requested for this task. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrK8X_YXqhxe"},"outputs":[],"source":["    #############################\n","    #### YOUR CODE GOES HERE ####\n","\n","    #############################"]},{"cell_type":"markdown","metadata":{"id":"iT62DPlesXhK"},"source":["# Part 2: Finetune the pretrained CRDNN model"]},{"cell_type":"markdown","metadata":{"id":"3Bm4ZQZQmJU_"},"source":["The performance of this off the shelf model is disappointing and fundamentally, the model was trained on a different data domain than call center transcripts. To see if we can get some better performance on our dataset, we fine tune the model and test it in this part. For this experiment, you won't need to modify it much. Just get training working, and you can try adjusting some training or decoding parameters as you like. The key thing to learn here is simply how it looks to work with SpeechBrain and fine tune on a new corpus. This is a relatively state-of-the-art approach to building and adjusting ASR models you might use in industry projects (for example).\n","\n","We've set up the training script, experiment yaml, and inference yaml for you, but we encourage you to take a look at how it works (and most importantly what an neat ML experiment yaml file looks like). Training this model for 2 epochs on Colab GPU should take around **1.5 hours**.\n","\n","The model will save checkpoints during training which you can specify for use during inference / testing below. That means you should be able to use a fine tuned version of the model, even if you don't wait the full time for the model to completely train. It is okay to submit the homework with your fine tuning model partially trained, but not 2 full epochs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iM2jQrwTmIw2"},"outputs":[],"source":["# this downloads the training and config files for our fine tuning setup\n","!gdown 1v_3Kl8OrUd6_1_D0ZGoYVFEuOKhZ7YMo # train.py\n","!gdown 17cQIpx5kLLMCD23EDaE0EYg2E9LPqMCF # train.yaml\n","!gdown 1CWYOD2PC97gXguW4krc9122HKAraHkYS # inference.yaml"]},{"cell_type":"markdown","metadata":{"id":"gj_2NC5YywaG"},"source":["### Training configuraiton"]},{"cell_type":"markdown","metadata":{"id":"qg-TThsjy2Ul"},"source":["There are two files you've just downloaded which specify the architecture to train, and the main training loop for improving the neural net ASR system. \n","\n","* `train.yaml` is the yaml config file SpeechBrain uses to specify both the network / ASR system architecture, as well as the parameters of training procedures, loss functions, datasets, etc. This is a good starting point for understanding the architecture of the ASR system you're working with. Note that you are not able to modify much about the ASR network architecture as it needs to match what we load from file. You can adjust things like loss functions weights, learning rate, and training time to adjust the fine tuning setup. \n","* `train.py` specifies the main training loop for fitting the acoustic model. You do not need to modify this file. "]},{"cell_type":"markdown","metadata":{"id":"XgNHQ9_ty2zG"},"source":["### **Task: Run fine tuning training. (15 points)**"]},{"cell_type":"markdown","metadata":{"id":"ITsPPq1ly5fc"},"source":["Edit the training yaml file and run the training loop as shown below. You should see training and validation loss < 1.75 after epoch 1. \n","\n","Run training, and show your training output to validate the model learns something useful and your fine tuning training is configured properly. Ensure your training and validation loss values are displayed somewhere in the output when you submit your solution. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpCTGMTGohrq"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","!python train.py train.yaml --batch_size=4\n","# OOM on batch_size=5"]},{"cell_type":"markdown","metadata":{"id":"IJV8Ym7DR6QF"},"source":["# Part 3: Evaluate our finetuned model\n"]},{"cell_type":"markdown","metadata":{"id":"bOM1BF3TtRB0"},"source":["To run inference, we need to use a different yaml to be compatible with the `EncoderDecoderASR` class. \n","\n","NOTE: You need to set your checkpoint path in a few locations to make this work. Be careful your paths are set before other debugging if things aren't working (e.g. trying to download from HuggingFace)\n","\n","To get inference working there are two steps:\n","1. Note the directory that your checkpoints are saved in (under `./results/CRDNN_BPE_960h_LM/2602/save/{your ckpt here}`)\n","2. Paste this directory into the ckptdir entry in `inference.yaml`\n","3. Paste this directory after the `ckpt_path = ` in the below cell.\n","\n","We can then use the same inference procedure as Part 1.\n","NOTE: set the `ckpt_path` below AND change the path in `inference.yaml` before or after it is copied into your checkpoint path.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3ScTzkkqMYY"},"outputs":[],"source":["ckpt_path = \"/content/results/CRDNN_BPE_960h_LM/2602/save/CKPT+2022-05-09+17-22-55+00\"\n","!cp inference.yaml {ckpt_path}"]},{"cell_type":"markdown","metadata":{"id":"ocuFZxPkZ9Y-"},"source":["## **Task: Run inference on your reduced test set** (5 points)"]},{"cell_type":"markdown","metadata":{"id":"09717HkzuXWR"},"source":["Now we're ready to evaluate our model on our previous test set. Generate predictions by setting up a model object, and calling `inference()`. Remember your checkpoint paths must be set correctly in the copy of inference.yaml read to run inference. For this step simply populate pred_dict with inferences from your reduced dev/test set from before. You will use this for WER analysis in the next step.\n","\n","NOTE: Running inference on ~200 utterances might require ~45 minutes of computation on a Colab cpu. The code below uses CPU inference as we could not get checkpoint-loaded DNNs to work with SpeechBrain's inference on the GPU (you are free to try this). To earn full credit for the homework, you can run on as few as 50 test utterances, but try to choose interesting test examples if you use fewer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ylj89CJOsm91"},"outputs":[],"source":["device = 'cpu'\n","our_model = EncoderDecoderASR.from_hparams(\n","    source=ckpt_path, \n","    hparams_file='inference.yaml', \n","    savedir=\"our_ckpt\",\n","    run_opts={'device': device}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGbPqTws59Vh"},"outputs":[],"source":["pred_dict = inference(our_model.to(device), test_manifest)"]},{"cell_type":"markdown","metadata":{"id":"4PPXslATd0B-"},"source":["## **Task: WER analysis on fine tuned model's predictions** (10 points)"]},{"cell_type":"markdown","metadata":{"id":"xWxF0RqsfDHg"},"source":["Report WER on the dev set for the fine tuned model. Include the full WER summary information provided by the SpeechBrain util. \n","\n","In our experiments, fine tuning yields a WER < 50%, which is an improvement compared with the original system. This is without filtering the dev set for noisy examples. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gk1LdlqatEyw"},"outputs":[],"source":["details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYZbMjgdC8AY"},"outputs":[],"source":["  # As before, print the predicted and true utterances for some large mistakes\n","    #############################\n","    #### YOUR CODE GOES HERE ####\n","\n","    #############################"]},{"cell_type":"markdown","metadata":{"id":"h1_0ZaS4ukx4"},"source":["## **Task: Investigate particular utterances and describe errors** (5 points)"]},{"cell_type":"markdown","metadata":{"id":"n6J5OZTPoQP6"},"source":["Plot some specific utterances as spectrograms and give examples on how the fine tuned model makes mistkes. Can you hypothesize ways to improve the model based on errors you see? It's okay if actually trying the ideas isn't feasible in the homework."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zf0t4I2enlLF"},"outputs":[],"source":["    #############################\n","    #### YOUR CODE GOES HERE ####\n","\n","    #############################"]},{"cell_type":"markdown","metadata":{"id":"Q4OaFlyTolW3"},"source":["# Extra Credit: Iterate to improve this model (up to 20 points)"]},{"cell_type":"markdown","metadata":{"id":"Oaa5Z7vHu1zz"},"source":["The WER still isn't great, and 173M parameters for this domain-specific ASR system is quite a lot. Can you tweak the training script and yamls to perhaps try other models with or without pretraining, change the architecture, or change the training hyperparameters to improve the WER further? \n","\n","We will award >= 10 points for any system that improves upon the WER of basic fine tuning. And we will award 20 points for the best WER system demonstrated. You may try training a speechbrain model from scratch, or changing the fine tuning and decoding of the model above. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKRyM6_Koj8F"},"outputs":[],"source":["    #############################\n","    #### YOUR CODE GOES HERE ####\n","\n","    #############################"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"HW4_SpeechBrain_ASR_clean.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}